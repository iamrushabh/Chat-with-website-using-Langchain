{"file_name": "botpenguin.com_glossary_pruning", "text": "URL: https://botpenguin.com/glossary/pruning\nPruning: Types and Algorithms | BotPenguin\nWhy BotPenguin\nProduct\nSolutions\nPricing\nPartners\nResources\nLogin\nGet Started FREE\nIntegrations\nExperience 80+ world-class integrations.\nKey Features\nTake your business to the next level with our awesome key features.\nLive Chat\nStay in the loop with your clientele 24*7!\nUnified Inbox\nServe your customers across all platforms.\nAnalytics\nSpeedtrack your growth with our top-of-the-line analytics tools!\nMobile App\nMake, monitor, & manage your AI chatbots with our mobile app.\nCHATBOT COMPARISONS\nBotPenguin vs. Manychat\nBotPenguin vs. Tidio\nBotPenguin vs. Tawk.to\nBotPenguin vs. Wati\nBotPenguin vs. Interakt\nBotPenguin vs. AiSensy\nBotPenguin vs. Landbot\nWHAT CAN IT DO?\nMarketing Automation\nMake marketing a boon from the automation gods!\nFB Automation\nEngage with your customers on a deeper level.\nWhatsApp Automation\nGet that nifty automation for WhatsApp too!\nAppointment Bookings\nNo more delays, BotPenguin\u2019s got you here!\nCustomer Support\nYour customers are in for a treat with this automation.\nLead Generation\nGain more lead without any extra effort or expenses\nWHO CAN USE IT?\nHealthcare\nGive your patients world-class healthcare service!\nEducation\nMake admissions and automate processes in a jiffy!\nE-commerce\nCreate the best E-commerce service with ease!\nReal Estate\nMake Real Estate great again with BotPenguin!\nConsultants\nBoost up with our one-stop marketing solution!\nSaaS\nTake your SAAS game to the next level with BotPenguin!\nTours & Travels\nProvide extraordinary tour and travel services with BotPenguin!\nInsurance\nLaunch AI-driven Insurance Bot to Promote, Sell, & Manage Policies.\nWHERE CAN IT RUN?\nInstagram Chatbot\nAttract leads, boost sales, and chat 24/7 with Instagram Chatbots.\nWhatsApp Chatbot\nStart conversing like a real person with BotPenguin!\nTelegram Chatbot\nCutting-edge features for you to grow your business on Telegram.\nFacebook Chatbot\nDo everything at once with BotPenguin on Facebook.\nWebsites Chatbot\nBotPenguin grows your website and keeps your customers engaged.\nWordpress Chatbot\nBotPenguin thrives on WordPress and makes it awesome.\nMicrosoft Teams Chatbot\nMaximize your teams productivity with MS Teams Bot.\nShopify Chatbot\nBoost your Shopify Business With BotPenguin\u2019s AI-powered chatbot.\nWooCommerce Chatbot\nSell effortlessly on your WooCommerce store with BotPenguin.\nSquarespace Chatbot\nGet the most out of Squarespace with BotPenguin\nCUSTOM DEVELOPMENT\nWhitelabel ChatGPT\nApply your branding on ChatGPT, Launch your own AI platform\nChatGPT Custom Plugins\nIntegrate your service straight into ChatGPT\nCustom Chatbot Development\nBuild enterprise-grade chatbots with the best\nChatGPT Clone\nAdd functionality and branding on ChatGPT\nHIRE DEVELOPERS\nChatbot Developers\nBuild Lighter, Faster, Smarter-Efficiently\nChatGPT Developers\nRide the GPT wave with trained surfers\nChatGPT Consultants\nAdvice that makes the difference in your AI journey\nPARTNER PROGRAMS\nPartners Home\nJoin hands with us, and welcome growth\nWhatsApp Whitelabel Partners\nConquer the WhatsApp land with BotPenguin\u2019s White Label Platform\nWhitelabel Partners\nSay hi to the best Whitelabel chatbot platform ever\nAffiliate Partners\nEarn more and keep your clients happier\nImplementation Partners\nAs they say, a partner is worth trillions!\nPARTNER PRICING\nWhitelabel Chatbot Pricing\nOur pricing for Whitelabel Chatbot\nImplementation Partnership Pricing\nOur pricing for Implementation Partnership\nOUR RESOURCES\nBlogs\nRead the latest blogs on chatbots, AI, automations & more\nVideos\nWatch tutorials, webinars, and demos to master our chatbots.\nCase Study\nRead how BotPenguin transformed business communication\nE-book\nExplore e-books written by experts for all your business needs!\nHelp Docs\nFind detailed guides and tips for all your chatbot needs.\nNewsroom\nExplore how BotPenguin is making headlines in the chatbot industry.\nCommunity Support\nJoin our vibrant community to unlock exclusive content & expert guidance\nLATEST BLOG\nWhy is BotPenguin the best platform to develop a chatbot?\nIntroducing ChatGPT 4o for BotPenguin\nCreate your first AI Chatbot\nGet Started FREE\nGLOSSARY\nPruning\nTable of Contents\nWhat is Pruning?\nWhy Pruning?\nTypes of Pruning\nHow does Pruning Work?\nWhen to Prune?\nPruning Algorithms\nEvaluation Measures for Pruning\nApplication of Pruning in Machine Learning\nChallenges and Limitations of Pruning\nFrequently Asked Questions (FAQs)\nShare\nLink copied\nWhat is Pruning?\nPruning is a technique used in machine learning to optimize and improve the performance of models. It involves removing unnecessary components, such as weights or units, from the model without sacrificing accuracy. By eliminating these less important components, pruning reduces the model's complexity, making it more efficient and easier to deploy.\nWhy Pruning?\nPruning is an important technique in machine learning for several reasons. Firstly, it reduces the size of models, making them more compact and efficient. This can be particularly useful for deploying models on resource-constrained devices or in situations where memory and storage are limited.\nSecondly, pruning enhances model interpretability. By removing unnecessary components, pruning makes the underlying structure of the model more transparent and easier to analyze. This can be crucial for understanding the decision-making process of complex models like neural networks.\nTypes of Pruning\nPruning can be performed using different approaches, depending on the specific requirements of the model and the desired outcome. Let's take a look at some common types of pruning:.\nPre-pruning\nPre-pruning, or early stopping, involves halting the growth of the decision tree before it reaches its full size, preventing overfitting.\nPost-pruning\nPost-pruning entails building the full decision tree and then selectively trimming it to improve the generalization and reduce model complexity.\nEnsemble Pruning\nEnsemble pruning targets ensemble learning models by refining and selecting optimal base learners to maximize accuracy and model efficiency.\nWeight Pruning\nWeight pruning, applied to neural networks, eliminates weak connections or weights below a threshold, simplifying the network and reducing computational complexity.\nNeuron Pruning\nNeuron pruning focuses on removing entire neurons or entire layers with lesser contributions, streamlining the neural network while retaining its performance.\nHow does Pruning Work?\nIn this section, we'll cover the key idea behind pruning in\nmachine learning\nmodels.\nRemoving Redundant Connections\nPruning works by removing connections between neurons in a neural network that have little impact on the model's accuracy. This process identifies and eliminates redundant or non-critical connections.\nImproving Efficiency\nBy removing unnecessary connections, pruning reduces the size of the model and eliminates superfluous parameters. This improves the model's efficiency by decreasing storage requirements, reducing overfitting, and speeding up inference.\nIterative Approach\nPruning is done iteratively by removing a small percentage of low-importance connections, retraining the model, and evaluating performance after each round. This maintains accuracy while incrementally compressing the model.\nTradeoff Between Accuracy and Efficiency\nThe degree of pruning involves a tradeoff between model accuracy and efficiency. More aggressive pruning leads to higher efficiency but can impact accuracy if taken too far. Careful tuning is required to balance the two.\nWhen to Prune?\nDeciding when to perform pruning is an important consideration. Pruning can be done during the training process (during iterative optimization) or post-training. The optimal timing for pruning depends on factors such as the model complexity, dataset size, and computational resources available.\nPruning during training allows for continuous refinement of the model and can potentially lead to more efficient convergence. However, it requires additional computational resources during the training process.\nPruning post-training can be performed on pre-trained models and offers flexibility in terms of choosing the pruning threshold and fine-tuning strategies. It is suitable when computational resources are limited or when the training process is time-sensitive.\nPruning Algorithms\nSeveral pruning algorithms have been developed to automate the pruning process. Here are a few popular ones:\nDifferent Pruning Algorithms\nIn this section, we'll delve into various pruning algorithms that aid in optimizing decision trees and neural networks.\nReduced Error Pruning\nThis algorithm removes nodes from the decision tree based on its impact on the overall error rate. The tree is pruned until further pruning increases the validation error.\nCost Complexity Pruning\nAlso known as weakest link pruning, it calculates the complexity of the tree and prunes the nodes that contribute to the highest complexity.\nMinimum Description Length Pruning\nThis approach balances the trade-off between tree simplicity and its fit to training data, pruning nodes to ensure effective generalization.\nOptimal Brain Damage and Optimal Brain Surgeon\nThese algorithms, mainly used in pruning neural networks, remove weights that have minimal contribution to the network's performance.\nMagnitude Based Pruning\nIn neural networks, this technique omits neurons with weights below a certain threshold to simplify the network.\nEvaluation Measures for Pruning\nWhen evaluating the impact of pruning, several metrics can be considered:\nModel Accuracy\nModel accuracy measures the ratio of correct predictions to the total predictions made.\nF1 Score\nF1 score balances precision and recall, offering a comprehensive evaluation of model performance, especially in cases with imbalanced data.\nConfusion Matrix\nA confusion matrix helps visualize the classification performance of a model by showing correct and incorrect predictions in a table.\nFeature Importance\nFeature importance highlights the significance of individual features in contributing to the model's predictions, aiding the pruning process.\nModel Complexity\nEvaluating model complexity can help identify areas to reduce overfitting and improve generalization through pruning.\nApplication of Pruning in Machine Learning\nPruning finds applications in various\nmachine learning\ndomains. Some common examples include:\nReducing Overfitting\nPruning helps reduce overfitting, as it trims network connections or tree branches that have minimal impact on prediction accuracy, ultimately yielding a more generalized model.\nAccelerating Training Time\nBy removing less influential model components, pruning can accelerate training time. This streamlined process contributes to swift development and faster performance tuning.\nEnhancing Model Interpretability\nPruned models can become easier to interpret due to their reduced complexity. Simplified models facilitate better understanding and interpretation of learning patterns and decision-making processes in Machine learning algorithms.\nResource Optimization\nPruning helps minimize resource consumption, like memory and computation demands, by eliminating extraneous elements. This is particularly valuable when deploying models on resource-constrained environments and embedded systems.\nImplementing Post-Training Pruning\nPost-training pruning is an approach where the model is first fully trained and then selectively pruned for optimization. It enables developers to identify and remove unnecessary connections while preserving the model's effectiveness.\nChallenges and Limitations of Pruning\nWhile pruning offers many benefits, it also has some challenges and limitations:\nLoss of Important Information\nSelective removal of nodes or weights while pruning carries the risk of losing potentially valuable information, which may lead to reduced model performance or inaccurate predictions.\nSelection of an Appropriate Threshold\nDetermining an optimal threshold for pruning is critical, as it dictates the balance between model complexity and performance. Finding the right threshold can often be challenging and closely depends on the specific problem domain.\nDependency on the Initial Network Structure\nThe success of pruning relies heavily on the initial network architecture. A suboptimal design can obstruct the pruning process and make it difficult to achieve desired improvements in model efficiency and performance.\nImpact on Model Generalization\nImproper pruning can lead to underfitting or overfitting issues, compromising a model's generalization capabilities. Striking the right balance is crucial for maintaining model performance on unforeseen data.\nComputational Cost and Time\nCertain pruning methods can be computationally intensive, resulting in longer training times and ultimately offsetting the benefits gained by reducing the model's complexity.\nFrequently Asked Questions (FAQs)\nWhat is pruning in machine learning?\nPruning is a technique used to optimize and improve the performance of machine learning models by removing unnecessary components like weights or units while maintaining accuracy.\nHow does pruning work?\nPruning involves assigning importance scores to different components and then removing less important weights or units. The model is then fine-tuned to maintain performance and accuracy.\nWhat are the benefits of pruning?\nPruning reduces the size of models, making them more efficient and easier to deploy, while also enhancing model interpretability. It can also be used to optimize model performance.\nWhen is the best time to perform pruning?\nPruning can be done either during the training process or post-training. The optimal timing for pruning depends on factors such as the model complexity, dataset size, and computational resources available.\nWhat are some pruning algorithms?\nPopular pruning algorithms include the Optimal Brain Surgeon (OBS), Optimal Brain Damage (OBD), Iterative Magnitude Pruning (IMP), and Connection Pruning.\nBuild your first AI chatbot for FREE in just 5 minutes!\nGet Started FREE\nSurprise! BotPenguin has fun blogs too\nWe know you\u2019d love reading them, enjoy and learn.\nWhat is a WhatsApp Campaign? (With Real-World Examples)\nUpdated at Nov 15, 2024\n16 min to read\nBotPenguin\nContent Writer, BotPenguin\nA Comprehensive Look at Generative AI Use Cases Across Industries\nUpdated at Nov 14, 2024\n14 min to read\nManish Goyal\nAI Technical Lead, BotPenguin\nHow Generative AI Models Help in Enhancing Chatbot Conversations\nUpdated at Nov 14, 2024\n8 min to read\nManish Goyal\nAI Technical Lead, BotPenguin\nTable of Contents\nWhat is Pruning?\nWhy Pruning?\nTypes of Pruning\nHow does Pruning Work?\nWhen to Prune?\nPruning Algorithms\nEvaluation Measures for Pruning\nApplication of Pruning in Machine Learning\nChallenges and Limitations of Pruning\nFrequently Asked Questions (FAQs)\nBotPenguin is the best AI Chatbot maker platform. Create a Chatbot for WhatsApp, Website, Facebook Messenger, Telegram, WordPress & Shopify with BotPenguin - 100% FREE! Our chatbot creator helps with lead generation, appointment booking, customer support, marketing automation, WhatsApp & Facebook Automation for businesses. AI-powered No-Code chatbot maker with live chat plugin & ChatGPT integration.\nMobile app\niOS App\nAndroid App\nFully Operational\nStatus\nMobile app\niOS App\nAndroid App\nFully Operational\nStatus\nPlatforms\nMicrosoft Teams\nNew\nInstagram\nNew\nWhatsapp\nTelegram\nFacebook\nWebsites\nPartners\nWhitelabel Partner\nWhatsapp Whitelabel Partner\nImplementation Partner\nAffiliate Partner\nGet Started\nChatbot For Ecommerce\nChatbot For Real Estate\nChatbot For Education\nChatbot For Travel\nAll Templates\nFree Tools\nWhatsApp Link Generator\nWhatsApp QR Code Generator\nOpenAI API Pricing Calculator\nChatbot ROI Calculator\nAll Free Tools\nIntegrations\nChatGPT\nGoHighLevel\nBitrix 24\nZoho CRM\nZapier\nAll Integrations\nResources\nBlogs\nGlossary\nHelp Center\nWrite for us\nContact Us\nWhat\u2019s New\nProduct Updates\nComparisons\nBotPenguin vs Manychat\nBotPenguin vs Tidio\nBotPenguin vs Tawk.to\nBotPenguin vs Wati\nBotPenguin vs Landbot\nAll Comparisons\nAlternatives\nWhat you get\nLead Generation Bot\nSupport Bot\nAssistant Bot\nPlatforms\nMicrosoft Teams\nNew\nInstagram\nNew\nWhatsapp\nTelegram\nFacebook\nWebsites\nGet Started\nIntegrations\nComparisons\nPartners\nFree Tools\nResources\nWhat you get\nTerms & Conditions\nPrivacy Policy\nSecurity\nGDPR\nRefund Policy\nTerms & Conditions\nPrivacy Policy\nSecurity\nGDPR\nRefund Policy\nCopyright \u00a9 2018 - 2024 Relinns Technologies Pvt. Ltd. All RightsReserved.", "embedding": [-0.10109881311655045, -0.05775850638747215, 0.0063059451058506966, -0.007146006915718317, -0.029263608157634735, -0.04806068539619446, 0.03364573046565056, 0.0686945840716362, -0.02346831001341343, 0.024678243324160576, 0.061592575162649155, 0.004792592488229275, 0.02138167805969715, 0.024436717852950096, 0.11435723304748535, -0.012652130797505379, 0.07982666790485382, -0.08305230736732483, -0.08662810176610947, -0.04591778293251991, -0.02349841594696045, -0.036531075835227966, 0.030514467507600784, -0.03300844877958298, -0.0038577269297093153, -0.020253334194421768, -0.03658199682831764, -0.03163876757025719, -0.005365503951907158, -0.08580226451158524, 0.0008879501256160438, 0.028246060013771057, 0.01946614682674408, 0.05516429618000984, 1.1315029951219913e-05, -0.021779676899313927, -0.023795276880264282, 0.01764831878244877, 0.07035325467586517, -0.05611836910247803, -0.08582115918397903, -0.08545128256082535, -0.07258288562297821, -0.010791590437293053, 0.08130694180727005, -0.0006122729973867536, -0.07306472212076187, 0.007940677925944328, 0.0022781831212341785, 0.05167878419160843, -0.0844789445400238, -0.03859349340200424, 0.029491007328033447, 0.06086434796452522, -0.041803110390901566, 0.03655344247817993, -0.026112262159585953, 0.02433544211089611, 0.02806871011853218, -0.012355023995041847, -0.039482515305280685, 0.008716704323887825, 0.03153948858380318, 0.04074430093169212, -0.020659582689404488, 0.009817960672080517, -0.07111217826604843, 0.00028150848811492324, 0.0024654236622154713, -0.027080906555056572, -0.011412194930016994, -0.005141083151102066, -0.05392879247665405, 0.08296441286802292, 0.033439747989177704, -0.024784456938505173, 0.012066487222909927, -0.007973037660121918, 0.004877641797065735, -0.05227551981806755, -0.033332958817481995, 0.04060933738946915, -0.010872721672058105, 0.08514280617237091, -0.02839086577296257, -0.04030187427997589, -0.0252719484269619, 0.017635932192206383, -0.017506593838334084, -0.02030985988676548, 0.01334110926836729, 0.02665112353861332, 0.025103053078055382, 0.012174422852694988, 0.015987010672688484, 0.021916454657912254, -0.08106032758951187, 0.027772609144449234, -0.06393864005804062, 0.00984832551330328, -0.011127268895506859, -0.037894926965236664, -0.04405810311436653, -0.10943849384784698, -0.041248075664043427, 0.011020360514521599, 0.049211613833904266, -0.024520665407180786, 0.13126063346862793, 0.002783035160973668, -0.11921209841966629, -0.04879435524344444, -0.017284246161580086, -0.015980927273631096, 0.016045909374952316, 0.017633579671382904, -0.052520036697387695, 0.05490018427371979, 0.14831727743148804, 0.02775931917130947, 0.07020542770624161, 0.06911367177963257, 0.002334308112040162, -0.009784860536456108, 0.06922166794538498, 0.055849507451057434, 0.006265267264097929, 1.1403383269934669e-32, -0.03510764613747597, 0.04306446760892868, -0.053827669471502304, 0.08005341142416, 0.04248395189642906, 0.027813522145152092, 0.011776970699429512, 0.04021535813808441, -0.053376391530036926, -0.016522720456123352, -0.10158543288707733, 0.11627986282110214, -0.08742189407348633, 0.03298148140311241, 0.05864787846803665, -0.09807758778333664, -0.004101961851119995, 0.0627889558672905, 0.03743082284927368, -0.008241329342126846, 0.08862165361642838, -0.00853583961725235, 0.059490736573934555, 0.07244356721639633, 0.14594073593616486, 0.033815979957580566, 0.07113880664110184, 0.0073197828605771065, 0.034738920629024506, 0.032801445573568344, -0.07084976136684418, 0.02357788011431694, -0.06869208067655563, 0.01891273260116577, -0.05023703724145889, -0.0479639433324337, -0.03790423274040222, -0.11571309715509415, -0.07192797213792801, 0.038248687982559204, -0.15018907189369202, 0.019212394952774048, -0.09168452024459839, -0.08199111372232437, 0.05194345489144325, 0.011631323955953121, 0.03331851586699486, 0.033501073718070984, 0.03907852619886398, 0.030688831582665443, -0.02843596413731575, 0.03705822676420212, 0.05897556617856026, 0.06376828253269196, -0.021124284714460373, -0.05325734615325928, 0.05225010961294174, -0.038779351860284805, 0.00496787391602993, -0.017520833760499954, -0.011798466555774212, -0.02952711097896099, -0.02136654034256935, 0.0029585971496999264, 0.0651712492108345, 0.01015749666839838, 0.048855509608983994, 0.026704711839556694, 0.034310996532440186, 0.04126652330160141, 0.03356681019067764, 0.05329732596874237, -0.028504392132163048, 0.02500692382454872, -0.026808807626366615, -0.012258639559149742, -0.07191699743270874, 0.03148997202515602, -0.03771232068538666, -0.0312582403421402, -0.06198615953326225, 0.012118940241634846, -0.031191866844892502, -0.04075424000620842, 0.05310187116265297, -0.04495180398225784, 0.04397542029619217, -0.03169034793972969, -0.011252980679273605, 0.05146286264061928, -0.04899370297789574, 0.07202401012182236, -0.07952455431222916, 0.07872191071510315, -0.031689755618572235, -9.29310421992782e-33, -0.05483438819646835, 0.008213435299694538, -0.042447492480278015, 0.0952775627374649, 0.01086882222443819, -0.03251862898468971, 0.039456550031900406, -0.03442755714058876, 0.08511103689670563, -0.011065850965678692, -0.09870825707912445, -0.014127794653177261, 0.02203507348895073, -0.029990248382091522, -0.016972659155726433, 0.02391575276851654, -0.06492363661527634, -0.04639972001314163, 0.024366730824112892, 0.015549053438007832, -0.009052622132003307, 0.07720673084259033, -0.10669557750225067, 0.004578377120196819, 0.003059187438338995, 0.03213060647249222, -0.07997957617044449, 0.0380592904984951, 0.0321524441242218, 0.0028320718556642532, -0.013703228905797005, -0.020576586946845055, -0.04401751980185509, -0.007849901914596558, 0.008282280527055264, 0.03221387416124344, 0.011635837145149708, -0.01259507518261671, 0.0011983804870396852, -0.01879063993692398, 0.08505281060934067, -0.06080489978194237, -0.06410109251737595, -0.07261335104703903, 0.0004772061656694859, 0.0352339930832386, -0.13637207448482513, -0.06036975607275963, -0.07271964102983475, 0.03382546082139015, 0.06711877882480621, 0.023955820128321648, 0.04004562273621559, -0.06008640676736832, -0.09307198971509933, -0.03698786348104477, 0.07958611845970154, 0.010691750794649124, -0.09619423747062683, 0.018025390803813934, 0.03833021596074104, -0.00924517773091793, 0.059176113456487656, 0.09447932988405228, 0.031063558533787727, -0.023254578933119774, 0.07030338793992996, 0.052704181522130966, -0.02493729256093502, -0.09111469984054565, 0.0535048171877861, -0.019441956654191017, -0.011399127542972565, -0.010365166701376438, -0.008937488310039043, 0.08146629482507706, 0.02960323542356491, -0.11575772613286972, 0.0101753119379282, -0.027489647269248962, -0.05961054936051369, -0.0038506570272147655, 0.0659172311425209, 0.028938570991158485, -0.09012431651353836, 0.08311434835195541, -0.018924493342638016, 0.00386962853372097, 0.008739352226257324, 0.01772318035364151, -0.06459462642669678, 0.021194938570261, -0.007192846387624741, 0.10332639515399933, -0.01224813424050808, -6.080333747604527e-08, -0.03335633501410484, 0.012037341482937336, 0.04137901961803436, 0.06454869359731674, 0.06334469467401505, -0.06258437037467957, -0.06809362024068832, 0.10651983320713043, 0.014696444384753704, 0.030038271099328995, 0.02491168864071369, -0.02508300356566906, -0.05735085904598236, 0.07958516478538513, 0.08579116314649582, -0.016324250027537346, -0.01643730141222477, -0.022089965641498566, -0.005758532788604498, -0.040804699063301086, 0.06034921482205391, 0.039234161376953125, -0.061071619391441345, -0.024201426655054092, 0.004909095354378223, -0.07173794507980347, -0.04993479326367378, 0.10087525099515915, -0.031874481588602066, -0.004562926013022661, -0.024124298244714737, -0.02500421740114689, 0.05857962742447853, -0.04284031689167023, 0.004289804492145777, 0.01261799968779087, -0.05285535752773285, -0.052756257355213165, 0.0012803623685613275, 0.01764092594385147, 0.003921201918274164, 0.013986925594508648, 0.015192286111414433, -0.07324843108654022, -0.016583530232310295, -0.05922412872314453, -0.07471760362386703, -0.108216293156147, 0.05366041138768196, -0.019798751920461655, -0.04808143526315689, 0.01815585419535637, 0.03135307505726814, 0.029316134750843048, 0.07947248220443726, 0.007229149807244539, 0.0535549633204937, -0.008443762548267841, 0.07555261999368668, 0.09706293046474457, 0.03992381691932678, 0.003291786415502429, 0.02653375267982483, 0.02913348749279976]}