{"file_name": "botpenguin.com_glossary_knowledge-distillation", "text": "URL: https://botpenguin.com/glossary/knowledge-distillation\nUnderstanding Knowledge Distillation, its Process & Trends\nWhy BotPenguin\nProduct\nSolutions\nPricing\nPartners\nResources\nLogin\nGet Started FREE\nIntegrations\nExperience 80+ world-class integrations.\nKey Features\nTake your business to the next level with our awesome key features.\nLive Chat\nStay in the loop with your clientele 24*7!\nUnified Inbox\nServe your customers across all platforms.\nAnalytics\nSpeedtrack your growth with our top-of-the-line analytics tools!\nMobile App\nMake, monitor, & manage your AI chatbots with our mobile app.\nCHATBOT COMPARISONS\nBotPenguin vs. Manychat\nBotPenguin vs. Tidio\nBotPenguin vs. Tawk.to\nBotPenguin vs. Wati\nBotPenguin vs. Interakt\nBotPenguin vs. AiSensy\nBotPenguin vs. Landbot\nWHAT CAN IT DO?\nMarketing Automation\nMake marketing a boon from the automation gods!\nFB Automation\nEngage with your customers on a deeper level.\nWhatsApp Automation\nGet that nifty automation for WhatsApp too!\nAppointment Bookings\nNo more delays, BotPenguin\u2019s got you here!\nCustomer Support\nYour customers are in for a treat with this automation.\nLead Generation\nGain more lead without any extra effort or expenses\nWHO CAN USE IT?\nHealthcare\nGive your patients world-class healthcare service!\nEducation\nMake admissions and automate processes in a jiffy!\nE-commerce\nCreate the best E-commerce service with ease!\nReal Estate\nMake Real Estate great again with BotPenguin!\nConsultants\nBoost up with our one-stop marketing solution!\nSaaS\nTake your SAAS game to the next level with BotPenguin!\nTours & Travels\nProvide extraordinary tour and travel services with BotPenguin!\nInsurance\nLaunch AI-driven Insurance Bot to Promote, Sell, & Manage Policies.\nWHERE CAN IT RUN?\nInstagram Chatbot\nAttract leads, boost sales, and chat 24/7 with Instagram Chatbots.\nWhatsApp Chatbot\nStart conversing like a real person with BotPenguin!\nTelegram Chatbot\nCutting-edge features for you to grow your business on Telegram.\nFacebook Chatbot\nDo everything at once with BotPenguin on Facebook.\nWebsites Chatbot\nBotPenguin grows your website and keeps your customers engaged.\nWordpress Chatbot\nBotPenguin thrives on WordPress and makes it awesome.\nMicrosoft Teams Chatbot\nMaximize your teams productivity with MS Teams Bot.\nShopify Chatbot\nBoost your Shopify Business With BotPenguin\u2019s AI-powered chatbot.\nWooCommerce Chatbot\nSell effortlessly on your WooCommerce store with BotPenguin.\nSquarespace Chatbot\nGet the most out of Squarespace with BotPenguin\nCUSTOM DEVELOPMENT\nWhitelabel ChatGPT\nApply your branding on ChatGPT, Launch your own AI platform\nChatGPT Custom Plugins\nIntegrate your service straight into ChatGPT\nCustom Chatbot Development\nBuild enterprise-grade chatbots with the best\nChatGPT Clone\nAdd functionality and branding on ChatGPT\nHIRE DEVELOPERS\nChatbot Developers\nBuild Lighter, Faster, Smarter-Efficiently\nChatGPT Developers\nRide the GPT wave with trained surfers\nChatGPT Consultants\nAdvice that makes the difference in your AI journey\nPARTNER PROGRAMS\nPartners Home\nJoin hands with us, and welcome growth\nWhatsApp Whitelabel Partners\nConquer the WhatsApp land with BotPenguin\u2019s White Label Platform\nWhitelabel Partners\nSay hi to the best Whitelabel chatbot platform ever\nAffiliate Partners\nEarn more and keep your clients happier\nImplementation Partners\nAs they say, a partner is worth trillions!\nPARTNER PRICING\nWhitelabel Chatbot Pricing\nOur pricing for Whitelabel Chatbot\nImplementation Partnership Pricing\nOur pricing for Implementation Partnership\nOUR RESOURCES\nBlogs\nRead the latest blogs on chatbots, AI, automations & more\nVideos\nWatch tutorials, webinars, and demos to master our chatbots.\nCase Study\nRead how BotPenguin transformed business communication\nE-book\nExplore e-books written by experts for all your business needs!\nHelp Docs\nFind detailed guides and tips for all your chatbot needs.\nNewsroom\nExplore how BotPenguin is making headlines in the chatbot industry.\nCommunity Support\nJoin our vibrant community to unlock exclusive content & expert guidance\nLATEST BLOG\nWhy is BotPenguin the best platform to develop a chatbot?\nIntroducing ChatGPT 4o for BotPenguin\nCreate your first AI Chatbot\nGet Started FREE\nGLOSSARY\nKnowledge Distillation\nTable of Contents\nWhat is Knowledge Distillation?\nWhat is Knowledge Distillation?\nWho uses Knowledge Distillation?\nWhere is Knowledge Distillation applied?\nWhy is Knowledge Distillation important?\nHow does Knowledge Distillation work?\nBest Practices of Knowledge Distillation\nChallenges of Knowledge Distillation\nTrends in Knowledge Distillation\nFrequently Asked Questions (FAQs)\nShare\nLink copied\nWhat is Knowledge Distillation?\nKnowledge distillation, often referred to as Teacher-Student learning, is a process where a simpler model (the Student) is trained to reproduce the behavior of a more complex one (the Teacher).\nHistory of Knowledge Distillation\nKnowledge distillation, in the field of machine learning, really started to shine when Geoffrey Hinton, Oriol Vinyals, and Jeff Dean introduced it in their paper discussing Dark Knowledge in 2015.\nPrior to this, models were only trained once - there was no idea of re-training or using one model to train another.\nImportance of Knowledge Distillation\nKnowledge distillation is particularly crucial when deploying machine learning models on devices with constraints in computational resources.\nThe distilled model is more efficient, faster, and lighter than the original complex one while still maintaining competence in terms of prediction accuracy.\nDifferent from Traditional Machine Learning\nUnlike traditional\nmachine learning\nwhere each model is trained and used independently, the concept of knowledge distillation involves using a preexisting, often larger and more complex model to teach a simpler one.\nImplementation and Use Cases\nKnowledge distillation has been used in several fields. It has been used in Deep Neural Networks (DNNs) to reduce their size and increase run-time efficiency.\nFurthermore, it's used in quantized Neural Networks, where the goal is to train a quantized model that matches the full precision model's accuracy.\nWhat is Knowledge Distillation?\nNow, let's dive a bit deeper into this interesting topic.\nKnowledge distillation is a method used in the training of machine learning models, where a smaller, relatively simpler model (student) is trained to behave like a more complex, pre-existing model (teacher).\nHow is it Different?\nKnowledge Distillation is a type of transfer learning but instead of reusing model structures or pre-trained weights directly, the student learns from the softened output of the teacher model, which contains more information than just hard labels.\nThe Core Concept\nThe crux of this process revolves around the concept of \"\nsoft targets\n.\" These are the output probabilities or logits which the teacher model gives out.\nThe student model then tries to mimic these outputs, thus learning to generalize the task as the teacher model.\nThe Learning Process\nDuring the distillation process, the student model is trained using a softened version of the teacher model's predictive distribution. This softening is usually achieved by raising the temperature of the final softmax, which controls the sharpness of the distribution.\nKey Components\nThe two key components in knowledge distillation process are the teacher model (which is usually pre-trained) and the student model (which is often a smaller architecture and is the one to be trained).\nBoost Your AI-Powered Chatbot Game!\nGet Started FREE\nWho uses Knowledge Distillation?\nSo who is actually employing this process and why? Let's find out.\nKnowledge distillation has wide-ranging applications across industries where machine learning models are deployed.\nTech Giants\nTechnology giants like Google, Facebook, and Amazon, with vast resources and complex models, use it to enhance computational efficiency without compromising on the quality of predictions.\nMobile-First Businesses\nCompanies in the mobile-first era use knowledge distillation extensively to enable the scalability of AI systems on devices with limited computational power.\nMachine Learning Researchers\nResearchers use it to investigate and uncover insights into how sophisticated models such as deep neural networks make decisions, thus contributing to the broader field of explainable\nAI\n.\nEdTech Companies\nEdTech companies also utilize the teacher-student framework to provide personalized learning content tailored to students' capabilities.\nSecurity and Defense\nIt's widely used in Cybersecurity and defense applications where real-time performance is crucial.\nWhere is Knowledge Distillation applied?\nIts application extends far beyond what one may initially think. Here are some areas where it's applied.\nImage recognition\nIn image recognition tasks, a larger convolutional neural network (CNN) can be used as the teacher model to teach a smaller CNN to detect and classify images.\nNatural language processing\nIn the field of Natural Language Processing (\nNLP\n), large models like BERT can be distilled into smaller models providing near-identical performance with much less computational resources.\nSelf-driving cars\nIn the realm of self-driving cars, a larger model that performs a huge number of complex computations can be distilled into a smaller, fast-performing model that still maintains reasonable accuracy.\nSpeech recognition\nIn speech recognition, a very deep neural network can be distilled into a smaller, more compact, and efficient model to carry out real-time tasks on a device with restricted computational resources.\nRecommendation systems\nComplex recommendation systems that run on powerful servers can be distilled into simpler models that can run efficiently on users\u2019 devices, providing personalized recommendations with significantly less\ncomputing\npower.\nWhy is Knowledge Distillation important?\nLet's now understand why knowledge distillation is important.\nEfficient deployment\nOrganizations can deploy high-performing machine learning models on devices with limited computational power and memory because of knowledge distillation.\nBetter performance\nEven with fewer\nparameters\n, distilled models offer comparable or superior performance to larger, more complex models. Hence, distillation is often the key to deploying machine learning solutions on edge devices.\nCost-effective\nBy utilizing fewer computational resources and reducing storage requirements, the cost of maintaining and running models is reduced significantly.\nFaster response times\nAs distilled models are lighter, prediction or response times decrease, offering an improved user experience in real-time applications.\nPrivacy Enhancements\nEdge deployment made possible by knowledge distillation also enhances user data privacy, as the data doesn't have to be sent to a central server for processing.\nHow does Knowledge Distillation work?\nSo how does this teacher-student learning happen? Let's demystify it.\nSelection of Teacher and Student Models\nThe first step involves choosing a pre-trained model (teacher) of complex architecture and a smaller model (student), which is often less sophisticated.\nSoftening the Labels\nThe teacher model's predictions called hard labels, are then 'softened' by applying temperature scaling. This gives more information about the teacher model's confidence in its predictions.\nTraining the Student\nNext, the student model is trained on the softened labels from the teacher model instead of hard labels from the\ndataset\n.\nFine-Tuning the Student\nAfter initial training, further\nfine-tuning\ncan be done on the student model by training it again on the original hard labels.\nAssessing Model Performance\nFinally, the effectiveness of the student model is gauged by comparing its performance with that of the teacher model.\nBest Practices of Knowledge Distillation\nLet\u2019s explore the different practices that help in enhancing the effectiveness of knowledge distillation.\nChoosing the Right Models\nSelecting appropriate teacher and student models is crucial. Generally, the teacher model should be more complex than the student model.\nTemperature Tuning\nCorrect tuning of temperature in the softmax function helps in softening labels and is pivotal for improved performance.\nFine-Tuning\nClose\nfine-tuning\nof the student model on the original\ndataset\nafter initial training can lead to better results.\nDiverse Ensemble as Teachers\nSometimes, using diverse ensemble models as teachers can lead to a richer variety of information and potentially better learning performance.\nProgressive Distillation\nTraining intermediate student models between the initial teacher model and the final student model could lessen the gap between them, thus making learning easier.\nChallenges of Knowledge Distillation\nWhile knowledge distillation is a powerful technique, it's not without its challenges.\nComplexity Misbalance\nIf the sizes of the teacher and student models are too disparate, the student model might struggle to learn effectively.\nOverfitting\nIf the temperature\nparameter\nduring distillation is not selected carefully, the student model can end up overfitting to the teacher model's outputs.\nLoss of Precision\nThere's a risk that important details are lost during distillation due to the process of simplification.\nLack of Transparency\nAs with most\ndeep learning\ntechniques, there's also the potential issue of lack of transparency and interpretability of the process.\nDependence on Teacher\nThe student model's performance heavily depends on the teacher model, and any inaccuracies in the teacher model might propagate to the student model.\nTrends in Knowledge Distillation\nAs the\nmachine learning\nfield expands, so do the trends and approaches in knowledge distillation.\nAugmented Distillation\nThis involves introducing\nartificial\n'jitter' or variations into the data during training to potentially improve the quality of the distilled model.\nSelf-Distillation\nThis is a process where a model is distilled multiple times, with the student model becoming the teacher for the next student model.\nDistillation with Reinforcement Learning\nThe process of distillation has started to enter the realm of\nreinforcement learning\n, where a large and complex policy network can be distilled into a simpler one.\nLayer-wise Distillation\nThis new trend focuses on distilling specific model layers instead of the entire architecture, offering problematic layers a course correction.\nProgressive Distillation\nThis involves the creation and training of intermediate student models between the initial teacher model and the final student model, making the entire learning process smoother.\nThis rounds up our comprehensive dive into the universe of knowledge distillation. We've learned about its history, what it is, its importance and applications, the best practices, and the challenges apart from the trends in this domain.\nWith AI constantly reshaping the world around us, the significance of knowledge distillation is bound to increase over time.\nBoost Your AI-Powered Chatbot Game!\nGet Started FREE\nFrequently Asked Questions (FAQs)\nHow Does Knowledge Distillation Improve Model Deployment on Edge Devices?\nKnowledge distillation compresses complex models into smaller ones that retain original proficiency, making them suitable for resource-constrained edge devices.\nCan Knowledge Distillation be applied to any Machine Learning Model?\nIt is primarily applicable to models where a smaller \"student\" model learns to mimic a larger \"teacher\" model, such as neural networks.\nHow does Temperature affect Knowledge Distillation?\nTemperature controls the softness of the probabilities in the output distribution, affecting the smoothness of the knowledge transferred from teacher to student.\nWhat is the role of the Student Model in Knowledge Distillation?\nThe student model learns the generalized knowledge from the teacher model, aiming to replicate its performance in a more compact architecture.\nCan Knowledge Distillation help in Reducing Overfitting?\nYes, by learning from the softened outputs of the teacher model, a student model may generalize better and reduce overfitting compared to training from scratch.\nBuild your first AI chatbot for FREE in just 5 minutes!\nGet Started FREE\nSurprise! BotPenguin has fun blogs too\nWe know you\u2019d love reading them, enjoy and learn.\nWhat is a WhatsApp Campaign? (With Real-World Examples)\nUpdated at Nov 15, 2024\n16 min to read\nBotPenguin\nContent Writer, BotPenguin\nA Comprehensive Look at Generative AI Use Cases Across Industries\nUpdated at Nov 14, 2024\n14 min to read\nManish Goyal\nAI Technical Lead, BotPenguin\nHow Generative AI Models Help in Enhancing Chatbot Conversations\nUpdated at Nov 14, 2024\n8 min to read\nManish Goyal\nAI Technical Lead, BotPenguin\nTable of Contents\nWhat is Knowledge Distillation?\nWhat is Knowledge Distillation?\nWho uses Knowledge Distillation?\nWhere is Knowledge Distillation applied?\nWhy is Knowledge Distillation important?\nHow does Knowledge Distillation work?\nBest Practices of Knowledge Distillation\nChallenges of Knowledge Distillation\nTrends in Knowledge Distillation\nFrequently Asked Questions (FAQs)\nBotPenguin is the best AI Chatbot maker platform. Create a Chatbot for WhatsApp, Website, Facebook Messenger, Telegram, WordPress & Shopify with BotPenguin - 100% FREE! Our chatbot creator helps with lead generation, appointment booking, customer support, marketing automation, WhatsApp & Facebook Automation for businesses. AI-powered No-Code chatbot maker with live chat plugin & ChatGPT integration.\nMobile app\niOS App\nAndroid App\nFully Operational\nStatus\nMobile app\niOS App\nAndroid App\nFully Operational\nStatus\nPlatforms\nMicrosoft Teams\nNew\nInstagram\nNew\nWhatsapp\nTelegram\nFacebook\nWebsites\nPartners\nWhitelabel Partner\nWhatsapp Whitelabel Partner\nImplementation Partner\nAffiliate Partner\nGet Started\nChatbot For Ecommerce\nChatbot For Real Estate\nChatbot For Education\nChatbot For Travel\nAll Templates\nFree Tools\nWhatsApp Link Generator\nWhatsApp QR Code Generator\nOpenAI API Pricing Calculator\nChatbot ROI Calculator\nAll Free Tools\nIntegrations\nChatGPT\nGoHighLevel\nBitrix 24\nZoho CRM\nZapier\nAll Integrations\nResources\nBlogs\nGlossary\nHelp Center\nWrite for us\nContact Us\nWhat\u2019s New\nProduct Updates\nComparisons\nBotPenguin vs Manychat\nBotPenguin vs Tidio\nBotPenguin vs Tawk.to\nBotPenguin vs Wati\nBotPenguin vs Landbot\nAll Comparisons\nAlternatives\nWhat you get\nLead Generation Bot\nSupport Bot\nAssistant Bot\nPlatforms\nMicrosoft Teams\nNew\nInstagram\nNew\nWhatsapp\nTelegram\nFacebook\nWebsites\nGet Started\nIntegrations\nComparisons\nPartners\nFree Tools\nResources\nWhat you get\nTerms & Conditions\nPrivacy Policy\nSecurity\nGDPR\nRefund Policy\nTerms & Conditions\nPrivacy Policy\nSecurity\nGDPR\nRefund Policy\nCopyright \u00a9 2018 - 2024 Relinns Technologies Pvt. Ltd. All RightsReserved.", "embedding": [-0.07396601885557175, -0.06935849040746689, 0.02629721537232399, 0.014695435762405396, -0.02644762024283409, -0.05079532414674759, 0.05713214725255966, 0.07490649074316025, -0.013474610634148121, 0.03392676264047623, 0.04742677882313728, 0.00545158889144659, 0.01876704767346382, 0.030787890776991844, 0.09432843327522278, -0.00926155038177967, 0.0919874757528305, -0.09397674351930618, -0.09458283334970474, -0.032072514295578, -0.008263926021754742, -0.029864605516195297, 0.0367724671959877, -0.017640044912695885, -0.02759779430925846, -0.02285720966756344, -0.03562096506357193, -0.035267554223537445, -0.003519211197271943, -0.08392944931983948, 0.006145200692117214, 0.04310077428817749, 0.008497006259858608, 0.05613935366272926, -0.011595524847507477, -0.029694147408008575, -0.013088833540678024, 0.03368585929274559, 0.08255933970212936, -0.044438645243644714, -0.07944563776254654, -0.08149765431880951, -0.05949069932103157, -0.001441411441192031, 0.09936556220054626, 0.018827173858880997, -0.06831693649291992, 0.026672957465052605, 0.007205474190413952, 0.07527146488428116, -0.10410907864570618, -0.059184834361076355, 0.03932821378111839, 0.04364285618066788, -0.02855011448264122, 0.037964530289173126, -0.03489825502038002, 0.007816839963197708, 0.020593957975506783, -0.01245971955358982, -0.04191850125789642, -0.00926436297595501, 0.02258933149278164, 0.04028863087296486, -0.025417057797312737, 0.01780187338590622, -0.10878270119428635, 0.01392243430018425, 0.010801033116877079, -0.04729761928319931, -0.01470879465341568, -0.017840620130300522, -0.02785683423280716, 0.05357049033045769, 0.022787639871239662, -0.03927883505821228, 0.0035703503526747227, -0.005347635131329298, 0.011824453249573708, -0.06770522892475128, 0.010023588314652443, 0.06612211465835571, -0.003456059144809842, 0.06099836900830269, -0.02834153175354004, -0.038169976323843, -0.008379606530070305, 0.03297390043735504, -0.01220784243196249, -0.018450560048222542, 0.031525369733572006, -0.014709807001054287, 0.024288643151521683, -0.003618053160607815, 0.040721695870161057, 0.03356583043932915, -0.056643929332494736, 0.01637880690395832, -0.0685218796133995, 0.006168881896883249, -0.0016306252218782902, -0.019080331549048424, -0.06596200913190842, -0.095405712723732, -0.04833126440644264, 0.017159540206193924, 0.05448475107550621, -0.023394864052534103, 0.14628228545188904, 0.010805669240653515, -0.1312600076198578, -0.03643462806940079, 0.007412014529109001, -0.021378718316555023, 0.008442907594144344, 0.026053406298160553, -0.03325463831424713, 0.0726543515920639, 0.1413043588399887, 0.023185858502984047, 0.04833582788705826, 0.05744262412190437, 0.016744360327720642, -0.002722102217376232, 0.05667172744870186, 0.031092340126633644, -0.007149070966988802, 1.0895137716205513e-32, -0.032967161387205124, 0.03543965145945549, -0.059320032596588135, 0.10325285792350769, 0.02859826758503914, 0.020372437313199043, 0.013543862849473953, 0.03208599239587784, -0.0589141920208931, -0.03120664693415165, -0.0817115381360054, 0.10131201148033142, -0.09168654680252075, 0.029606422409415245, 0.03713236004114151, -0.0765111967921257, -0.02811277098953724, 0.029795346781611443, 0.03999330475926399, 0.004782333504408598, 0.09036462008953094, -0.02869187481701374, 0.036748334765434265, 0.07252699881792068, 0.1178349107503891, 0.011462925001978874, 0.061203356832265854, 0.034437913447618484, 0.05812563747167587, 0.027921155095100403, -0.07097402215003967, 0.015260585583746433, -0.051718417555093765, 0.01219369936734438, -0.053990498185157776, -0.06318482756614685, -0.044852931052446365, -0.1198936402797699, -0.06094496697187424, 0.02238372713327408, -0.14744028449058533, 0.016125798225402832, -0.09659703820943832, -0.06336037069559097, 0.025127850472927094, 0.03294886648654938, 0.041651513427495956, 0.0009762959089130163, 0.025315092876553535, 0.023675179108977318, -0.038662321865558624, 0.030795376747846603, 0.035281095653772354, 0.030740393325686455, -0.003770909970626235, -0.02784528024494648, 0.03849843144416809, -0.05359407514333725, 0.011376842856407166, -0.02246920019388199, -0.020103631541132927, -0.011927117593586445, -0.015588134527206421, -0.013252465054392815, 0.07661905139684677, 0.029366346076130867, 0.04159153997898102, 0.03886944055557251, 0.03943592309951782, 0.030702441930770874, 0.02406618557870388, 0.040676698088645935, -0.020486781373620033, 0.012578349560499191, -0.041366495192050934, 0.0033054908271878958, -0.11203140020370483, 0.018278449773788452, -0.031629472970962524, -0.008896572515368462, -0.01585763320326805, -0.022686803713440895, -0.030992789193987846, -0.03724440559744835, 0.06215880811214447, -0.014612698927521706, 0.035801198333501816, -0.03618539124727249, -0.008205464109778404, 0.03791007772088051, -0.037893377244472504, 0.07801476120948792, -0.07879684120416641, 0.07926497608423233, -0.03838444873690605, -8.55303060906537e-33, -0.039176639169454575, 0.010906306095421314, -0.05440317466855049, 0.10883088409900665, 0.030149590224027634, -0.02943175658583641, 0.02013431489467621, -0.035843782126903534, 0.0883350670337677, -0.0108086122199893, -0.08969990164041519, -0.02959519997239113, -0.013809406198561192, -0.017217103391885757, -0.030082177370786667, 0.03168662637472153, -0.04744185507297516, -0.028006399050354958, 0.01748242788016796, 0.018853625282645226, -0.04100386053323746, 0.053591225296258926, -0.13953155279159546, 0.020912574604153633, -0.007167321629822254, 0.04414602369070053, -0.07568683475255966, 0.048522308468818665, 0.020529426634311676, 0.037006083875894547, -0.009852648712694645, -0.0026315213181078434, -0.0022177686914801598, -0.005058320239186287, -0.003470120020210743, 0.052746474742889404, 0.011661196127533913, -0.0394064299762249, -0.026707656681537628, -0.052563268691301346, 0.08862453699111938, -0.04738976061344147, -0.049546267837285995, -0.10729081928730011, -0.01315759401768446, 0.029482629150152206, -0.15321381390094757, -0.03546524792909622, -0.03517105430364609, 0.038468144834041595, 0.08047837018966675, 0.008086725138127804, 0.03803781792521477, -0.054948851466178894, -0.08405795693397522, -0.02141459845006466, 0.09456192702054977, 0.014289766550064087, -0.09878157079219818, 0.026861343532800674, 0.04082740098237991, 0.0031298906542360783, 0.04263167083263397, 0.07735784351825714, 0.043452274054288864, -0.009417746216058731, 0.05437876656651497, 0.06511929631233215, -0.007172241806983948, -0.0902220606803894, 0.03206702321767807, -0.04714883863925934, -0.006581136025488377, -0.03747380152344704, 0.02494925819337368, 0.08727888762950897, 0.03277059644460678, -0.11867648363113403, -0.017353713512420654, -0.027450745925307274, -0.0664091557264328, -0.01679065078496933, 0.06094551458954811, 0.051929768174886703, -0.07718776166439056, 0.0813879519701004, -0.03284338489174843, 0.0043546524830162525, 0.0031456935685127974, 0.025303179398179054, -0.04975620657205582, -0.0049602556973695755, -0.028904873877763748, 0.1018308624625206, 0.001133628305979073, -5.880781017708614e-08, -0.045013345777988434, -0.010393807664513588, 0.054974474012851715, 0.04002662003040314, 0.04635848477482796, -0.06021234393119812, -0.05145701393485069, 0.09402506053447723, 0.015818579122424126, 0.04005032777786255, 0.018360735848546028, -0.018795646727085114, -0.05939318239688873, 0.07960475981235504, 0.08513588458299637, 0.010719305835664272, -0.014508534222841263, -0.004473495297133923, 0.006217469926923513, -0.03815247118473053, 0.10620225220918655, -0.0020190610084682703, -0.028060926124453545, -0.01837545819580555, 0.02356538362801075, -0.07266553491353989, -0.057595089077949524, 0.10210993140935898, -0.039561234414577484, -0.009769470430910587, -0.02763066254556179, -0.04803931340575218, 0.061704833060503006, -0.03634706512093544, 0.013570833951234818, -0.007920300588011742, -0.07258450984954834, -0.09418609738349915, 0.022152118384838104, 0.049007222056388855, 0.00010668863978935406, 0.03835669159889221, 0.014339739456772804, -0.060572411864995956, -0.031527191400527954, -0.047954343259334564, -0.07131078094244003, -0.09344872087240219, 0.03474993631243706, 0.012434822507202625, -0.046912118792533875, 0.023243730887770653, 0.0706118494272232, 0.03473098948597908, 0.09336941689252853, -0.005431672092527151, 0.06328500062227249, 0.007531523238867521, 0.07616676390171051, 0.0881253033876419, 0.045724235475063324, 0.010109169408679008, 0.047347698360681534, 0.039016880095005035]}