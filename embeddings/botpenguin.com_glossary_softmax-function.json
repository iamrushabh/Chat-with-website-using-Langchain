{"file_name": "botpenguin.com_glossary_softmax-function", "text": "URL: https://botpenguin.com/glossary/softmax-function\nSoftmax Function: Advantages and Applications | BotPenguin\nWhy BotPenguin\nProduct\nSolutions\nPricing\nPartners\nResources\nLogin\nGet Started FREE\nIntegrations\nExperience 80+ world-class integrations.\nKey Features\nTake your business to the next level with our awesome key features.\nLive Chat\nStay in the loop with your clientele 24*7!\nUnified Inbox\nServe your customers across all platforms.\nAnalytics\nSpeedtrack your growth with our top-of-the-line analytics tools!\nMobile App\nMake, monitor, & manage your AI chatbots with our mobile app.\nCHATBOT COMPARISONS\nBotPenguin vs. Manychat\nBotPenguin vs. Tidio\nBotPenguin vs. Tawk.to\nBotPenguin vs. Wati\nBotPenguin vs. Interakt\nBotPenguin vs. AiSensy\nBotPenguin vs. Landbot\nWHAT CAN IT DO?\nMarketing Automation\nMake marketing a boon from the automation gods!\nFB Automation\nEngage with your customers on a deeper level.\nWhatsApp Automation\nGet that nifty automation for WhatsApp too!\nAppointment Bookings\nNo more delays, BotPenguin\u2019s got you here!\nCustomer Support\nYour customers are in for a treat with this automation.\nLead Generation\nGain more lead without any extra effort or expenses\nWHO CAN USE IT?\nHealthcare\nGive your patients world-class healthcare service!\nEducation\nMake admissions and automate processes in a jiffy!\nE-commerce\nCreate the best E-commerce service with ease!\nReal Estate\nMake Real Estate great again with BotPenguin!\nConsultants\nBoost up with our one-stop marketing solution!\nSaaS\nTake your SAAS game to the next level with BotPenguin!\nTours & Travels\nProvide extraordinary tour and travel services with BotPenguin!\nInsurance\nLaunch AI-driven Insurance Bot to Promote, Sell, & Manage Policies.\nWHERE CAN IT RUN?\nInstagram Chatbot\nAttract leads, boost sales, and chat 24/7 with Instagram Chatbots.\nWhatsApp Chatbot\nStart conversing like a real person with BotPenguin!\nTelegram Chatbot\nCutting-edge features for you to grow your business on Telegram.\nFacebook Chatbot\nDo everything at once with BotPenguin on Facebook.\nWebsites Chatbot\nBotPenguin grows your website and keeps your customers engaged.\nWordpress Chatbot\nBotPenguin thrives on WordPress and makes it awesome.\nMicrosoft Teams Chatbot\nMaximize your teams productivity with MS Teams Bot.\nShopify Chatbot\nBoost your Shopify Business With BotPenguin\u2019s AI-powered chatbot.\nWooCommerce Chatbot\nSell effortlessly on your WooCommerce store with BotPenguin.\nSquarespace Chatbot\nGet the most out of Squarespace with BotPenguin\nCUSTOM DEVELOPMENT\nWhitelabel ChatGPT\nApply your branding on ChatGPT, Launch your own AI platform\nChatGPT Custom Plugins\nIntegrate your service straight into ChatGPT\nCustom Chatbot Development\nBuild enterprise-grade chatbots with the best\nChatGPT Clone\nAdd functionality and branding on ChatGPT\nHIRE DEVELOPERS\nChatbot Developers\nBuild Lighter, Faster, Smarter-Efficiently\nChatGPT Developers\nRide the GPT wave with trained surfers\nChatGPT Consultants\nAdvice that makes the difference in your AI journey\nPARTNER PROGRAMS\nPartners Home\nJoin hands with us, and welcome growth\nWhatsApp Whitelabel Partners\nConquer the WhatsApp land with BotPenguin\u2019s White Label Platform\nWhitelabel Partners\nSay hi to the best Whitelabel chatbot platform ever\nAffiliate Partners\nEarn more and keep your clients happier\nImplementation Partners\nAs they say, a partner is worth trillions!\nPARTNER PRICING\nWhitelabel Chatbot Pricing\nOur pricing for Whitelabel Chatbot\nImplementation Partnership Pricing\nOur pricing for Implementation Partnership\nOUR RESOURCES\nBlogs\nRead the latest blogs on chatbots, AI, automations & more\nVideos\nWatch tutorials, webinars, and demos to master our chatbots.\nCase Study\nRead how BotPenguin transformed business communication\nE-book\nExplore e-books written by experts for all your business needs!\nHelp Docs\nFind detailed guides and tips for all your chatbot needs.\nNewsroom\nExplore how BotPenguin is making headlines in the chatbot industry.\nCommunity Support\nJoin our vibrant community to unlock exclusive content & expert guidance\nLATEST BLOG\nWhy is BotPenguin the best platform to develop a chatbot?\nIntroducing ChatGPT 4o for BotPenguin\nCreate your first AI Chatbot\nGet Started FREE\nGLOSSARY\nSoftmax Function: Advantages and Applications\nTable of Contents\nIntroduction to Softmax Function\nHow does the Softmax Function work?\nAdvantages of Softmax Function\nApplications of Softmax Function\nHow to Implement the Softmax Function?\nFrequently Asked Questions (FAQs)\nShare\nLink copied\nIntroduction to Softmax Function\nThe Softmax Function is an activation function used in machine learning and deep learning, particularly in multi-class classification problems.\nIts primary role is to transform a vector of arbitrary values into a vector of probabilities.\nThe sum of these probabilities is one, which makes it handy when the output needs to be a probability distribution.\nWhy is the Softmax Function Important?\nSource: ResearchGate\nSoftmax comes into play in various machine learning tasks, particularly those involving multi-class\nclassification\n.\nIt gives the probability distribution of multiple classes, making the decision-making process straightforward and effective.\nBy converting raw scores to probabilities, it not only provides a value to be worked with but also brings clarity to interpreting results.\nWho can utilize the Softmax Function?\nThe Softmax Function is utilized by data scientists,\nmachine learning\nengineers, and deep learning practitioners.\nIt's a fundamental tool in their toolkit, especially when they're working with models that predict the probability of multiple potential outcomes, as in the case of neural network classifiers.\nWhen is the Softmax Function Used?\nThe Softmax Function comes into its own when dealing with multi-class classification tasks in machine learning. In these scenarios, you need your model to predict one out of several possible outcomes.\nSoftmax is typically applied in the final layer of a\nneural network\nduring the training phase, converting raw output scores from previous layers into probabilities that sum up to one.\nWhere is the Softmax Function Implemented?\nIn practice, the Softmax Function finds its application in various fields where machine or\ndeep learning\nmodels are used for prediction.\nThis could be anything from identifying objects in an image, predicting sentiment in a block of text, or even predicting market trends in finance.\nAny field requiring a definitive class prediction based on multiple potential outcomes could benefit from the Softmax Function.\nHow does the Softmax Function work?\nThe Softmax function is a wonderful tool used predominantly in the field of\nmachine learning\nand deep learning for converting a vector of numbers into a vector of probabilities.\nBut how does it do this? Let's break down its magic.\nThe Basic Calculation\nThe function works by taking an input vector and computing the exponential (e-power) of every number in the vector.\nIt then takes each of these results and divides each by the sum of the exponentials of all the numbers. This is the basic formula:\nsoftmax(x)_i = exp(x_i) / \u03a3_j exp(x_j)\nHere,\nx\nis the input vector, and\ni\nand\nj\nare individual indices in the vector.\nProbabilistic Representation\nSource: Towards Data Science\nThe output from this process is another vector, but with a twist - each element in the output vector represents the probabilistic representation of the input.\nThe values of the output vector are in the range of 0 to 1, and the total sum of the elements will be 1.\nThe Benefit of Softmax\nThe key advantage of the Softmax function is that it highlights the largest values and suppresses values which are significantly below the maximum value.\nThis behavior is very useful in multiclass classification problems where the model needs to be confident in its predictions, hence the extensive use of the Softmax function in\ndeep learning\nfor normalizing the output layer.\nIn Action: Classification\nIn the case of classification models, softmax is applied to the output layer of the model, which will then return the probabilities of each class.\nThe class with the highest probability is considered the model\u2019s output (i.e., its prediction).\nIn a nutshell, the Softmax function interprets every number in the input vector as a representation of a certain degree or intensity, and it normalizes these values to get a probability distribution - super helpful in classification problems!\nGet Your Own AI Chatbot\nwith BotPenguin's Custom Solution\nTry BotPenguin\nAdvantages of Softmax Function\nIn this section, we'll discuss the key benefits of using a Softmax function in machine learning and deep learning applications.\nProbabilistic Interpretation\nThe Softmax function converts a vector of real numbers into a probability distribution, with each output value ranging from 0 to 1.\nThe resulting probabilities sum up to 1, which allows for a direct interpretation of the values as probabilities of particular outcomes or classes.\nSuitability for Multi-Classification Problems\nIn\nmachine learning\n,\nthe Softmax function is widely used for multi-classification problems where an instance can belong to one of many possible classes.\nIt provides a way to allocate probabilities to each class, thus helping decide the most likely class for an input instance.\nWorks Well with Gradient Descent\nThe Softmax function is differentiable, meaning it can calculate the gradient of the input values.\nTherefore, it can be used in conjunction with gradient-based optimization methods (such as\nGradient Descent\n), which is essential for training deep learning models.\nStability in Numerical Computation\nThe Softmax function, combined with strategies like log-softmax and softmax with cross-entropy, increases numerical stability during computations, which is important in deep learning models where numerical calculations can span several orders of magnitude.\nEnhances Model Generalization\nSoftmax encourages model to be confident about its most probable prediction, while simultaneously reducing confidence in incorrect predictions.\nThis helps enhance the generalization ability of the model, reducing the chances of overfitting.\nInduces Real-World Decision Making\nAs Softmax outputs class probabilities, it provides a level of uncertainty about the model's predictions, which closely aligns with real-world scenarios.\nThis is particularly useful in decision-making applications where understanding the certainty of model predictions can be critical.\nApplicability Across Various Networks\nThe Softmax function can work effectively across numerous network structures, including convolutional neural networks (CNNs) and\nrecurrent neural networks (RNNs)\n.\nIt's also a crucial component of architectures like transformers used in\nnatural language processing (NLP)\n.\nApplications of Softmax Function\nIn this section, we'll dive into various ways in which the Softmax function is applied.\nMulticlass Classification Problems\nSource: Towards Data Science\nThe Softmax function is extensively used in machine learning, particularly for multiclass classification problems.\nIt helps in assigning the probabilities to each class in a classification problem, ensuring that the sum of all probabilities equals one.\nProbabilistic Models\nProbabilistic models like the Gaussian Mixture model or soft clustering also apply the softmax function.\nIt helps in distributing probability mass among various components of the model.\nNeural Networks\nSource: GeeksforGeeks\nNeural Networks\nharness the Softmax function in the output layer. It translates the outputs of the network into probability values for each class in a multiclass problem, making the final decision of classification.\nDeep Learning Models\nDeep Learning models\nalso apply the Softmax function in their architectures.\nIt's used in models for computer vision, natural language processing, and more, contributing to tasks such as object recognition, semantic segmentation, or machine translation.\nReinforcement Learning\nSource: Towards Data Science\nIn\nreinforcement learning\n, an agent can use the Softmax function to select an action to take in a particular state.\nThis helps in balancing exploration and exploitation, allowing the agent to learn and adapt effectively.\nHow to Implement the Softmax Function?\nImplementing the softmax function is fairly straightforward if you're familiar with a programming language, especially one like\nPython\n, which is commonly used for mathematical operations and\nmachine learning activities\n. Here's a quick guide:\nStep 1\nImport the Necessary Packages\nFirst, import the required package. In Python,\nnumpy\nis used for numerical computations.\nimport numpy as np\nStep 2\nCreate the Softmax Function\nDefine the softmax function. The softmax function computes the exponential (e-power) of the given input value and the sum of exponential values of all the values in the inputs.\ndef softmax(inputs):\u00a0 return np.exp(inputs) / float(sum(np.exp(inputs)))\nStep 3\nSpecify Your Inputs\nYou need to specify the input values to run the softmax function:\ninputs = np.array([2.0, 1.0, 0.1])\nStep 4\nRun the Softmax Function\nFinally, you can call the softmax function that you created with your specified inputs:\nprint(\"Softmax Function Output = \", softmax(inputs))\nOnce run, this program will apply the softmax function to the input array, and print out the results. It's worth noting that the results will all be probabilities and their sum will be approximately one.\nThat's it! The simplicity of implementation is one of the reasons the Softmax function is widely used across machine learning and\ndata science\nfields.\nBuild your own AI\nChatbots in Minutes\nGet Started FREE\nFrequently Asked Questions (FAQs)\nWhat is the Softmax Function used for in Machine Learning?\nThe Softmax Function is widely used in machine learning as it converts the output of a neural network into a probability distribution and enabling\ndecision-making\nfor classification tasks.\nHow does the Softmax Function avoid negative values in its calculation process?\nThe Softmax Function exponentiates the input array before division, which means that negative values are shifted towards zero, resulting in only non-negative outputs.\nIs it possible to apply the Softmax Function to non-classification problems?\nYes, the Softmax Function can be used for any problem that requires calculation of a probability distribution from a set of numbers.\nIt is commonly used in multi-class classification but can also be used in other regression models.\nHow to select the number of neurons in the output layer when using the Softmax Function?\nThe number of neurons in the output layer of a model using the Softmax Function should match the number of classes in the classification problem.\nFor example, if there are five classes, five output neurons should be used.\nWhat are the common techniques used to handle the overflow and underflow errors when calculating the Softmax Function?\nTo handle overflow and underflow errors, techniques such as the log-sum-exp trick, which works by using natural logarithms, can be implemented.\nThe max function can also be used to normalize the inputs before applying the exponential.\nBuild your first AI chatbot for FREE in just 5 minutes!\nGet Started FREE\nSurprise! BotPenguin has fun blogs too\nWe know you\u2019d love reading them, enjoy and learn.\nWhat is a WhatsApp Campaign? (With Real-World Examples)\nUpdated at Nov 15, 2024\n16 min to read\nBotPenguin\nContent Writer, BotPenguin\nA Comprehensive Look at Generative AI Use Cases Across Industries\nUpdated at Nov 14, 2024\n14 min to read\nManish Goyal\nAI Technical Lead, BotPenguin\nHow Generative AI Models Help in Enhancing Chatbot Conversations\nUpdated at Nov 14, 2024\n8 min to read\nManish Goyal\nAI Technical Lead, BotPenguin\nTable of Contents\nIntroduction to Softmax Function\nHow does the Softmax Function work?\nAdvantages of Softmax Function\nApplications of Softmax Function\nHow to Implement the Softmax Function?\nFrequently Asked Questions (FAQs)\nBotPenguin is the best AI Chatbot maker platform. Create a Chatbot for WhatsApp, Website, Facebook Messenger, Telegram, WordPress & Shopify with BotPenguin - 100% FREE! Our chatbot creator helps with lead generation, appointment booking, customer support, marketing automation, WhatsApp & Facebook Automation for businesses. AI-powered No-Code chatbot maker with live chat plugin & ChatGPT integration.\nMobile app\niOS App\nAndroid App\nFully Operational\nStatus\nMobile app\niOS App\nAndroid App\nFully Operational\nStatus\nPlatforms\nMicrosoft Teams\nNew\nInstagram\nNew\nWhatsapp\nTelegram\nFacebook\nWebsites\nPartners\nWhitelabel Partner\nWhatsapp Whitelabel Partner\nImplementation Partner\nAffiliate Partner\nGet Started\nChatbot For Ecommerce\nChatbot For Real Estate\nChatbot For Education\nChatbot For Travel\nAll Templates\nFree Tools\nWhatsApp Link Generator\nWhatsApp QR Code Generator\nOpenAI API Pricing Calculator\nChatbot ROI Calculator\nAll Free Tools\nIntegrations\nChatGPT\nGoHighLevel\nBitrix 24\nZoho CRM\nZapier\nAll Integrations\nResources\nBlogs\nGlossary\nHelp Center\nWrite for us\nContact Us\nWhat\u2019s New\nProduct Updates\nComparisons\nBotPenguin vs Manychat\nBotPenguin vs Tidio\nBotPenguin vs Tawk.to\nBotPenguin vs Wati\nBotPenguin vs Landbot\nAll Comparisons\nAlternatives\nWhat you get\nLead Generation Bot\nSupport Bot\nAssistant Bot\nPlatforms\nMicrosoft Teams\nNew\nInstagram\nNew\nWhatsapp\nTelegram\nFacebook\nWebsites\nGet Started\nIntegrations\nComparisons\nPartners\nFree Tools\nResources\nWhat you get\nTerms & Conditions\nPrivacy Policy\nSecurity\nGDPR\nRefund Policy\nTerms & Conditions\nPrivacy Policy\nSecurity\nGDPR\nRefund Policy\nCopyright \u00a9 2018 - 2024 Relinns Technologies Pvt. Ltd. All RightsReserved.", "embedding": [-0.10684055835008621, -0.0813378319144249, 0.016196560114622116, -0.0013939104974269867, -0.010800729505717754, -0.04103914275765419, 0.03355808183550835, 0.08115941286087036, 0.002316136145964265, 0.007157774642109871, 0.03583009913563728, -0.005874317605048418, 0.0036292399745434523, 0.03338009864091873, 0.09977776557207108, 0.0011449923040345311, 0.11325348168611526, -0.08389437943696976, -0.1112736314535141, -0.027967356145381927, 0.00271419994533062, -0.04401432350277901, 0.06067191809415817, -0.030937379226088524, -0.020932869985699654, -0.051631808280944824, -0.042573895305395126, -0.002746962709352374, 0.015008727088570595, -0.07566762715578079, -0.014059205539524555, 0.02044389769434929, 0.038792934268713, 0.06192827597260475, -0.023147838190197945, -0.02306058444082737, -0.01973983272910118, 0.004385031294077635, 0.0814766213297844, -0.042560189962387085, -0.0914488285779953, -0.08863723278045654, -0.047363750636577606, -0.010483184829354286, 0.09389880299568176, -0.02609865553677082, -0.06523716449737549, 0.023621194064617157, 0.015343600884079933, 0.09768523275852203, -0.06377898901700974, -0.014285323210060596, 0.0391925647854805, 0.06587975472211838, -0.027836307883262634, 0.0335845910012722, -0.05923818424344063, 0.00869259424507618, 0.03315586596727371, 0.012719151563942432, -0.03885485231876373, -0.01675644889473915, 0.07213281095027924, 0.029236629605293274, -0.0216281209141016, 0.024570254608988762, -0.11962074786424637, -0.001985085429623723, -0.02087567001581192, -0.023657817393541336, -0.03434273600578308, -0.02472260408103466, -0.06411395967006683, 0.05009203031659126, 0.016049521043896675, -0.039327241480350494, 0.012296139262616634, -0.015904081985354424, 0.0034674277994781733, -0.040792275220155716, -0.00807922426611185, 0.020821640267968178, 0.0016354607651010156, 0.03638823330402374, -0.01114603504538536, -0.05050716921687126, -0.0210803784430027, 0.03457464277744293, -0.03347279876470566, 0.013744335621595383, 0.02921059913933277, 0.0061806561425328255, 0.03481956198811531, 0.027188600972294807, 0.016533829271793365, 0.014366213232278824, -0.06788582354784012, 0.021013563498854637, -0.05469745770096779, -0.008073404431343079, -0.01975419372320175, -0.02436823770403862, -0.03751895949244499, -0.09452939033508301, -0.01925811544060707, -0.01502651534974575, 0.066303551197052, -0.019999222829937935, 0.12265677750110626, -0.012565833516418934, -0.13234440982341766, -0.036111488938331604, 0.014257767237722874, -0.02333053946495056, 0.02745230682194233, 0.016630757600069046, -0.042298950254917145, 0.05307489633560181, 0.13704779744148254, 0.048366475850343704, 0.05296272411942482, 0.0661497488617897, -0.002501794835552573, 0.021985666826367378, 0.06018151715397835, 0.04299134761095047, -0.01984931342303753, 1.1532664873322073e-32, -0.019605735316872597, 0.029634801670908928, -0.07782936096191406, 0.08179892599582672, 0.021825073286890984, 0.02301456220448017, 0.012753560207784176, 0.03815937414765358, -0.0890536978840828, -0.015778031200170517, -0.10564310848712921, 0.1254081428050995, -0.06774728745222092, 0.037991512566804886, 0.03492162749171257, -0.0832788273692131, -0.009219720028340816, 0.01491266768425703, 0.052423395216464996, -0.003330133855342865, 0.07310295850038528, -0.017280912026762962, 0.06145826727151871, 0.06613040715456009, 0.1289856731891632, 0.04965585470199585, 0.09186936169862747, 0.016184194013476372, 0.04250514507293701, 0.023764532059431076, -0.10454513877630234, -0.0068632871843874454, -0.05882434919476509, 0.008442019112408161, -0.023303285241127014, -0.03412766754627228, -0.053859248757362366, -0.08749149739742279, -0.06897562742233276, 0.022372178733348846, -0.1458480805158615, 0.013836711645126343, -0.12408371269702911, -0.053836677223443985, 0.02168574184179306, 0.03424864262342453, 0.018331320956349373, 0.025379719212651253, 0.03208823874592781, 0.021664395928382874, -0.06032048165798187, 0.03166131675243378, 0.03219890967011452, 0.03620840236544609, -0.0264107808470726, -0.04803524166345596, 0.03386612981557846, -0.03983103856444359, -0.021494023501873016, -0.023838404566049576, -0.01032180618494749, -0.043919872492551804, -0.0014611819060519338, -0.0194079726934433, 0.057430919259786606, 0.022807834669947624, 0.036627396941185, 0.026008201763033867, 0.04431893676519394, 0.044846124947071075, 0.0559847354888916, 0.07955911010503769, -0.003609130624681711, 0.01843203231692314, -0.01923353783786297, 0.00739323953166604, -0.0715542659163475, 0.004335264675319195, -0.05302641913294792, -0.007732166908681393, -0.026637718081474304, 0.025581758469343185, -0.009738540276885033, -0.034819383174180984, 0.04833201691508293, -0.026353253051638603, 0.020639710128307343, -0.04807216301560402, -0.020605195313692093, 0.05573582649230957, -0.07133045047521591, 0.05730472877621651, -0.06104655936360359, 0.07449328899383545, -0.059273529797792435, -8.90646355679123e-33, -0.05416838824748993, 0.022731581702828407, -0.05766173079609871, 0.08907878398895264, 0.011036605574190617, -0.01656392775475979, 0.02940002642571926, -0.03268282860517502, 0.09710807353258133, 0.0132869528606534, -0.0753723680973053, -0.022160351276397705, -0.003152294782921672, -0.03520263731479645, 0.005454297177493572, 0.03418394550681114, -0.07112141698598862, -0.0423533134162426, 0.007978448644280434, 0.0014651014935225248, -0.02133389376103878, 0.07994679361581802, -0.11191749572753906, 0.035894911736249924, -0.012872472405433655, 0.050580766052007675, -0.09400515258312225, 0.04136686772108078, 0.017165787518024445, -0.009115316905081272, -0.026181500405073166, 0.0014760334743186831, -0.04182315617799759, -0.010709773749113083, 0.013530506752431393, 0.03761155903339386, 0.02632593922317028, -0.006167826242744923, -0.014955825172364712, -0.04803824424743652, 0.1159747987985611, -0.03127576783299446, -0.044402189552783966, -0.07606062293052673, -0.00028510778793133795, 0.0550154484808445, -0.12874606251716614, -0.07146846503019333, -0.0739472433924675, 0.041424766182899475, 0.05467385798692703, -0.014275371097028255, 0.055019184947013855, -0.02667808160185814, -0.09573840349912643, -0.03341364860534668, 0.07350566983222961, -0.002259740373119712, -0.07085475325584412, 0.02442922256886959, 0.0316513366997242, -0.011850579641759396, 0.05334533005952835, 0.07124731689691544, 0.02245519682765007, -0.006940382998436689, 0.061487942934036255, 0.025769589468836784, -0.0074154408648610115, -0.06538747996091843, 0.06634886562824249, -0.03564121946692467, 0.005348118953406811, 0.023328935727477074, -0.007419375237077475, 0.08532355725765228, 0.022086217999458313, -0.12228792905807495, -0.021393999457359314, -0.024416297674179077, -0.047236230224370956, -0.00894850492477417, 0.06582334637641907, 0.04574880748987198, -0.09529820829629898, 0.07612286508083344, -0.0010795429116114974, 0.023857347667217255, -0.003776313504204154, 0.00432194210588932, -0.04733739793300629, 0.01934082806110382, -0.0412430614233017, 0.07382351160049438, -0.026622293516993523, -5.969499028424252e-08, -0.0327620729804039, -0.0019135975744575262, 0.022746311500668526, 0.03535715118050575, 0.04684269428253174, -0.06291196495294571, -0.0540509894490242, 0.07987167686223984, 0.02193492278456688, 0.008731035515666008, 0.030032793059945107, -0.042309097945690155, -0.0580010786652565, 0.054087504744529724, 0.07461801171302795, -0.006098999176174402, -0.006756657734513283, -0.010617317631840706, 0.03073476068675518, -0.032996732741594315, 0.07021131366491318, 0.05234719067811966, -0.04312307387590408, -0.020243080332875252, 0.022230293601751328, -0.06547737121582031, -0.06666503101587296, 0.09856157004833221, -0.029462505131959915, -0.00945611298084259, -0.0016765868058428168, -0.00542045384645462, 0.07354019582271576, -0.03338470309972763, -0.011080523952841759, 0.014551824890077114, -0.05240815877914429, -0.07923350483179092, -0.0018946512136608362, 0.07588808983564377, 0.020879341289401054, 0.02391004003584385, -0.0035351738333702087, -0.08758646994829178, -0.03286661580204964, -0.056615568697452545, -0.05641252174973488, -0.14794833958148956, 0.07380146533250809, -0.002291928743943572, -0.025686651468276978, 0.011941326782107353, 0.05729289352893829, 0.03765669837594032, 0.08859964460134506, -0.016525110229849815, 0.059782788157463074, -0.01945459470152855, 0.08645017445087433, 0.10569844394922256, 0.03207973763346672, 0.0369696244597435, 0.02996341697871685, 0.02943859063088894]}