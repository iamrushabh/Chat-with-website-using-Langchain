{"file_name": "botpenguin.com_glossary_pre-training", "text": "URL: https://botpenguin.com/glossary/pre-training\nPre-training: Stages, Importance, Factors & Challenges\nWhy BotPenguin\nProduct\nSolutions\nPricing\nPartners\nResources\nLogin\nGet Started FREE\nIntegrations\nExperience 80+ world-class integrations.\nKey Features\nTake your business to the next level with our awesome key features.\nLive Chat\nStay in the loop with your clientele 24*7!\nUnified Inbox\nServe your customers across all platforms.\nAnalytics\nSpeedtrack your growth with our top-of-the-line analytics tools!\nMobile App\nMake, monitor, & manage your AI chatbots with our mobile app.\nCHATBOT COMPARISONS\nBotPenguin vs. Manychat\nBotPenguin vs. Tidio\nBotPenguin vs. Tawk.to\nBotPenguin vs. Wati\nBotPenguin vs. Interakt\nBotPenguin vs. AiSensy\nBotPenguin vs. Landbot\nWHAT CAN IT DO?\nMarketing Automation\nMake marketing a boon from the automation gods!\nFB Automation\nEngage with your customers on a deeper level.\nWhatsApp Automation\nGet that nifty automation for WhatsApp too!\nAppointment Bookings\nNo more delays, BotPenguin\u2019s got you here!\nCustomer Support\nYour customers are in for a treat with this automation.\nLead Generation\nGain more lead without any extra effort or expenses\nWHO CAN USE IT?\nHealthcare\nGive your patients world-class healthcare service!\nEducation\nMake admissions and automate processes in a jiffy!\nE-commerce\nCreate the best E-commerce service with ease!\nReal Estate\nMake Real Estate great again with BotPenguin!\nConsultants\nBoost up with our one-stop marketing solution!\nSaaS\nTake your SAAS game to the next level with BotPenguin!\nTours & Travels\nProvide extraordinary tour and travel services with BotPenguin!\nInsurance\nLaunch AI-driven Insurance Bot to Promote, Sell, & Manage Policies.\nWHERE CAN IT RUN?\nInstagram Chatbot\nAttract leads, boost sales, and chat 24/7 with Instagram Chatbots.\nWhatsApp Chatbot\nStart conversing like a real person with BotPenguin!\nTelegram Chatbot\nCutting-edge features for you to grow your business on Telegram.\nFacebook Chatbot\nDo everything at once with BotPenguin on Facebook.\nWebsites Chatbot\nBotPenguin grows your website and keeps your customers engaged.\nWordpress Chatbot\nBotPenguin thrives on WordPress and makes it awesome.\nMicrosoft Teams Chatbot\nMaximize your teams productivity with MS Teams Bot.\nShopify Chatbot\nBoost your Shopify Business With BotPenguin\u2019s AI-powered chatbot.\nWooCommerce Chatbot\nSell effortlessly on your WooCommerce store with BotPenguin.\nSquarespace Chatbot\nGet the most out of Squarespace with BotPenguin\nCUSTOM DEVELOPMENT\nWhitelabel ChatGPT\nApply your branding on ChatGPT, Launch your own AI platform\nChatGPT Custom Plugins\nIntegrate your service straight into ChatGPT\nCustom Chatbot Development\nBuild enterprise-grade chatbots with the best\nChatGPT Clone\nAdd functionality and branding on ChatGPT\nHIRE DEVELOPERS\nChatbot Developers\nBuild Lighter, Faster, Smarter-Efficiently\nChatGPT Developers\nRide the GPT wave with trained surfers\nChatGPT Consultants\nAdvice that makes the difference in your AI journey\nPARTNER PROGRAMS\nPartners Home\nJoin hands with us, and welcome growth\nWhatsApp Whitelabel Partners\nConquer the WhatsApp land with BotPenguin\u2019s White Label Platform\nWhitelabel Partners\nSay hi to the best Whitelabel chatbot platform ever\nAffiliate Partners\nEarn more and keep your clients happier\nImplementation Partners\nAs they say, a partner is worth trillions!\nPARTNER PRICING\nWhitelabel Chatbot Pricing\nOur pricing for Whitelabel Chatbot\nImplementation Partnership Pricing\nOur pricing for Implementation Partnership\nOUR RESOURCES\nBlogs\nRead the latest blogs on chatbots, AI, automations & more\nVideos\nWatch tutorials, webinars, and demos to master our chatbots.\nCase Study\nRead how BotPenguin transformed business communication\nE-book\nExplore e-books written by experts for all your business needs!\nHelp Docs\nFind detailed guides and tips for all your chatbot needs.\nNewsroom\nExplore how BotPenguin is making headlines in the chatbot industry.\nCommunity Support\nJoin our vibrant community to unlock exclusive content & expert guidance\nLATEST BLOG\nWhy is BotPenguin the best platform to develop a chatbot?\nIntroducing ChatGPT 4o for BotPenguin\nCreate your first AI Chatbot\nGet Started FREE\nGLOSSARY\nPre-training\nTable of Contents\nWHAT is Pre-Training?\nWHY Do We Need Pre-Training?\nWHO Uses Pre-Training?\nWHEN Should We Use Pre-Training?\nWHERE is Pre-Training Implemented?\nHOW is Pre-Training Executed?\nBest Practices for Pre-Training\nFrequently Asked Questions (FAQs)\nShare\nLink copied\nWHAT is Pre-Training?\nPre-training refers to the initial stage in machine learning models where the model is exposed to a massive dataset to learn fundamental patterns. It's like a toddler learning to recognize objects before learning to read.\nStages in Pre-Training\nTypically, pre-training comprises two stages - feature learning and fine-tuning. Feature learning involves exposure to vast amounts of unlabelled data, while fine-tuning involves a smaller set of labelled data for specific tasks.\nSignificance of Pre-Training\nPre-training empowers the model with a solid foundation of language understanding. It equips the model with a general understanding of language patterns before training it on task-specific data.\nPre-Training in Various Machine Learning Models\nNumerous\nmachine learning\nmodels utilize pre-training, including BERT, GPT-2 and 3, and RoBERTa, to up their game in language understanding tasks.\nWHY Do We Need Pre-Training?\nThe process of pre-training might seem daunting, but its importance can't be understated. Let's explore why.\nBuilding Stronger Foundations\nPre-training equips models with a thorough understanding of language features. This stronger foundation translates into superior performance when the model is fine-tuned on task-specific data.\nImproving Generalization\nPre-training enables models to better generalize across different tasks by learning from a broader range of data, thus improving transfer learning capabilities.\nReducing Overfitting Risk\nBy exposing the model to an extensive\ndataset\n, pre-training reduces the risk of overfitting when\nfine-tuning\non a specific, smaller dataset.\nSaving Computational Resources\nArguably, one of the biggest perks of pre-training is resource-saving. By reusing pre-trained models and fine-tuning them on specific tasks, we can expedite model\ntraining\nwhile conserving computational resources significantly.\nSupercharge Your Chatbot with Automation Today!\nGet Started FREE\nWHO Uses Pre-Training?\nWhether you are a dabbling novice or a seasoned veteran of the machine learning universe, navigating the who's who in the vast plain of pre-training is a valuable exercise.\nData Scientists and AI Engineers\nPre-training is a boon to those who build, refine, and regulate\nmachine learning\nmodels by providing them with a springboard for advanced model performance.\nResearchers\nResearchers find pre-training valuable for devising novel algorithms, improving existing models, or investigating the depths of unsupervised learning methodologies.\nTech Companies\nVarious technology companies, big or small, employ pre-training in their\nAI\nsolutions to fuel cutting-edge innovations and iterate on their product offerings swiftly.\nIndividual Users\nEven individual users or beginners in machine learning and\nAI\ncan harness the power of pre-training by utilizing readily available pre-trained models for their projects.\nWHEN Should We Use Pre-Training?\nPre-training isn't a one-size-fits-all solution. Defining the right time to use pre-training can help you yield optimal results.\nWhen Dealing with Large Datasets\nOne should opt for pre-training when dealing with large and diverse\ndatasets\n. Pre-training helps draw comprehensive language patterns and features from this data.\nWhen Building Robust Language Models\nWhen it comes to building robust language models like\nBERT\nor GPT-3, pre-training is indispensable. It provides these models with a fundamental understanding of language.\nWhen Lack of Task-Specific Data\nWhen the amount of task-specific data is limited, pre-training the model on a large corpus can be beneficial to improve model performance later.\nWhen Time and Computational Resources are Limited\nWhen time and computational resources are constrained, using a pre-trained model, and fine-tuning it on the task-specific data saves both.\nWHERE is Pre-Training Implemented?\nThe fascinating world of pre-training isn't confined to a certain area. It finds application in various fields of machine learning.\nIn Building AI Models\nPre-training forms the building blocks of\nAI\nmodels, including machine translation models, computer vision models, and voice recognition models.\nIn Fine-Tuning Tasks\nPre-training is the predecessor that sets the stage for fine-tuning tasks, allowing models to excel in specific tasks ranging from\nsentiment analysis\nto text extraction and more.\nIn Transfer Learning Approaches\nIn the realm of transfer learning, pre-training finds its place by providing a base model that can be adapted and fine-tuned for various tasks.\nIn AI-Based Applications\nPre-training plays a pivotal role in real-world AI-based applications like\nvoice assistants\n, search engines, and\nAI chatbots\n, contributing to a more intuitive, human-like interaction.\nHOW is Pre-Training Executed?\nEmbarking on the pathway of pre-training execution might seem arduous, but understanding each step will clear the fog and help pave the way.\nChoosing the Right Dataset\nBefore diving into pre-training, one must choose an appropriate, large-scale dataset. This\ndataset\nshould be diverse and cover a broad spectrum of language patterns to inculcate a comprehensive understanding.\nPerforming Feature Learning\nOnce the dataset is in place, the model goes through the feature learning phase, where it learns the intricate nuances of language, syntax, grammar, context, and more.\nFine-Tuning Phase\nPost pre-training, the fine-tuning phase follows where the pre-trained model is further trained on a smaller, task-specific dataset. The model refines its capabilities for specific tasks during this phase.\nEvaluate and Utilize the Model\nFinally, after performing pre-training and fine-tuning, the model is evaluated on unseen data. Post-evaluation, the model is ready to be put to use in real-world applications.\nBest Practices for Pre-Training\nAs we venture into the territory of pre-training, it's important to equip ourselves with some best practices. These guidelines will assist you with an efficient and effective pre-training process.\nChoosing the Right Dataset\nSelecting an appropriate dataset for pre-training can make or break your model's eventual performance. Aim for a large, diverse dataset that represents a broad range of language patterns and contexts. Consider public datasets like Wikipedia or the Common Crawl.\nMatching Pre-Training and Fine-Tuning Domains\nIf possible, ensure the domains of your training and fine-tuning datasets are similar. For instance, if you're fine-tuning a model on medical texts, pre-training on a general language corpus and a corpus of medical literature might produce a better-performing model.\nCautious Use of Regularization\nDuring pre-training, care should be taken to avoid overuse of regularization techniques such as dropout or weight decay. While these techniques can be beneficial to prevent overfitting, too much regularization can hinder the model\u2019s ability to learn from the data.\nMonitoring and Evaluating Performances\nRegularly monitor the performance of your model during pre-training. Watch out for signs of overfitting, and evaluate the model on held-out validation data. By doing so, you can adjust training hyperparameters in a timely manner if required.\nBuilding Better Businesses with Bots!\nTry BotPenguin\nFrequently Asked Questions (FAQs)\nDoes pre-training in NLP improve the accuracy of NLP models?\nYes, pre-training improves model accuracy by providing a better understanding of language patterns, enhancing generalization, and enabling transfer learning across different\nNLP\ntasks.\nWhat are some common use cases for pre-trained NLP models?\nPre-trained\nNLP\nmodels are commonly used for tasks like text classification, sentiment analysis, named entity recognition, text summarization, and machine translation.\nHow do I select the most suitable pre-trained NLP model for my task?\nConsider factors such as task compatibility, language and domain, model architecture, training dataset, model performance, availability, resources required, bias and fairness, documentation, and community support.\nWhat challenges come with pre-training in NLP?\nSome challenges include addressing biases in pre-trained models, the computational demands of training large datasets, the need for labeled data for fine-tuning, and overcoming limitations for better performance.\nWhat is the future of pre-training in NLP?\nThe future of pre-training in\nNLP\ninvolves addressing limitations, developing new techniques to improve pre-training methods, and enhancing language understanding and processing capabilities of NLP models.\nBuild your first AI chatbot for FREE in just 5 minutes!\nGet Started FREE\nSurprise! BotPenguin has fun blogs too\nWe know you\u2019d love reading them, enjoy and learn.\nWhat is a WhatsApp Campaign? (With Real-World Examples)\nUpdated at Nov 15, 2024\n16 min to read\nBotPenguin\nContent Writer, BotPenguin\nA Comprehensive Look at Generative AI Use Cases Across Industries\nUpdated at Nov 14, 2024\n14 min to read\nManish Goyal\nAI Technical Lead, BotPenguin\nHow Generative AI Models Help in Enhancing Chatbot Conversations\nUpdated at Nov 14, 2024\n8 min to read\nManish Goyal\nAI Technical Lead, BotPenguin\nTable of Contents\nWHAT is Pre-Training?\nWHY Do We Need Pre-Training?\nWHO Uses Pre-Training?\nWHEN Should We Use Pre-Training?\nWHERE is Pre-Training Implemented?\nHOW is Pre-Training Executed?\nBest Practices for Pre-Training\nFrequently Asked Questions (FAQs)\nBotPenguin is the best AI Chatbot maker platform. Create a Chatbot for WhatsApp, Website, Facebook Messenger, Telegram, WordPress & Shopify with BotPenguin - 100% FREE! Our chatbot creator helps with lead generation, appointment booking, customer support, marketing automation, WhatsApp & Facebook Automation for businesses. AI-powered No-Code chatbot maker with live chat plugin & ChatGPT integration.\nMobile app\niOS App\nAndroid App\nFully Operational\nStatus\nMobile app\niOS App\nAndroid App\nFully Operational\nStatus\nPlatforms\nMicrosoft Teams\nNew\nInstagram\nNew\nWhatsapp\nTelegram\nFacebook\nWebsites\nPartners\nWhitelabel Partner\nWhatsapp Whitelabel Partner\nImplementation Partner\nAffiliate Partner\nGet Started\nChatbot For Ecommerce\nChatbot For Real Estate\nChatbot For Education\nChatbot For Travel\nAll Templates\nFree Tools\nWhatsApp Link Generator\nWhatsApp QR Code Generator\nOpenAI API Pricing Calculator\nChatbot ROI Calculator\nAll Free Tools\nIntegrations\nChatGPT\nGoHighLevel\nBitrix 24\nZoho CRM\nZapier\nAll Integrations\nResources\nBlogs\nGlossary\nHelp Center\nWrite for us\nContact Us\nWhat\u2019s New\nProduct Updates\nComparisons\nBotPenguin vs Manychat\nBotPenguin vs Tidio\nBotPenguin vs Tawk.to\nBotPenguin vs Wati\nBotPenguin vs Landbot\nAll Comparisons\nAlternatives\nWhat you get\nLead Generation Bot\nSupport Bot\nAssistant Bot\nPlatforms\nMicrosoft Teams\nNew\nInstagram\nNew\nWhatsapp\nTelegram\nFacebook\nWebsites\nGet Started\nIntegrations\nComparisons\nPartners\nFree Tools\nResources\nWhat you get\nTerms & Conditions\nPrivacy Policy\nSecurity\nGDPR\nRefund Policy\nTerms & Conditions\nPrivacy Policy\nSecurity\nGDPR\nRefund Policy\nCopyright \u00a9 2018 - 2024 Relinns Technologies Pvt. Ltd. All RightsReserved.", "embedding": [-0.09917866438627243, -0.06955011188983917, 0.0215720497071743, 0.02723032794892788, -0.023213528096675873, -0.013947034254670143, 0.042075298726558685, 0.08240187168121338, -0.03808000311255455, 0.02441646344959736, 0.04511864855885506, -0.0117172347381711, -0.0014049179153516889, 0.026474574580788612, 0.10305782407522202, -0.022238964214920998, 0.11728321015834808, -0.11570866405963898, -0.0761038288474083, -0.03856225684285164, -0.003014453686773777, -0.02984101139008999, 0.045557450503110886, -0.015506069175899029, -0.023748639971017838, -0.046761404722929, -0.03461705893278122, -0.020489783957600594, -0.0003865398757625371, -0.07163406163454056, 0.003332793014124036, 0.016082046553492546, 0.022393813356757164, 0.05888912081718445, -0.039686169475317, -0.01847890391945839, -0.017385853454470634, 0.011424439027905464, 0.08251961320638657, -0.041144076734781265, -0.09103552997112274, -0.0921260342001915, -0.06333919614553452, -0.0042351926676929, 0.10751131922006607, 0.0014647790230810642, -0.05704617127776146, 0.008800016716122627, 0.0052927532233297825, 0.07453416287899017, -0.09192017465829849, -0.07109176367521286, 0.04822923615574837, 0.03737498074769974, -0.043010164052248, 0.05123543366789818, -0.017412696033716202, 0.008979832753539085, 0.030921263620257378, -0.019003110006451607, -0.07241565734148026, -0.0199747197329998, 0.024378912523388863, 0.029106512665748596, -0.035009898245334625, 0.01724778488278389, -0.09823305904865265, 0.012692410498857498, 0.02854589745402336, -0.025823626667261124, -0.017055587843060493, -0.0034254714846611023, -0.036349739879369736, 0.06825943291187286, 0.024337342008948326, -0.017185376957058907, 0.005871020257472992, 0.01945251040160656, 0.01816500909626484, -0.06035378947854042, -0.004946969449520111, 0.058961741626262665, 0.003196262987330556, 0.05804417282342911, -0.035674482583999634, -0.024935301393270493, -0.007380980998277664, 0.04161813482642174, -0.030476603657007217, -0.0007911670254543424, 0.049238964915275574, 0.010435477830469608, 0.010250135324895382, 0.023252084851264954, 0.006508295424282551, 0.0444435253739357, -0.07619976997375488, 0.016153035685420036, -0.05732368305325508, -0.006018585991114378, 0.014806711114943027, -0.03929336369037628, -0.06959269940853119, -0.04748521372675896, -0.05303981527686119, 0.013244232162833214, 0.05689920857548714, -0.020281942561268806, 0.15158315002918243, 0.012965773232281208, -0.10814015567302704, -0.05143376812338829, -0.015008380636572838, -0.024998625740408897, -0.000847103598061949, 0.055658113211393356, -0.047560106962919235, 0.04984360188245773, 0.14039713144302368, 0.04335888475179672, 0.047092415392398834, 0.062496691942214966, 0.019289659336209297, -0.015096912160515785, 0.04706829786300659, -0.0020572084467858076, -0.0208213422447443, 9.5065416679425e-33, -0.015098298899829388, 0.024722713977098465, -0.052003372460603714, 0.09224752336740494, 0.018409807235002518, 0.00478667626157403, 0.03570053353905678, 0.03956760838627815, -0.05143117159605026, -0.021249212324619293, -0.0970424935221672, 0.11685047298669815, -0.08277911692857742, 0.04208557680249214, 0.023690247908234596, -0.08944565802812576, -0.03341394290328026, 0.021373510360717773, 0.054586488753557205, 0.006595045328140259, 0.08387599140405655, -0.04468299821019173, 0.05484912917017937, 0.07301250100135803, 0.1392383575439453, 0.0387498177587986, 0.06941495090723038, 0.017214324325323105, 0.05141180753707886, 0.03406824171543121, -0.09687254577875137, 0.00668792799115181, -0.05280845984816551, -0.004156861454248428, -0.04376903176307678, -0.05300339683890343, -0.042431436479091644, -0.10919089615345001, -0.0709187388420105, 0.027775531634688377, -0.13970719277858734, 0.007865478284657001, -0.07920601963996887, -0.08284135162830353, 0.025660395622253418, 0.0015362680424004793, 0.02029617875814438, 0.005765833891928196, 0.028318971395492554, 0.03681017830967903, -0.06323949992656708, 0.011272415518760681, 0.03088965080678463, 0.03517237678170204, -0.01424733828753233, -0.011895307339727879, 0.05334102734923363, -0.03162682428956032, -0.011952649801969528, -0.04146372526884079, -0.005485376343131065, -0.023746568709611893, -0.02417919598519802, -0.0023931197356432676, 0.03151362016797066, -0.0004468127153813839, 0.0524764247238636, 0.025235887616872787, 0.04911767691373825, 0.0113908089697361, 0.024916883558034897, 0.023336222395300865, -0.011587281711399555, 0.0268959179520607, -0.008917083032429218, 0.019094528630375862, -0.08424787223339081, 0.026824137195944786, -0.050080329179763794, -0.0033337720669806004, -0.016171002760529518, 0.005546921864151955, -0.016323814168572426, -0.038004737347364426, 0.06205719709396362, -0.04253742843866348, 0.042774178087711334, -0.04643208160996437, -0.007342672906816006, 0.04493550956249237, -0.04555835947394371, 0.08213762938976288, -0.07716035097837448, 0.09630461037158966, -0.04671391099691391, -7.382829656241629e-33, -0.004941283725202084, 0.034562744200229645, -0.052849434316158295, 0.08878248184919357, 0.03016074001789093, -0.013286605477333069, 0.05329516902565956, -0.024046894162893295, 0.08089017868041992, -0.0046436055563390255, -0.09108711779117584, -0.031048882752656937, 0.015927061438560486, -0.025990072637796402, -0.013848465867340565, 0.014082904905080795, -0.07297533750534058, -0.027814697474241257, 0.032598745077848434, 0.01483851671218872, -0.016849171370267868, 0.06334132701158524, -0.13632619380950928, -0.0062677739188075066, -0.007964503020048141, 0.027625316753983498, -0.06683127582073212, 0.07155945152044296, 0.013570628128945827, 0.03440723940730095, -0.022235523909330368, -0.014869297854602337, -0.00843727495521307, -0.005524262320250273, -0.00332161714322865, 0.053360890597105026, 0.026262294501066208, -0.012690072879195213, -0.008202501572668552, -0.021698081865906715, 0.11744625866413116, -0.06685816496610641, -0.052059318870306015, -0.07795185595750809, -0.016689946874976158, 0.017490770667791367, -0.12810485064983368, -0.032838162034749985, -0.04838976636528969, 0.03623932600021362, 0.05145702511072159, 0.013984140940010548, 0.035018324851989746, -0.06282923370599747, -0.07802178710699081, -0.0561005063354969, 0.0970156341791153, -0.011496832594275475, -0.08685976266860962, 0.02780063822865486, 0.03133968263864517, 0.009165537543594837, 0.07235860079526901, 0.0694722831249237, 0.030325332656502724, -0.03760608658194542, 0.03793065994977951, 0.06027969345450401, -0.003348916070535779, -0.08890706300735474, 0.030087381601333618, -0.03337467834353447, -0.0011049701133742929, -0.008713721297681332, -0.0016264751320704818, 0.07763122022151947, 0.023053420707583427, -0.10548275709152222, 0.00825230497866869, -0.02651846595108509, -0.06908334046602249, -0.0006107906228862703, 0.045179445296525955, 0.06681528687477112, -0.06847329437732697, 0.10960361361503601, -0.0022241331171244383, 0.010863211005926132, 0.0004313156823627651, 0.0071905748918652534, -0.046247199177742004, 0.03832739219069481, -0.007512889336794615, 0.0931902527809143, -0.014405058696866035, -5.687477866445079e-08, -0.04006829485297203, 0.0001097662461688742, 0.060994893312454224, 0.06154758110642433, 0.04251566529273987, -0.07117687910795212, -0.040980271995067596, 0.09351406246423721, 0.015218264423310757, 0.04307191073894501, 0.013820006512105465, -0.011032352223992348, -0.05730723589658737, 0.04905656725168228, 0.0608583427965641, 0.002572939032688737, -0.02385985665023327, 0.0004029152332805097, 0.004920110572129488, -0.0403444878757, 0.09412909299135208, 0.016533320769667625, -0.030198512598872185, -0.03904430940747261, 0.029436413198709488, -0.08432738482952118, -0.05481622740626335, 0.09337630867958069, -0.041724078357219696, -0.0070236180908977985, -0.028992921113967896, -0.03212933614850044, 0.06693566590547562, -0.05322697386145592, 0.012031732127070427, -0.005699033383280039, -0.06729518622159958, -0.08622665703296661, 0.00812600925564766, 0.03804134950041771, -0.01584233157336712, 0.025448810309171677, 0.026845214888453484, -0.0890338122844696, -0.027664680033922195, -0.031836822628974915, -0.07788034528493881, -0.10866928845643997, 0.0403616689145565, -0.0064789666794240475, -0.06718248873949051, 0.014553016051650047, 0.04374605417251587, 0.03641374036669731, 0.10473541915416718, 0.015756964683532715, 0.05836811289191246, -0.010969976894557476, 0.07173069566488266, 0.12011277675628662, 0.023648029193282127, 0.014500955119729042, 0.03168220445513725, 0.04524075239896774]}