{"file_name": "botpenguin.com_glossary_regularization", "text": "URL: https://botpenguin.com/glossary/regularization\nHow Does Regularization Work & What Are Its Challenges\nWhy BotPenguin\nProduct\nSolutions\nPricing\nPartners\nResources\nLogin\nGet Started FREE\nIntegrations\nExperience 80+ world-class integrations.\nKey Features\nTake your business to the next level with our awesome key features.\nLive Chat\nStay in the loop with your clientele 24*7!\nUnified Inbox\nServe your customers across all platforms.\nAnalytics\nSpeedtrack your growth with our top-of-the-line analytics tools!\nMobile App\nMake, monitor, & manage your AI chatbots with our mobile app.\nCHATBOT COMPARISONS\nBotPenguin vs. Manychat\nBotPenguin vs. Tidio\nBotPenguin vs. Tawk.to\nBotPenguin vs. Wati\nBotPenguin vs. Interakt\nBotPenguin vs. AiSensy\nBotPenguin vs. Landbot\nWHAT CAN IT DO?\nMarketing Automation\nMake marketing a boon from the automation gods!\nFB Automation\nEngage with your customers on a deeper level.\nWhatsApp Automation\nGet that nifty automation for WhatsApp too!\nAppointment Bookings\nNo more delays, BotPenguin\u2019s got you here!\nCustomer Support\nYour customers are in for a treat with this automation.\nLead Generation\nGain more lead without any extra effort or expenses\nWHO CAN USE IT?\nHealthcare\nGive your patients world-class healthcare service!\nEducation\nMake admissions and automate processes in a jiffy!\nE-commerce\nCreate the best E-commerce service with ease!\nReal Estate\nMake Real Estate great again with BotPenguin!\nConsultants\nBoost up with our one-stop marketing solution!\nSaaS\nTake your SAAS game to the next level with BotPenguin!\nTours & Travels\nProvide extraordinary tour and travel services with BotPenguin!\nInsurance\nLaunch AI-driven Insurance Bot to Promote, Sell, & Manage Policies.\nWHERE CAN IT RUN?\nInstagram Chatbot\nAttract leads, boost sales, and chat 24/7 with Instagram Chatbots.\nWhatsApp Chatbot\nStart conversing like a real person with BotPenguin!\nTelegram Chatbot\nCutting-edge features for you to grow your business on Telegram.\nFacebook Chatbot\nDo everything at once with BotPenguin on Facebook.\nWebsites Chatbot\nBotPenguin grows your website and keeps your customers engaged.\nWordpress Chatbot\nBotPenguin thrives on WordPress and makes it awesome.\nMicrosoft Teams Chatbot\nMaximize your teams productivity with MS Teams Bot.\nShopify Chatbot\nBoost your Shopify Business With BotPenguin\u2019s AI-powered chatbot.\nWooCommerce Chatbot\nSell effortlessly on your WooCommerce store with BotPenguin.\nSquarespace Chatbot\nGet the most out of Squarespace with BotPenguin\nCUSTOM DEVELOPMENT\nWhitelabel ChatGPT\nApply your branding on ChatGPT, Launch your own AI platform\nChatGPT Custom Plugins\nIntegrate your service straight into ChatGPT\nCustom Chatbot Development\nBuild enterprise-grade chatbots with the best\nChatGPT Clone\nAdd functionality and branding on ChatGPT\nHIRE DEVELOPERS\nChatbot Developers\nBuild Lighter, Faster, Smarter-Efficiently\nChatGPT Developers\nRide the GPT wave with trained surfers\nChatGPT Consultants\nAdvice that makes the difference in your AI journey\nPARTNER PROGRAMS\nPartners Home\nJoin hands with us, and welcome growth\nWhatsApp Whitelabel Partners\nConquer the WhatsApp land with BotPenguin\u2019s White Label Platform\nWhitelabel Partners\nSay hi to the best Whitelabel chatbot platform ever\nAffiliate Partners\nEarn more and keep your clients happier\nImplementation Partners\nAs they say, a partner is worth trillions!\nPARTNER PRICING\nWhitelabel Chatbot Pricing\nOur pricing for Whitelabel Chatbot\nImplementation Partnership Pricing\nOur pricing for Implementation Partnership\nOUR RESOURCES\nBlogs\nRead the latest blogs on chatbots, AI, automations & more\nVideos\nWatch tutorials, webinars, and demos to master our chatbots.\nCase Study\nRead how BotPenguin transformed business communication\nE-book\nExplore e-books written by experts for all your business needs!\nHelp Docs\nFind detailed guides and tips for all your chatbot needs.\nNewsroom\nExplore how BotPenguin is making headlines in the chatbot industry.\nCommunity Support\nJoin our vibrant community to unlock exclusive content & expert guidance\nLATEST BLOG\nWhy is BotPenguin the best platform to develop a chatbot?\nIntroducing ChatGPT 4o for BotPenguin\nCreate your first AI Chatbot\nGet Started FREE\nGLOSSARY\nRegularization\nTable of Contents\nWhat is Regularization?\nWhy is Regularization Critical?\nWhen to Use Regularization?\nWho Benefits from Regularization?\nWhere is Regularization Applied?\nHow to Implement Regularization?\nChallenges in Regularization\nTrends in Regularization\nFrequently Asked Questions (FAQs)\nShare\nLink copied\nWhat is Regularization?\nBefore diving into the deep end, let's start with the basics. Regularization is a technique used to prevent overfitting in machine learning models, ensuring they perform well not just on the training data but on unseen data too.\nDefinition\nRegularization is a process applied during the training of a model that aims to reduce the model's complexity to prevent overfitting, ensuring a more generalized, robust performance.\nPurpose\nThe core purpose of regularization is to improve model generalization. Overfitting happens when a model learns the\ntraining data\ntoo well, including its noise and outliers, failing to perform on new, unseen data.\nHow It Works\nRegularization works by adding a penalty on the different\nparameters\nof the model to reduce the freedom of the model hence discouraging overly complex models.\nTypes\nThere are primarily two types of regularization techniques in machine learning:\nL1 regularization (Lasso) and L2 regularization (Ridge)\n. Each has its way of adding penalties and serves slightly different purposes.\nImportance\nThe importance of regularization lies in its ability to create models that generalize well, are less prone to overfitting, and perform reliably on unseen data.\nWhy is Regularization Critical?\nDelving deeper, let's discuss why regularization is not just an optional tweak but a critical necessity in machine learning models.\nPrevents Overfitting\nThe paramount reason for regularization is to avoid overfitting, ensuring that the model remains general and applicable to new data.\nEnhances Model Predictive Power\nBy preventing overfitting, regularization enhances the model's ability to predict accurately unseen data, which is the ultimate goal of any\nmachine learning model\n.\nBalances Bias-Variance Tradeoff\nRegularization helps in managing the bias-variance tradeoff, reducing variance without increasing bias too much, which in turn improves model performance.\nEncourages Feature Selection\nL1 regularization has the added benefit of feature selection by shrinking some coefficients to zero, effectively reducing the number of features the model considers.\nSimplifies Models\nRegularization simplifies models by encouraging smaller weights, making the models more interpretable, especially important in fields like healthcare or finance where model decisions need to be explained.\nBoost Your AI-Powered Chatbot Game!\nGet Started FREE\nWhen to Use Regularization?\nKnowing when to apply regularization can significantly impact your model's effectiveness.\nHigh Variance Scenarios\nWhenever your model is showing signs of high variance or overfitting( performs well on\ntraining data\nbut poorly on validation/test data), regularization should be considered.\nComplex Datasets\nFor complex\ndatasets\nwith many features, regularization can help in simplifying the model, making sure it generalizes well.\nLimited Data\nIn situations where the amount of data is limited, regularization can prevent the model from learning the noise in the training set.\nPrior to Model Training\nRegularization is a preventive measure, so it should ideally be incorporated during the model training phase, not after the model has been fully trained.\nModels Prone to Overfitting\nAlgorithms that are inherently more complex and prone to overfitting, such as deep learning networks, greatly benefit from regularization techniques.\nWho Benefits from Regularization?\nRegularization isn't a one-size-fits-all solution but it has its champions who can utilize it to its full potential.\nMachine Learning Practitioners\nData scientists and\nmachine learning\nengineers are the primary beneficiaries, using regularization to build robust models.\nIndustries with Complex Data\nSectors dealing with complex and high-dimensional data, like finance, healthcare, and e-commerce, can realize improved model performance with regularization.\nAcademics and Researchers\nResearchers in fields that utilize predictive modeling can use regularization to ensure their models are not overfitting and are generalizable.\nStudents Learning Machine Learning\nFor learners, understanding and applying regularization is a crucial step in grasping the nuances of building predictive models.\nCompanies Investing in AI\nOrganizations investing in AI-driven products benefit from regularization by ensuring their\nalgorithms\nperform reliably in real-world scenarios.\nWhere is Regularization Applied?\nLet's explore the arenas where regularization truly shines.\nPredictive Modeling\nIn any scenario where the goal is to predict future outcomes based on historical data, regularization helps in building models that generalize well.\nDeep Learning\nDeep learning networks, known for their complexity, are at high risk of overfitting. Regularization techniques, especially dropout, are essential here.\nNatural Language Processing (NLP)\nNLP\ntasks, often dealing with sparse\ndatasets\n, benefit from regularization to prevent models from overfitting to the\ntraining data\n.\nImage Recognition\nRegularization techniques are critical in image recognition tasks to ensure models can generalize from their training set to new, unseen images.\nFinancial Analysis\nIn finance, where predictive models are used for risk assessment, portfolio management, etc., regularization helps in dealing with inherently noisy financial data.\nHow to Implement Regularization?\nImplementing regularization correctly can make or break your machine learning model's performance.\nChoosing the Right Technique\nUnderstand the difference between L1, L2, and Elastic Net regularization to choose which best fits your model's needs.\nTuning Hyperparameters\nUtilize techniques like cross-validation to find the optimal lambda value that balances the complexity and performance of the model.\nFeature Scaling\nBefore applying regularization, it's crucial to scale your features since regularization is sensitive to the scale of input features.\nIntegrating with Model Training\nRegularization should be integrated as part of the model training process, not as an afterthought, to ensure it functions as intended.\nMonitoring Model Performance\nKeep an eye on how regularization affects your model's performance, making adjustments as necessary based on validation data feedback.\nChallenges in Regularization\nDespite its usefulness, regularization comes with its set of challenges that practitioners need to be aware of.\nChoosing the Right Lambda\nFinding the optimal regularization strength (lambda) can be tricky and requires careful tuning.\nRisk of Underfitting\nOver-regularization can lead to underfitting, where the model is too simple to capture the underlying pattern in the data.\nComputation Cost\nEspecially in large models, regularization can increase the computational cost due to the added complexity of calculation.\nBalancing L1 and L2 Regularization\nIn Elastic Net regularization, finding the right balance between L1 and L2 regularization can be challenging.\nFeature Selection Bias\nL1 regularization can introduce bias in feature selection, particularly in scenarios where there are correlations amongst features.\nTrends in Regularization\nThe field is constantly evolving, and staying atop trends is crucial.\nAutoML and Regularization\nAutomated Machine Learning (AutoML) platforms are incorporating intelligent regularization techniques that automatically adjust to model needs.\nAdvances in Deep Learning Regularization\nNew regularization techniques specific to deep learning, like dropout and batch normalization, are becoming standard practices.\nRegularization in Transfer Learning\nWith the rise of transfer learning, especially in deep learning, regularization plays a crucial role in adapting pre-trained models to new tasks.\nIntegration with Reinforcement Learning\nRegularization techniques are being explored in\nreinforcement learning\nto ensure policies learned by agents generalize across different environments.\nExpanding Beyond Traditional Methods\nResearch is ongoing into new forms of regularization that can cater to the unique needs of evolving\nmachine learning\nlandscapes, such as graph\nneural networks\nand federated learning.\nAs models become increasingly complex and data becomes ever more intricate, the principles of regularization remind us that sometimes, less is indeed more.\nApproached with thoughtfulness and a clear understanding of its nuances, regularization can be the linchpin in creating models that not only perform well on paper but stand the test of real-world application.\nBoost Your AI-Powered Chatbot Game!\nGet Started FREE\nFrequently Asked Questions (FAQs)\nHow does Regularization help in Machine Learning?\nRegularization helps prevent overfitting in machine learning by adding a penalty term to the loss function, discouraging overly complex models, and promoting generalization.\nWhat are the Common Types of Regularization Techniques?\nThe common types are L1 regularization (Lasso), L2 regularization (Ridge), and Elastic Net which is a combination of L1 and L2.\nCan Regularization Improve Model Performance?\nYes, regularization can improve the performance of a\nmachine learning\nmodel on unseen data by reducing its complexity and improving its generalization capacity.\nIs Regularization relevant for all Machine Learning Algorithms?\nNo, regularization is mostly relevant for parametric learning\nalgorithms\nwhere the model complexity can be controlled by adjusting the model\nparameters\n.\nWhat is the role of the Regularization Parameter?\nThe regularization parameter controls the strength of the penalty term. A higher value creates a simpler model, while a lower value permits a more complex model.\nBuild your first AI chatbot for FREE in just 5 minutes!\nGet Started FREE\nSurprise! BotPenguin has fun blogs too\nWe know you\u2019d love reading them, enjoy and learn.\nWhat is a WhatsApp Campaign? (With Real-World Examples)\nUpdated at Nov 15, 2024\n16 min to read\nBotPenguin\nContent Writer, BotPenguin\nA Comprehensive Look at Generative AI Use Cases Across Industries\nUpdated at Nov 14, 2024\n14 min to read\nManish Goyal\nAI Technical Lead, BotPenguin\nHow Generative AI Models Help in Enhancing Chatbot Conversations\nUpdated at Nov 14, 2024\n8 min to read\nManish Goyal\nAI Technical Lead, BotPenguin\nTable of Contents\nWhat is Regularization?\nWhy is Regularization Critical?\nWhen to Use Regularization?\nWho Benefits from Regularization?\nWhere is Regularization Applied?\nHow to Implement Regularization?\nChallenges in Regularization\nTrends in Regularization\nFrequently Asked Questions (FAQs)\nBotPenguin is the best AI Chatbot maker platform. Create a Chatbot for WhatsApp, Website, Facebook Messenger, Telegram, WordPress & Shopify with BotPenguin - 100% FREE! Our chatbot creator helps with lead generation, appointment booking, customer support, marketing automation, WhatsApp & Facebook Automation for businesses. AI-powered No-Code chatbot maker with live chat plugin & ChatGPT integration.\nMobile app\niOS App\nAndroid App\nFully Operational\nStatus\nMobile app\niOS App\nAndroid App\nFully Operational\nStatus\nPlatforms\nMicrosoft Teams\nNew\nInstagram\nNew\nWhatsapp\nTelegram\nFacebook\nWebsites\nPartners\nWhitelabel Partner\nWhatsapp Whitelabel Partner\nImplementation Partner\nAffiliate Partner\nGet Started\nChatbot For Ecommerce\nChatbot For Real Estate\nChatbot For Education\nChatbot For Travel\nAll Templates\nFree Tools\nWhatsApp Link Generator\nWhatsApp QR Code Generator\nOpenAI API Pricing Calculator\nChatbot ROI Calculator\nAll Free Tools\nIntegrations\nChatGPT\nGoHighLevel\nBitrix 24\nZoho CRM\nZapier\nAll Integrations\nResources\nBlogs\nGlossary\nHelp Center\nWrite for us\nContact Us\nWhat\u2019s New\nProduct Updates\nComparisons\nBotPenguin vs Manychat\nBotPenguin vs Tidio\nBotPenguin vs Tawk.to\nBotPenguin vs Wati\nBotPenguin vs Landbot\nAll Comparisons\nAlternatives\nWhat you get\nLead Generation Bot\nSupport Bot\nAssistant Bot\nPlatforms\nMicrosoft Teams\nNew\nInstagram\nNew\nWhatsapp\nTelegram\nFacebook\nWebsites\nGet Started\nIntegrations\nComparisons\nPartners\nFree Tools\nResources\nWhat you get\nTerms & Conditions\nPrivacy Policy\nSecurity\nGDPR\nRefund Policy\nTerms & Conditions\nPrivacy Policy\nSecurity\nGDPR\nRefund Policy\nCopyright \u00a9 2018 - 2024 Relinns Technologies Pvt. Ltd. All RightsReserved.", "embedding": [-0.11500664800405502, -0.056784696877002716, 0.04081873223185539, -0.008847287856042385, -0.026944564655423164, -0.025984108448028564, 0.04052869603037834, 0.053320758044719696, -0.053649429231882095, -0.008710086345672607, 0.03411087393760681, 0.026131926104426384, 0.0038128728047013283, 0.008104774169623852, 0.09151943773031235, -0.01699962466955185, 0.09366486966609955, -0.0699390396475792, -0.13441313803195953, -0.048188429325819016, 0.005235053598880768, -0.029940877109766006, 0.04852670058608055, -0.012417738325893879, -0.018758703023195267, -0.02926727384328842, -0.010668634437024593, -0.0030860514380037785, -0.0005301260971464217, -0.08362477272748947, -0.026773354038596153, 0.030634980648756027, 0.010908680967986584, 0.06600143015384674, -0.03349830210208893, -0.04548327624797821, -0.033257465809583664, 0.016120009124279022, 0.044441476464271545, -0.029723389074206352, -0.07201831787824631, -0.08430232107639313, -0.07317342609167099, -0.010863563045859337, 0.0889311134815216, -0.01796596311032772, -0.07584360241889954, 0.008831421844661236, 0.0051567391492426395, 0.06135120242834091, -0.05150991678237915, -0.06794991344213486, 0.0221460722386837, 0.05250730738043785, -0.015342320315539837, 0.043059226125478745, -0.03574270382523537, 0.04914389178156853, 0.058284204453229904, -0.03324096277356148, -0.050053924322128296, -0.04115747660398483, 0.020776355639100075, 0.04420660808682442, -0.02628837153315544, -0.004262610804289579, -0.10957266390323639, -0.013733910396695137, 0.01810458116233349, -0.01266219187527895, -0.06996527314186096, 0.014955624006688595, -0.022184783592820168, 0.11303488165140152, 0.04410548135638237, -0.014038283377885818, -0.022560421377420425, -0.009621446020901203, -0.011403288692235947, -0.06629819422960281, 0.0018224481027573347, 0.011618525721132755, 0.01498560979962349, 0.07493621110916138, 0.021567758172750473, -0.009474534541368484, -0.032206397503614426, 0.010678866878151894, -0.006872342899441719, -0.009616382420063019, 0.03182883933186531, -0.006080521270632744, 0.043240025639534, -0.0020799210760742426, -0.0014311595587059855, 0.013342957012355328, -0.05784261226654053, 7.634617213625461e-05, -0.03545086458325386, 0.025908686220645905, 0.01378755085170269, -0.05582374334335327, -0.04470435529947281, -0.042465467005968094, -0.013691382482647896, -0.00297459471039474, 0.06959230452775955, -0.006169838365167379, 0.1583520472049713, 0.016567684710025787, -0.10941527783870697, -0.04763507470488548, 0.02251425012946129, -0.02049090899527073, -0.025648698210716248, -0.01130611076951027, 0.006801459938287735, 0.04073372483253479, 0.12618905305862427, 0.03268655762076378, 0.05460844188928604, 0.04271766543388367, 0.011879262514412403, -0.025508420541882515, 0.04358804598450661, 0.07521291822195053, 0.024631155654788017, 1.0529353261588388e-32, -0.0296524316072464, 0.029959961771965027, -0.07470715790987015, 0.06585639715194702, -0.021301008760929108, 0.025984542444348335, -0.011239629238843918, 0.044629424810409546, -0.07404371351003647, -0.0378972664475441, -0.06356634199619293, 0.1200685128569603, -0.07841258496046066, 0.07685881853103638, 0.03380732238292694, -0.08275619149208069, -0.0044745104387402534, 0.054481297731399536, 0.07561927288770676, 0.004024095367640257, 0.09297966957092285, 0.011024931445717812, 0.0676150768995285, 0.06296279281377792, 0.1226988136768341, 0.00945957563817501, 0.09051550179719925, -0.003283285303041339, 0.06429315358400345, 0.032845836132764816, -0.05445442348718643, -0.009805058129131794, -0.042901139706373215, 0.011879777535796165, -0.017179101705551147, -0.022328579798340797, 0.0033818671945482492, -0.12393781542778015, -0.06609033793210983, 0.03414895385503769, -0.14352433383464813, 0.004135093651711941, -0.10510102659463882, -0.09217537194490433, 0.03571653738617897, 0.006166978273540735, 0.026863539591431618, 0.022346245124936104, 0.03525855019688606, -0.027252672240138054, -0.04310985282063484, 0.031051326543092728, 0.010859206318855286, 0.046444009989500046, -0.03444645553827286, -0.04850286245346069, 0.027643881738185883, -0.045110657811164856, 0.011083655059337616, -0.017689401283860207, 0.007205539848655462, -0.028060808777809143, -0.012172088027000427, -0.003910281229764223, 0.026777874678373337, 0.01767555996775627, 0.05730443075299263, 0.0353943333029747, 0.004690682049840689, 0.038315191864967346, 0.06868433952331543, 0.04434647411108017, -0.01887364871799946, 0.019740501418709755, -0.02408413589000702, -0.011452225968241692, -0.0625937432050705, 0.019831912592053413, -0.0513467863202095, -0.03129704296588898, -0.06716098636388779, 0.01977165788412094, -0.03425056114792824, -0.010857454501092434, 0.025227628648281097, -0.04693398252129555, 0.06018306687474251, -0.06010979041457176, -0.002554417122155428, 0.012983463704586029, -0.01092290785163641, 0.07583244144916534, -0.0782146006822586, 0.0764070525765419, -0.03253145515918732, -8.545865236313139e-33, -0.03966014087200165, 0.02971666119992733, -0.07063497602939606, 0.11658811569213867, 0.0018187806708738208, -0.03245894983410835, -0.007841016165912151, -0.010330108925700188, 0.09745404869318008, -0.03297277167439461, -0.04459555447101593, -0.026192590594291687, 0.009539228864014149, -0.04424634203314781, -0.021950192749500275, 0.016846295446157455, -0.0753980204463005, -0.006460999138653278, -0.00014877547801006585, 0.02196548692882061, 0.006075478624552488, 0.03893432393670082, -0.14220821857452393, -0.009023493155837059, 0.014786956831812859, 0.017254319041967392, -0.10713965445756912, 0.0634598657488823, 0.028864683583378792, 0.01674065552651882, -0.006148072425276041, -0.012728933244943619, -0.05996468663215637, -0.03899877518415451, 0.004480007570236921, 0.05511659383773804, -0.014550607651472092, 0.03166722133755684, -0.012186877429485321, -0.05347515270113945, 0.09234041720628738, -0.047429174184799194, -0.04983653128147125, -0.07396440207958221, 0.0008855357300490141, 0.026758523657917976, -0.14845964312553406, -0.05798255279660225, -0.05928560346364975, 0.015620867721736431, 0.06691478192806244, -0.009810399264097214, 0.06015010550618172, -0.03151583671569824, -0.10764827579259872, -0.004237221088260412, 0.0988333597779274, 0.019123563542962074, -0.11461437493562698, 0.03848002105951309, 0.024087924510240555, -0.005544615909457207, 0.08638853579759598, 0.08928519487380981, 0.026660991832613945, -0.0011509888572618365, 0.0631185919046402, 0.012400742620229721, -0.0006803410942666233, -0.10916981101036072, 0.028142765164375305, -0.06402254104614258, -0.02423357591032982, 0.028082912787795067, 0.0037520958576351404, 0.05857200175523758, 0.05401268228888512, -0.10674829035997391, 0.0038378117606043816, 0.020820986479520798, -0.06835521012544632, 0.008164959028363228, 0.05724674463272095, 0.03589498624205589, -0.0769750252366066, 0.06675027310848236, -0.040410835295915604, 0.045238982886075974, 0.013324511237442493, 0.046833112835884094, -0.002684758510440588, 0.014101923443377018, -0.022697554901242256, 0.06657824665307999, -0.017262931913137436, -5.875310904457365e-08, -0.08271826803684235, -0.015912165865302086, 0.041402485221624374, 0.08324962854385376, 0.027851643040776253, -0.07012429088354111, -0.030665326863527298, 0.07147417217493057, 0.02895953319966793, 0.02580220252275467, 0.027112873271107674, -0.027126695960760117, -0.07464474439620972, 0.02945198491215706, 0.07038037478923798, 0.026314863935112953, -0.0021928809583187103, 0.03026554360985756, -0.0058778515085577965, -0.0075642745941877365, 0.008951185271143913, 0.0530974343419075, -0.040826793760061264, -0.03841230645775795, -0.0010398250306025147, -0.037645965814590454, -0.031562499701976776, 0.11618329584598541, -0.022687964141368866, 0.0070273722521960735, -0.033318713307380676, -0.0011491787154227495, 0.0615340992808342, -0.05600506067276001, -0.03212648630142212, 0.029827963560819626, -0.06751060485839844, -0.07223334163427353, 0.0016489499248564243, 0.030456846579909325, 0.026450464501976967, -0.004883224610239267, 0.04397362098097801, -0.08597245812416077, 0.0051034907810389996, -0.0686243548989296, -0.05115509033203125, -0.10307730734348297, 0.05116412416100502, -0.02448667772114277, -0.01794501580297947, -0.002422516932711005, 0.08227746188640594, 0.04157346487045288, 0.11491375416517258, -0.006146196275949478, 0.05616598576307297, -0.029294217005372047, 0.07057815045118332, 0.09591305255889893, 0.03261110559105873, 0.020524753257632256, 0.03483216091990471, 0.037544719874858856]}