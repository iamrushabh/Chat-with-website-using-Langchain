{"file_name": "botpenguin.com_glossary_transformer-architecture", "text": "URL: https://botpenguin.com/glossary/transformer-architecture\nTransformer Architecture: Key Components, Use Cases & Future\nWhy BotPenguin\nProduct\nSolutions\nPricing\nPartners\nResources\nLogin\nGet Started FREE\nIntegrations\nExperience 80+ world-class integrations.\nKey Features\nTake your business to the next level with our awesome key features.\nLive Chat\nStay in the loop with your clientele 24*7!\nUnified Inbox\nServe your customers across all platforms.\nAnalytics\nSpeedtrack your growth with our top-of-the-line analytics tools!\nMobile App\nMake, monitor, & manage your AI chatbots with our mobile app.\nCHATBOT COMPARISONS\nBotPenguin vs. Manychat\nBotPenguin vs. Tidio\nBotPenguin vs. Tawk.to\nBotPenguin vs. Wati\nBotPenguin vs. Interakt\nBotPenguin vs. AiSensy\nBotPenguin vs. Landbot\nWHAT CAN IT DO?\nMarketing Automation\nMake marketing a boon from the automation gods!\nFB Automation\nEngage with your customers on a deeper level.\nWhatsApp Automation\nGet that nifty automation for WhatsApp too!\nAppointment Bookings\nNo more delays, BotPenguin\u2019s got you here!\nCustomer Support\nYour customers are in for a treat with this automation.\nLead Generation\nGain more lead without any extra effort or expenses\nWHO CAN USE IT?\nHealthcare\nGive your patients world-class healthcare service!\nEducation\nMake admissions and automate processes in a jiffy!\nE-commerce\nCreate the best E-commerce service with ease!\nReal Estate\nMake Real Estate great again with BotPenguin!\nConsultants\nBoost up with our one-stop marketing solution!\nSaaS\nTake your SAAS game to the next level with BotPenguin!\nTours & Travels\nProvide extraordinary tour and travel services with BotPenguin!\nInsurance\nLaunch AI-driven Insurance Bot to Promote, Sell, & Manage Policies.\nWHERE CAN IT RUN?\nInstagram Chatbot\nAttract leads, boost sales, and chat 24/7 with Instagram Chatbots.\nWhatsApp Chatbot\nStart conversing like a real person with BotPenguin!\nTelegram Chatbot\nCutting-edge features for you to grow your business on Telegram.\nFacebook Chatbot\nDo everything at once with BotPenguin on Facebook.\nWebsites Chatbot\nBotPenguin grows your website and keeps your customers engaged.\nWordpress Chatbot\nBotPenguin thrives on WordPress and makes it awesome.\nMicrosoft Teams Chatbot\nMaximize your teams productivity with MS Teams Bot.\nShopify Chatbot\nBoost your Shopify Business With BotPenguin\u2019s AI-powered chatbot.\nWooCommerce Chatbot\nSell effortlessly on your WooCommerce store with BotPenguin.\nSquarespace Chatbot\nGet the most out of Squarespace with BotPenguin\nCUSTOM DEVELOPMENT\nWhitelabel ChatGPT\nApply your branding on ChatGPT, Launch your own AI platform\nChatGPT Custom Plugins\nIntegrate your service straight into ChatGPT\nCustom Chatbot Development\nBuild enterprise-grade chatbots with the best\nChatGPT Clone\nAdd functionality and branding on ChatGPT\nHIRE DEVELOPERS\nChatbot Developers\nBuild Lighter, Faster, Smarter-Efficiently\nChatGPT Developers\nRide the GPT wave with trained surfers\nChatGPT Consultants\nAdvice that makes the difference in your AI journey\nPARTNER PROGRAMS\nPartners Home\nJoin hands with us, and welcome growth\nWhatsApp Whitelabel Partners\nConquer the WhatsApp land with BotPenguin\u2019s White Label Platform\nWhitelabel Partners\nSay hi to the best Whitelabel chatbot platform ever\nAffiliate Partners\nEarn more and keep your clients happier\nImplementation Partners\nAs they say, a partner is worth trillions!\nPARTNER PRICING\nWhitelabel Chatbot Pricing\nOur pricing for Whitelabel Chatbot\nImplementation Partnership Pricing\nOur pricing for Implementation Partnership\nOUR RESOURCES\nBlogs\nRead the latest blogs on chatbots, AI, automations & more\nVideos\nWatch tutorials, webinars, and demos to master our chatbots.\nCase Study\nRead how BotPenguin transformed business communication\nE-book\nExplore e-books written by experts for all your business needs!\nHelp Docs\nFind detailed guides and tips for all your chatbot needs.\nNewsroom\nExplore how BotPenguin is making headlines in the chatbot industry.\nCommunity Support\nJoin our vibrant community to unlock exclusive content & expert guidance\nLATEST BLOG\nWhy is BotPenguin the best platform to develop a chatbot?\nIntroducing ChatGPT 4o for BotPenguin\nCreate your first AI Chatbot\nGet Started FREE\nGLOSSARY\nTransformer Architecture\nTable of Contents\nWhat is the Transformer Architecture?\nCore Components of The Transformer\nSalient Features of Transformer Architecture\nThe Inner Workings of the Transformer\nApplications of Transformer Models\nLimitations of Transformer Architecture\nAddressing the Limitations\nFuture of Transformer Architecture\nFrequently Asked Questions (FAQs)\nShare\nLink copied\nWhat is the Transformer Architecture?\nTransformer architecture is a model architecture used predominantly in Natural Language Processing (NLP). Introduced in the ground-breaking paper \"Attention is all you need\" by Vaswani et al., the model leverages attention mechanisms to capture different kinds of relationships in input data without depending on time-series data.\nOrigins of Transformer Architecture\nThe architecture was introduced as a solution to the limitations of Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs), which were commonly used for NLP tasks. The transformer model provides more efficient and context-sensitive language understanding than its predecessors.\nComponents of Transformer Architecture\nThe transformer model is composed of an encoder and a decoder, each of which is made up of several identical layers. The ability to process all words or symbols in parallel while capturing the context gives the architecture its edge.\nRole in NLP\nTransformer Architecture plays a highly significant role in a multitude of NLP tasks, including machine translation, text generation,\nnamed entity recognition\n, and more. Its remarkable feature of considering context through self-attention makes it apt for language processing tasks.\nSupercharge Your Chatbot with Automation Today!\nGet Started FREE\nCore Components of The Transformer\nMoving a step further, let's explore the key components that construct the amazing Transformer Architecture.\nEncoders\nEach encoder in the architecture consists of a self-attention layer and a feed-forward\nneural network.\nThe encoders consume the input sequence and encode the information into a continuous representation which carries throughout the model.\nDecoders\nThe decoders use the encoder's entire output to focus the attention mechanism at appropriate places in the input sequence. The target output sequence is generated using the encoded input and its own input.\nSelf-Attention Mechanism\nAt the core of the Transformer's success is the self-attention mechanism. It is through this mechanism that the model computes context-aware representations of words or symbols in the sequence by considering all words for every word.\nPositional Encoding\nDespite its many strengths, the Transformer inherently lacks knowledge of the order or position of words. Positional encoding is embedded in the sequence to retain the order of words, effectively compensating for the model's shortcomings.\nSalient Features of Transformer Architecture\nTo truly understand the Transformer, we need to unearth the unique features that truly make it stand out amongst others.\nParallelization Capabilities\nUnlike RNNs, Transformers deal with entire sequences at once, allowing model training to be parallelized. This feature significantly reduces training durations, making Transformers a preferred choice for large-scale language models.\nHandling of Long-Term Dependencies\nTransformers, with their self-attention mechanism, can manage long-term dependencies much better than RNNs can. The architecture efficiently highlights relationships between words irrespective of their positions in the sequence.\nScalability Advantage\nLarge-scale language models like GPT-3 and\nBERT\nutilize the Transformer architecture, demonstrating its excellent scalability. These models have achieved impressive results on several NLP tasks, including translation and text completion.\nFlexibility for Various Tasks\nTransformers can be used for various tasks in machine translation and beyond. With slight alterations, Transformer models can be fine-tuned for text classification,\nsentiment analysis\n, summarization, and more.\nThe Inner Workings of the Transformer\nLet's dive deep into the inner mechanisms of the Transformer architecture and understand its working.\n1. Input Representation\nTransformers process input data in the form of sequences. Each word or token in the sequence is converted into a fixed-size vector representation using an embedding layer. These vectors serve as the input for the encoder layers.\n2. Positional Encoding\nA crucial step in the Transformer's workings is incorporating positional information, as the model doesn't have inherent knowledge of word order. Positional encodings, usually sine and cosine functions, are added to the input word embeddings to capture word positions and enable the model to learn sequence patterns.\n3. Multi-Head Self-Attention Mechanism\nThe heart of the Transformer is the multi-head self-attention mechanism. In this step, the model calculates attention scores by comparing each world against all others in the sequence. The attention scores are then used to weigh the importance of individual words and compute a context-aware representation for each position.\nThe self-attention mechanism is performed multiple times in parallel within the architecture, known as the multi-head setup. This setup allows the model to capture different types of relationships between words and better understand the context.\n4. Normalization and Feedforward\nAfter multi-head self-attention computation, layer normalization is applied in both the encoder and decoder branches. It stabilizes the layer inputs and accelerates the training process. Then, each normalized output enters a position-wise feedforward\nneural network\nthat further refines the representation and helps the model learn high-level features of the input text.\n5. Decoder in Action\nIn the decoder, a masked multi-head self-attention mechanism prevents the model from attending to the \"future\" tokens in the target sequence. Then, a second multi-head attention layer connects the output of the decoder's self-attention layer with the encoder's outputs, forming a bridge between the context from the source text and the generated target text. This is followed by another layer of normalization, position-wise feedforward, and normalization steps.\n6. Output Generation\nThe final step in the Transformer's workings is generating the output sequence. Utilizing a linear layer followed by a\nsoftmax function\n, the transformer assigns probabilities to words in the target vocabulary. The highest probability word is selected as the next word in the output sequence. This process continues until a designated end-of-sequence token is generated or a maximum sequence length is reached.\nApplications of Transformer Models\nThe applications of Transformer models illustrate their utility in real-world scenarios. Let's uncover some of these applications.\nMachine Translation\nThe inaugural application of\nTransformer\narchitecture was in machine translation. Here, it excels with its ability to address long-distance relationships between components of language.\nText Summarization\nTransformers have proven to be an extraordinary tool for text summarization tasks. By understanding the context and prioritizing salient points, they generate coherent and informative summaries.\nText Generation\nTechnologies like chatbots, autocompletion tools, and\nAI\nwriting assistants use Transformer models for text generation, leveraging their ability to predict the next word in a sequence based on context.\nQuestion-Answering Tasks\nIn a question-answering setup, Transformers shine due to their proficiency in understanding the context. Models fine-tuned on SQuAD, an expansive question-answering\ndataset\n, have achieved human-level accuracy on these tasks.\nLimitations of Transformer Architecture\nNo systems are perfect, and the Transformer is no exception. Let's consider some of the limitations of this architecture.\nStruggle with Sequential Data\nAlthough Transformers have their strengths, they aren't the best fit for tasks that require understanding the precise timing and sequence of events, such as in music classification or generative tasks.\nHigh Resource Requirement\nTransformers, especially larger models, tend to have a high demand for computational resources, making them expensive and difficult to train.\nLack of Transparency\nLike many\ndeep learning models\n, Transformers are often considered black-box models, and their decision-making process can be hard to interpret.\nPossibility of Overfitting\nWith a larger number of parameters, Transformers bear the risk of overfitting, especially when dealing with limited\ntraining data\n.\nAddressing the Limitations\nIn spite of the limitations, with the right strategies, the benefits of Transformers can still be harnessed effectively.\nHybrid Models\nFor tasks where sequential understanding is crucial, hybrid models incorporating both Transformers and\nRNNs\nare being explored.\nEfficient Training Practices\nPractices like model pruning, knowledge distillation, and hardware optimization can mitigate resource concerns while training Transformers.\nVisualization Tools\nTo address transparency issues, different tools are being developed to visualize the attention mechanism within Transformers.\nRegularization Techniques\nImplementing regularization techniques during model training can help ward off overfitting and improve the model's stability and robustness.\nFuture of Transformer Architecture\nHaving covered numerous aspects of the present-day Transformer, it's time to ponder about its future prospects.\nHeralding Bigger Models\nWith continued advancements in hardware, expect to see even larger Transformer models that house more parameters and deliver richer language understanding.\nLeveraging in Other Domains\nWhile NLP has been the primary playground for Transformers, there is significant potential for their application in other domains, such as image recognition and genomic sequence prediction.\nAiding Low-Resource Languages\nTransformers could be instrumental in enhancing\nNLP\ntasks for low-resource languages. By deploying techniques like cross-lingual training, their comprehensiveness can be improved significantly.\nExploring Local Attention Mechanisms\nWhile current Transformers use global attention, in the future, models with local attention mechanisms could provide efficiency gains in tackling large sequences of data.\nIn conclusion, the Transformer architecture, a relatively new entrant in the NLP landscape, has revolutionized language understanding and NLP tasks. With its fascinating design and promising results, it certainly holds a thrilling future.\nConnect, Communicate, Convert\nTry BotPenguin\nFrequently Asked Questions (FAQs)\nHow do Transformers capture long-range dependencies in data?\nTransformers leverage the self-attention mechanism to assign different weights to different words, allowing them to focus on relevant parts of the sequence and capture long-range dependencies effectively.\nAre Transformers faster than recurrent neural networks?\nYes, Transformers are faster than r\necurrent neural networks\nas they process the entire sequence in parallel. This parallelization speeds up training and inference times, making them more efficient for large-scale applications.\nCan Transformers handle non-sequential data?\nWhile Transformers are optimized for sequential processing, they can be adapted to handle non-sequential data. However, additional adaptations or hybrid models might be needed to ensure their effectiveness in such cases.\nHow much data do Transformers require for optimal performance?\nTransformers typically require large amounts of data to achieve optimal performance. Pre-training on large-scale\ndatasets\nhas been found to significantly improve their performance, making them less suitable for tasks with limited labeled data.\nWhat are the limitations of Transformer architecture?\nSome limitations of Transformer architecture include its computational complexity, data requirements, and potential challenges in handling non-sequential data. Understanding these limitations is essential when considering the suitability of Transformers for specific applications.\nBuild your first AI chatbot for FREE in just 5 minutes!\nGet Started FREE\nSurprise! BotPenguin has fun blogs too\nWe know you\u2019d love reading them, enjoy and learn.\nWhat is a WhatsApp Campaign? (With Real-World Examples)\nUpdated at Nov 15, 2024\n16 min to read\nBotPenguin\nContent Writer, BotPenguin\nA Comprehensive Look at Generative AI Use Cases Across Industries\nUpdated at Nov 14, 2024\n14 min to read\nManish Goyal\nAI Technical Lead, BotPenguin\nHow Generative AI Models Help in Enhancing Chatbot Conversations\nUpdated at Nov 14, 2024\n8 min to read\nManish Goyal\nAI Technical Lead, BotPenguin\nTable of Contents\nWhat is the Transformer Architecture?\nCore Components of The Transformer\nSalient Features of Transformer Architecture\nThe Inner Workings of the Transformer\nApplications of Transformer Models\nLimitations of Transformer Architecture\nAddressing the Limitations\nFuture of Transformer Architecture\nFrequently Asked Questions (FAQs)\nBotPenguin is the best AI Chatbot maker platform. Create a Chatbot for WhatsApp, Website, Facebook Messenger, Telegram, WordPress & Shopify with BotPenguin - 100% FREE! Our chatbot creator helps with lead generation, appointment booking, customer support, marketing automation, WhatsApp & Facebook Automation for businesses. AI-powered No-Code chatbot maker with live chat plugin & ChatGPT integration.\nMobile app\niOS App\nAndroid App\nFully Operational\nStatus\nMobile app\niOS App\nAndroid App\nFully Operational\nStatus\nPlatforms\nMicrosoft Teams\nNew\nInstagram\nNew\nWhatsapp\nTelegram\nFacebook\nWebsites\nPartners\nWhitelabel Partner\nWhatsapp Whitelabel Partner\nImplementation Partner\nAffiliate Partner\nGet Started\nChatbot For Ecommerce\nChatbot For Real Estate\nChatbot For Education\nChatbot For Travel\nAll Templates\nFree Tools\nWhatsApp Link Generator\nWhatsApp QR Code Generator\nOpenAI API Pricing Calculator\nChatbot ROI Calculator\nAll Free Tools\nIntegrations\nChatGPT\nGoHighLevel\nBitrix 24\nZoho CRM\nZapier\nAll Integrations\nResources\nBlogs\nGlossary\nHelp Center\nWrite for us\nContact Us\nWhat\u2019s New\nProduct Updates\nComparisons\nBotPenguin vs Manychat\nBotPenguin vs Tidio\nBotPenguin vs Tawk.to\nBotPenguin vs Wati\nBotPenguin vs Landbot\nAll Comparisons\nAlternatives\nWhat you get\nLead Generation Bot\nSupport Bot\nAssistant Bot\nPlatforms\nMicrosoft Teams\nNew\nInstagram\nNew\nWhatsapp\nTelegram\nFacebook\nWebsites\nGet Started\nIntegrations\nComparisons\nPartners\nFree Tools\nResources\nWhat you get\nTerms & Conditions\nPrivacy Policy\nSecurity\nGDPR\nRefund Policy\nTerms & Conditions\nPrivacy Policy\nSecurity\nGDPR\nRefund Policy\nCopyright \u00a9 2018 - 2024 Relinns Technologies Pvt. Ltd. All RightsReserved.", "embedding": [-0.12343394756317139, -0.052515141665935516, 0.01736956089735031, -0.021230557933449745, -0.02691064402461052, -0.04586699232459068, 0.013264849781990051, 0.09452512115240097, -0.002083504106849432, 0.015194740146398544, 0.03123440220952034, -0.006227942183613777, 0.0066531081683933735, 0.029118696227669716, 0.11618972569704056, -0.0003379964909981936, 0.08661501109600067, -0.09366638958454132, -0.0828094556927681, -0.04258621111512184, -0.005989979952573776, -0.013623136095702648, 0.017880728468298912, -0.04451747238636017, -0.01924925111234188, -0.015444297343492508, -0.009984718635678291, -0.034394267946481705, -0.015930840745568275, -0.06588191539049149, -0.011452349834144115, 0.02309831604361534, -0.026136837899684906, 0.06705863773822784, -0.026177167892456055, -0.027918847277760506, -0.002736143535003066, 0.005825493950396776, 0.06973189860582352, -0.05686289444565773, -0.06027248874306679, -0.09558731317520142, -0.05068200081586838, -0.008229774422943592, 0.08572027832269669, -0.0011382023803889751, -0.05445769429206848, 0.010747082531452179, 0.004930081311613321, 0.07179168611764908, -0.07037074863910675, -0.06979068368673325, 0.030087921768426895, 0.08529725670814514, -0.061715755611658096, 0.06876707822084427, -0.00597793236374855, 0.002056816127151251, 0.03272385150194168, -0.027373995631933212, -0.053636886179447174, 0.015696052461862564, 0.046193428337574005, 0.04206803813576698, -0.045400917530059814, 0.028767984360456467, -0.08025410771369934, -0.0090524572879076, -0.002982549602165818, -0.056546904146671295, -0.020303450524806976, -0.0455879308283329, -0.026340968906879425, 0.06280282884836197, 0.01585714891552925, -0.04509836807847023, 0.028378523886203766, -0.013393159955739975, 0.034057408571243286, -0.05960371717810631, 0.01229463517665863, 0.06354907155036926, -0.018724538385868073, 0.08669650554656982, -0.03282274305820465, -0.03403148427605629, -0.022972462698817253, 0.02517189085483551, -0.04341008886694908, 0.0017750225961208344, 0.03061818517744541, -0.007881500758230686, 0.047428883612155914, 0.01187150739133358, 0.04510665312409401, 0.020223181694746017, -0.070172019302845, 0.021651916205883026, -0.05584869906306267, -0.003045855788514018, -0.0093621127307415, -0.05316731333732605, -0.0247044675052166, -0.07227057963609695, -0.06723853945732117, -0.013047612272202969, 0.04310782998800278, -0.011440888047218323, 0.10463371127843857, 0.00483442610129714, -0.11303440481424332, -0.04461129009723663, 0.0036531519144773483, -0.04775073379278183, 0.023799575865268707, 0.0027689298149198294, -0.04215288907289505, 0.07222636044025421, 0.12864279747009277, 0.02445860020816326, 0.06888939440250397, 0.07127119600772858, -0.00935453549027443, 0.013685439713299274, 0.06218741834163666, 0.056602250784635544, -0.015304092317819595, 1.0933207570124832e-32, -0.05189849063754082, 0.041774947196245193, -0.06065487861633301, 0.09752427041530609, 0.0308152474462986, 0.020746519789099693, 0.031598083674907684, 0.04819302633404732, -0.0536235049366951, -0.023880237713456154, -0.08969519287347794, 0.12780232727527618, -0.08682268857955933, 0.03141588345170021, 0.034409694373607635, -0.09003661572933197, -0.024840623140335083, 0.054117705672979355, 0.05397076904773712, 0.0028192629106342793, 0.07235180586576462, -0.02085268869996071, 0.0688471868634224, 0.07862145453691483, 0.11977224797010422, 0.021396638825535774, 0.06809026747941971, 0.0007872144924476743, 0.020933184772729874, 0.020402943715453148, -0.09238248318433762, 0.023270437493920326, -0.05778379738330841, 0.0013833525590598583, -0.039339154958724976, -0.053551085293293, -0.04468931257724762, -0.13497373461723328, -0.07977031171321869, 0.029993940144777298, -0.13679887354373932, 0.02643582783639431, -0.10812819004058838, -0.08058304339647293, 0.05505694821476936, 0.020607341080904007, 0.029851537197828293, 0.02162153273820877, 0.04914100468158722, 0.010607752948999405, -0.028366362676024437, 0.02796240709722042, 0.009611929766833782, 0.08308669924736023, 0.03805052489042282, -0.03549060598015785, 0.05347193405032158, -0.04659904167056084, 0.015606597065925598, -0.021950704976916313, -0.03143385052680969, -0.008910520002245903, -0.016035353764891624, -0.015570882707834244, 0.07221047580242157, -0.004259166773408651, 0.05618530884385109, 0.02373943291604519, 0.019225256517529488, 0.02327861450612545, 0.03557450696825981, 0.028414616361260414, -0.021424369886517525, 0.03135043755173683, -0.04490238428115845, 0.005639540031552315, -0.1017029881477356, 0.049419257789850235, -0.06234678998589516, -0.01910519041121006, -0.056927088648080826, 0.010182365775108337, -0.006299624685198069, -0.0249550212174654, 0.08084400743246078, -0.01183425821363926, 0.044933781027793884, -0.0368669293820858, -0.028269382193684578, 0.04530949145555496, -0.03909262269735336, 0.08279501646757126, -0.07299723476171494, 0.07632920145988464, -0.04095575585961342, -8.637418613826933e-33, -0.052223458886146545, 0.0010727491462603211, -0.06967784464359283, 0.09293870627880096, 0.017763353884220123, -0.045298982411623, 0.03386625275015831, -0.03859079256653786, 0.09968458116054535, -0.00024623129866085947, -0.08473086357116699, -0.016487644985318184, 0.0328095369040966, -0.040708091109991074, 0.008176940493285656, 0.01381163764744997, -0.06567377597093582, -0.06557748466730118, 0.00714702345430851, 0.0054778531193733215, -0.010007418692111969, 0.07247254252433777, -0.14005614817142487, -0.0029346472583711147, -0.022699790075421333, 0.0362887978553772, -0.10026596486568451, 0.05872324854135513, 0.046832650899887085, 0.00804298184812069, -0.01884840987622738, -0.007945667952299118, -0.0010884872172027826, -0.004277684725821018, 0.02056845836341381, 0.04416324943304062, 0.014847353100776672, -0.017134401947259903, -0.0026287867221981287, -0.0488857701420784, 0.07476761192083359, -0.061537668108940125, -0.05052235350012779, -0.05118709057569504, -0.018937615677714348, 0.02659105882048607, -0.13033397495746613, -0.03738601133227348, -0.06418375670909882, 0.02348422072827816, 0.05166090279817581, 0.01651090756058693, 0.024263929575681686, -0.05960823968052864, -0.07819972932338715, -0.0412350632250309, 0.09822466224431992, -0.006069005001336336, -0.06821833550930023, 0.01052379235625267, 0.051034729927778244, 0.01074150763452053, 0.08716791123151779, 0.045111898332834244, 0.026458190754055977, -0.007223708089441061, 0.061324506998062134, 0.02408415637910366, 0.005489672068506479, -0.06771861016750336, 0.052014898508787155, -0.05241861566901207, -0.00698894215747714, -0.014306473545730114, 0.02700749598443508, 0.06383058428764343, 0.04673385992646217, -0.09751050174236298, 0.019636092707514763, -0.026203801855444908, -0.09532561153173447, 0.006606261245906353, 0.047550980001688004, 0.051954761147499084, -0.0709969624876976, 0.06992705911397934, -0.0033469772897660732, -0.017458682879805565, 0.014215203933417797, 0.018748456612229347, -0.05646386370062828, 0.032240815460681915, -0.02343222312629223, 0.10038967430591583, 0.008765507489442825, -5.966758465092425e-08, -0.045985378324985504, 0.01805025525391102, 0.03342108801007271, 0.029941171407699585, 0.017204834148287773, -0.07098314166069031, -0.030908461660146713, 0.09728457778692245, 0.027296515181660652, 0.05573192611336708, 0.02605198137462139, -0.05117805302143097, -0.043964702636003494, 0.08477502316236496, 0.07986083626747131, 0.011357784271240234, -0.02114132232964039, 0.002333028707653284, -0.002305807312950492, -0.04643801227211952, 0.0631166473031044, 0.04321271553635597, -0.039130352437496185, -0.04165353626012802, 0.017292089760303497, -0.06276368349790573, -0.04719892144203186, 0.0718899667263031, -0.042731668800115585, 0.010506724938750267, -0.04453464597463608, -0.003252506023272872, 0.07580193877220154, -0.03925251588225365, 0.010076168924570084, 0.012315614148974419, -0.032030440866947174, -0.06774964928627014, 0.029367180541157722, 0.02613932080566883, 0.01401809137314558, 0.019040562212467194, -0.008292215876281261, -0.05923106521368027, -0.025998223572969437, -0.06417308747768402, -0.05837976187467575, -0.09874055534601212, 0.02541401982307434, 0.01751190796494484, -0.05617181956768036, 0.005748288705945015, 0.036826927214860916, 0.05924290791153908, 0.07944956421852112, 0.00014838320203125477, 0.059217799454927444, -0.03641146048903465, 0.09700006246566772, 0.11129865795373917, 0.028422873467206955, 0.02339949645102024, 0.020422939211130142, 0.022030571475625038]}