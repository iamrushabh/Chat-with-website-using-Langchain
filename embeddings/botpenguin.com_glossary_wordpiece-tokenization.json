{"file_name": "botpenguin.com_glossary_wordpiece-tokenization", "text": "URL: https://botpenguin.com/glossary/wordpiece-tokenization\nWordPiece Tokenization: What is it & how does it work?\nWhy BotPenguin\nProduct\nSolutions\nPricing\nPartners\nResources\nLogin\nGet Started FREE\nIntegrations\nExperience 80+ world-class integrations.\nKey Features\nTake your business to the next level with our awesome key features.\nLive Chat\nStay in the loop with your clientele 24*7!\nUnified Inbox\nServe your customers across all platforms.\nAnalytics\nSpeedtrack your growth with our top-of-the-line analytics tools!\nMobile App\nMake, monitor, & manage your AI chatbots with our mobile app.\nCHATBOT COMPARISONS\nBotPenguin vs. Manychat\nBotPenguin vs. Tidio\nBotPenguin vs. Tawk.to\nBotPenguin vs. Wati\nBotPenguin vs. Interakt\nBotPenguin vs. AiSensy\nBotPenguin vs. Landbot\nWHAT CAN IT DO?\nMarketing Automation\nMake marketing a boon from the automation gods!\nFB Automation\nEngage with your customers on a deeper level.\nWhatsApp Automation\nGet that nifty automation for WhatsApp too!\nAppointment Bookings\nNo more delays, BotPenguin\u2019s got you here!\nCustomer Support\nYour customers are in for a treat with this automation.\nLead Generation\nGain more lead without any extra effort or expenses\nWHO CAN USE IT?\nHealthcare\nGive your patients world-class healthcare service!\nEducation\nMake admissions and automate processes in a jiffy!\nE-commerce\nCreate the best E-commerce service with ease!\nReal Estate\nMake Real Estate great again with BotPenguin!\nConsultants\nBoost up with our one-stop marketing solution!\nSaaS\nTake your SAAS game to the next level with BotPenguin!\nTours & Travels\nProvide extraordinary tour and travel services with BotPenguin!\nInsurance\nLaunch AI-driven Insurance Bot to Promote, Sell, & Manage Policies.\nWHERE CAN IT RUN?\nInstagram Chatbot\nAttract leads, boost sales, and chat 24/7 with Instagram Chatbots.\nWhatsApp Chatbot\nStart conversing like a real person with BotPenguin!\nTelegram Chatbot\nCutting-edge features for you to grow your business on Telegram.\nFacebook Chatbot\nDo everything at once with BotPenguin on Facebook.\nWebsites Chatbot\nBotPenguin grows your website and keeps your customers engaged.\nWordpress Chatbot\nBotPenguin thrives on WordPress and makes it awesome.\nMicrosoft Teams Chatbot\nMaximize your teams productivity with MS Teams Bot.\nShopify Chatbot\nBoost your Shopify Business With BotPenguin\u2019s AI-powered chatbot.\nWooCommerce Chatbot\nSell effortlessly on your WooCommerce store with BotPenguin.\nSquarespace Chatbot\nGet the most out of Squarespace with BotPenguin\nCUSTOM DEVELOPMENT\nWhitelabel ChatGPT\nApply your branding on ChatGPT, Launch your own AI platform\nChatGPT Custom Plugins\nIntegrate your service straight into ChatGPT\nCustom Chatbot Development\nBuild enterprise-grade chatbots with the best\nChatGPT Clone\nAdd functionality and branding on ChatGPT\nHIRE DEVELOPERS\nChatbot Developers\nBuild Lighter, Faster, Smarter-Efficiently\nChatGPT Developers\nRide the GPT wave with trained surfers\nChatGPT Consultants\nAdvice that makes the difference in your AI journey\nPARTNER PROGRAMS\nPartners Home\nJoin hands with us, and welcome growth\nWhatsApp Whitelabel Partners\nConquer the WhatsApp land with BotPenguin\u2019s White Label Platform\nWhitelabel Partners\nSay hi to the best Whitelabel chatbot platform ever\nAffiliate Partners\nEarn more and keep your clients happier\nImplementation Partners\nAs they say, a partner is worth trillions!\nPARTNER PRICING\nWhitelabel Chatbot Pricing\nOur pricing for Whitelabel Chatbot\nImplementation Partnership Pricing\nOur pricing for Implementation Partnership\nOUR RESOURCES\nBlogs\nRead the latest blogs on chatbots, AI, automations & more\nVideos\nWatch tutorials, webinars, and demos to master our chatbots.\nCase Study\nRead how BotPenguin transformed business communication\nE-book\nExplore e-books written by experts for all your business needs!\nHelp Docs\nFind detailed guides and tips for all your chatbot needs.\nNewsroom\nExplore how BotPenguin is making headlines in the chatbot industry.\nCommunity Support\nJoin our vibrant community to unlock exclusive content & expert guidance\nLATEST BLOG\nWhy is BotPenguin the best platform to develop a chatbot?\nIntroducing ChatGPT 4o for BotPenguin\nCreate your first AI Chatbot\nGet Started FREE\nGLOSSARY\nWordPiece Tokenization\nTable of Contents\nWhat is WordPiece Tokenization?\nWhy is WordPiece Tokenization used?\nHow does WordPiece Tokenization work?\nThe Workings of WordPiece Tokenization\nInfluence of WordPiece Tokenization in Language Models\nThe Variants of WordPiece Tokenization\nWordPiece Tokenization and its Application in Modern NLP Models\nTackling Problems with WordPiece Tokenization\nFine-Tuning WordPiece Tokenization\nWho uses WordPiece Tokenization?\nWhen should WordPiece Tokenization be used?\nFrequently Asked Questions (FAQs)\nShare\nLink copied\nWhat is WordPiece Tokenization?\nWordPiece Tokenization refers to the process of splitting text into smaller subword units called tokens. It combines the advantages of both character-level and word-level tokenization, allowing for more flexibility in capturing the meaning of words and effectively handling unknown or out-of-vocabulary (OOV) words.\nWhy is WordPiece Tokenization used?\nWordPiece Tokenization has gained popularity due to its numerous advantages over other tokenization methods. It offers improved language modeling, better handling of rare or OOV words, and enhanced performance in machine translation,\nnamed entity recognition\n,\nsentiment analysis\n, and other\nNLP\ntasks.\nSubword Granularity\nWordPiece Tokenization provides a higher level of subword granularity that enables finer distinctions between words, making it suitable for languages with complex morphology or compound words. This helps in capturing the meaning and context more accurately.\nOut-of-Vocabulary Handling\nOne of the major benefits of WordPiece Tokenization is its ability to handle OOV words effectively. By breaking words into smaller subword units, even unseen words can be represented using known subword units, reducing the number of OOV instances.\nFlexible Vocabulary Size\nUnlike fixed vocabularies used in word tokenization, WordPiece Tokenization allows for flexible vocabulary sizes. This means that the size of the vocabulary can be adjusted based on the specific application needs or the available computational resources.\nRare Word Handling\nWordPiece Tokenization handles rare words better by breaking them into subword units that are more frequently observed in the training data. This results in a more accurate representation of such words and improves the overall performance of\nNLP\nmodels.\nParameter Optimizations\nWordPiece Tokenization parameters, such as the number of iterations in the BPE\nalgorithm\nor the initial maximum length of subword units, can be tuned to optimize the tokenization process for a specific dataset or task. This flexibility allows for fine-tuning and improves the overall performance of\nNLP\nmodels.\nDare to Discover Conversational AI Secrets!\nGet Started FREE\nHow does WordPiece Tokenization work?\nWordPiece Tokenization follows a systematic process to break down text into subword units. This section provides a step-by-step breakdown of the WordPiece Tokenization process and illustrates it with examples.\nSubword Unit Extraction\nThe first step in WordPiece Tokenization is to initialize the vocabulary with individual characters from the\ntraining dataset\n. Then, the most frequent pairs of subword units are iteratively merged to create new subword units until the desired number of subword units is reached.\nSubword Encoding\nOnce the vocabulary is established, the text is encoded by replacing each word with its corresponding subword units. This results in the transformation of the text into a sequence of subword tokens.\nSubword Decoding\nDuring decoding, the subword tokens are converted back into words. This process involves merging consecutive subword units to form complete words. The decoding process makes use of the vocabulary for matching subword tokens to their corresponding words.\nThe Workings of WordPiece Tokenization\nWith the basics covered, let's take a closer look at how WordPiece tokenization actually works and the mechanics behind its efficiency.\nThe Core Process of WordPiece Tokenization\nWordPiece tokenization starts with a base vocabulary of individual characters. Then, using an iterative process, it combines these individual characters or character n-grams into sub-word units or word pieces, based on their frequency in the training data.\nEncoding Words Using WordPiece Tokenization\nWhen encoding words, WordPiece tokenization first checks if the word is in its vocabulary. If it is, it encodes it as is. However, if the word isn't present, it progressively breaks down the word into smaller units until it identifies pieces that are in its vocabulary.\nHandling Out-of-Vocabulary Words with WordPiece Tokenization\nThe key strength of WordPiece tokenization is its adeptness with out-of-vocabulary words. It achieves this by breaking down unknown words into familiar sub-word units, which can then be processed by the model, enhancing language translation model's effectiveness.\nIncreasing Computational Efficiency with WordPiece Tokenization\nBy breaking text into sub-word units instead of individual characters, WordPiece tokenization reduces the sequence length, making\nNLP\nmodels more computationally efficient. In addition, it results in fewer unknown tokens, increasing the model's overall performance.\nInfluence of WordPiece Tokenization in Language Models\nMoving on, let's explore how WordPiece tokenization impacts various aspects of language models, from data representation to system performance.\nWordPiece Tokenization and Data Representation\nWordPiece tokenization significantly improves the representation of data in language models. By splitting words into sub-word units, it manages complex morphology and rare words more effectively, leading to higher-quality translations.\nImpact on Language Model Performance\nThe influence of WordPiece tokenization extends to the overall performance of language models. Reducing sequence length and better handling of unknown words makes these models more computationally efficient and accurate.\nFacilitating Generalization in Language Models\nWordPiece tokenization facilitates better model generalization by working at a sub-word level. This allows models to cope with rare words and exposes them to a broader vocabulary, enhancing their ability to understand and generate text.\nEnhancing Information Retrieval with WordPiece Tokenization\nIn information retrieval tasks, WordPiece tokenization provides the means to handle a wider range of words, supporting more accurate and comprehensive search results, thus improving the overall effectiveness of search systems.\nThe Variants of WordPiece Tokenization\nLet's expand our understanding by looking at some of the variants of WordPiece tokenization and how they differ from the original technique.\nSentencePiece: A Variant of WordPiece Tokenization\nSentencePiece, an open-source version of WordPiece tokenization, offers a critical advantage: it doesn't require pre-tokenization like its predecessor. Therefore, it allows the training process to be language-independent and can handle raw sentences directly.\nUnigram Language Model Tokenization\nInspired by WordPiece tokenization, the Unigram Language Model Tokenization is probabilistic in nature. It trains a sub-word vocabulary and deletes less probable sequences iteratively, resulting in a fixed-size but optimized vocabulary.\nByte Pair Encoding (BPE) and WordPiece Tokenization\nByte Pair Encoding (BPE) is another sub-word tokenization approach, similar to WordPiece. However, while WordPiece is driven by frequency, BPE merges character sequences based on their statistical occurrences, effectively bridging the gap between character-based and word-based models.\nHybrid Models: Combination of WordPiece and Other Tokenization Methods\nIn an attempt to leverage the best of multiple techniques, hybrid tokenization models have emerged that combine WordPiece with other methods such as morphemes or syllables segmentation. These variants aim at enhancing the tokenization process and push the boundaries of model capabilities.\nWordPiece Tokenization and its Application in Modern NLP Models\nLet's navigate through its specific applications across several modern\nNLP\nmodels, shedding light on why it is an invaluable asset.\nWordPiece Tokenization in Google's BERT\nBERT, Google's pre-training model for\nNLP\ntasks, utilizes WordPiece tokenization for handling its input. This allows\nBERT\nto preserve rich semantic connections while modestly managing its vocabulary size, thereby improving translation quality and efficiency.\nApplication in Transformer Based Models\nWordPiece tokenization is widely used in Transformer-based models, enhancing their capability to handle a broader range of words, manage unknowns and improve the overall efficiency of data processing.\nGoogle's Neural Machine Translation (GNMT)\nAs mentioned earlier, WordPiece tokenization originates from GNMT. Here, it is used to divide a sentence into tokens while balancing between the flexibility of character-level translation and the efficiency of word-level translation.\nLeveraging WordPiece in Multilingual Models\nWordPiece tokenization's approach of using sub-word units makes it feasible for multilingual models. With a universal and scalable vocabulary, the models need less training data, and they can support multiple languages seamlessly.\nTackling Problems with WordPiece Tokenization\nWhile WordPiece tokenization is incredibly potent, it isn't without its issues. It's essential to understand and address these challenges to better utilize the technique.\nAddressing Token Illusion\nToken illusion is a problem where WordPiece tokenization incorrectly splits words into sub-words. These problematic tokens might be genuine words in a language but not in the intended context. Addressing this requires monitoring and contextual comprehension.\nDealing with Sub-Word Ambiguity\nSub-word ambiguity arises when tokenized sub-words have different meanings in different contexts. This requires careful handling and advanced modeling that incorporates a broader understanding of the context.\nOvercoming Over-Segmentation\nOver-segmentation refers to overly breaking down words into smaller sub-word parts. This often leads to loss of semantic meaning and requires a delicate balance to ensure optimal results from\ntokenization\n.\nManaging Rare Words with WordPiece Tokenization\nWhile WordPiece tokenization shines with handling unknown and rare words, it is not foolproof. Again, balance is necessary - splitting rare words into sub-word units that may not convey the original meaning can be a challenge.\nFine-Tuning WordPiece Tokenization\nWith an understanding of its challenges, let's discuss how to fine-tune WordPiece tokenization to get the best out of it.\nAdapting Distribution of Sub-Word Units\nWith careful analysis and ongoing adjustments, the distribution of sub-word units can be attuned to the specificities of the\ndataset\nto enhance the performance of the language model.\nBalancing Granularity in WordPiece Tokenization\nWhile more granularity in tokenization allows handling more rare words and reducing the vocabulary size, it also increases the sequence length. Thus, a precise balance is critical for optimum outcomes.\nFine-Tuning Vocabulary Size in WordPiece Tokenization\nWhile larger vocabulary size can improve the accuracy of the model, it also makes the model more computationally intensive. Therefore,\nfine-tuning\nthe vocabulary size is an important consideration for improving the effectiveness of WordPiece tokenization.\nAddressing Multi-Lingual Challenges of WordPiece Tokenization\nFor truly global models, fine-tuning WordPiece tokenization requires additional considerations to effectively handle the complexities of various languages. This usually involves diversifying the underlying training data to reflect broad linguistic variability.\nWho uses WordPiece Tokenization?\nWordPiece Tokenization finds application in various industries and domains that involve NLP tasks. It is widely used by companies, researchers, and developers who deal with large volumes of text data and require effective language understanding capabilities.\nSocial Media Analysis\nWordPiece Tokenization is extensively used in social media analysis to process and analyze large volumes of social media posts, comments, or tweets. It helps in identifying and categorizing user sentiments, detecting trending topics, and extracting meaningful insights from social media content.\nMachine Translation\nIn machine translation tasks, WordPiece Tokenization plays a critical role in breaking down sentences into subword units before translation. This process enables the translation model to handle and translate rare or OOV words accurately and capture the nuances of the language.\nChatbots and Virtual Assistants\nChatbots and virtual assistants leverage WordPiece\nTokenization\nto understand user queries and generate appropriate responses. By breaking down user queries into subword units, chatbots can better comprehend the context and intent behind the input, resulting in more accurate and meaningful interactions.\nNamed Entity Recognition\nNamed Entity Recognition\n(NER) systems rely on WordPiece Tokenization to accurately identify and classify named entities, such as person names, locations, organizations, and dates, within a given text. The subword granularity provided by WordPiece Tokenization enhances the performance of NER models.\nSentiment Analysis\nWordPiece Tokenization is employed in sentiment analysis tasks to understand the sentiment or emotion expressed in a given text. By breaking down the text into subword units,\nsentiment analysis\nmodels can capture more nuanced variations in sentiment and provide more accurate sentiment classification.\nWhen should WordPiece Tokenization be used?\nWordPiece Tokenization is suitable for various scenarios where the advantages it offers align with the specific requirements of a task or a language. This section explores when WordPiece Tokenization is most effective and highlights the factors to consider when opting for this tokenization method.\nMorphologically Rich Languages\nWordPiece Tokenization is particularly beneficial for languages with rich morphology, such as Finnish, Hungarian, or Turkish. These languages often have complex word formations, making it challenging to split them into accurate word tokens. WordPiece Tokenization can capture the subword patterns and allow for better understanding of the language.\nHandling OOV Words\nIf the task at hand involves handling OOV words effectively, WordPiece Tokenization can be a suitable choice. By breaking words into subword units, even unseen words can be represented using known units, improving the model's capability to capture their meaning and context.\nDomain-Specific Language\nWhen dealing with domain-specific languages or jargon where word\ntokenization\nmay face difficulties, WordPiece Tokenization can provide a more effective alternative. The subword granularity helps capture the unique language patterns and terminology specific to the domain, resulting in better language understanding.\nLarge Text Corpus\nWordPiece Tokenization is particularly useful when working with large text corpora. Its ability to handle rare or OOV words ensures that the model gets sufficient exposure to subword patterns across a wide range of texts, enhancing its language understanding capabilities.\nComputational Resources\nConsidering the available computational resources is crucial when deciding to use WordPiece Tokenization. As the vocabulary size can be adjusted, it allows for managing memory and processing requirements better. However, larger vocabulary sizes may require more computational resources.\nReady to build your chatbot? Create your own\nTry BotPenguin\nFrequently Asked Questions (FAQs)\nWhat are the benefits of WordPiece Tokenization?\nWordPiece Tokenization offers subword granularity, improved OOV word handling, flexible vocabulary size, improved rare-word handling, and parameter optimization compared to other tokenization methods.\nWhat is an example of word tokenization?\nAn example of word tokenization is splitting the sentence \"I love cats.\" into the tokens ['I', 'love', 'cats.'].\nWhat is an example of a subword tokenization?\nAn example of subword tokenization is representing the word \"unhappiness\" as subword tokens ['un', 'happi', 'ness'].\nWhat is the difference between BPE tokenizer and WordPiece tokenizer?\nThe difference between BPE tokenizer and WordPiece tokenizer is that BPE splits words into subword tokens by merging the most frequent pairs of consecutive characters, while WordPiece also considers the likelihood of a subword appearing as a whole in the training corpus.\nWhat is a sentence with the word token in it?\nHere is a sentence with the word \"token\": \u201cPlease hand me a token to use for the bus fare.\u201d\nBuild your first AI chatbot for FREE in just 5 minutes!\nGet Started FREE\nSurprise! BotPenguin has fun blogs too\nWe know you\u2019d love reading them, enjoy and learn.\nWhat is a WhatsApp Campaign? (With Real-World Examples)\nUpdated at Nov 15, 2024\n16 min to read\nBotPenguin\nContent Writer, BotPenguin\nA Comprehensive Look at Generative AI Use Cases Across Industries\nUpdated at Nov 14, 2024\n14 min to read\nManish Goyal\nAI Technical Lead, BotPenguin\nHow Generative AI Models Help in Enhancing Chatbot Conversations\nUpdated at Nov 14, 2024\n8 min to read\nManish Goyal\nAI Technical Lead, BotPenguin\nTable of Contents\nWhat is WordPiece Tokenization?\nWhy is WordPiece Tokenization used?\nHow does WordPiece Tokenization work?\nThe Workings of WordPiece Tokenization\nInfluence of WordPiece Tokenization in Language Models\nThe Variants of WordPiece Tokenization\nWordPiece Tokenization and its Application in Modern NLP Models\nTackling Problems with WordPiece Tokenization\nFine-Tuning WordPiece Tokenization\nWho uses WordPiece Tokenization?\nWhen should WordPiece Tokenization be used?\nFrequently Asked Questions (FAQs)\nBotPenguin is the best AI Chatbot maker platform. Create a Chatbot for WhatsApp, Website, Facebook Messenger, Telegram, WordPress & Shopify with BotPenguin - 100% FREE! Our chatbot creator helps with lead generation, appointment booking, customer support, marketing automation, WhatsApp & Facebook Automation for businesses. AI-powered No-Code chatbot maker with live chat plugin & ChatGPT integration.\nMobile app\niOS App\nAndroid App\nFully Operational\nStatus\nMobile app\niOS App\nAndroid App\nFully Operational\nStatus\nPlatforms\nMicrosoft Teams\nNew\nInstagram\nNew\nWhatsapp\nTelegram\nFacebook\nWebsites\nPartners\nWhitelabel Partner\nWhatsapp Whitelabel Partner\nImplementation Partner\nAffiliate Partner\nGet Started\nChatbot For Ecommerce\nChatbot For Real Estate\nChatbot For Education\nChatbot For Travel\nAll Templates\nFree Tools\nWhatsApp Link Generator\nWhatsApp QR Code Generator\nOpenAI API Pricing Calculator\nChatbot ROI Calculator\nAll Free Tools\nIntegrations\nChatGPT\nGoHighLevel\nBitrix 24\nZoho CRM\nZapier\nAll Integrations\nResources\nBlogs\nGlossary\nHelp Center\nWrite for us\nContact Us\nWhat\u2019s New\nProduct Updates\nComparisons\nBotPenguin vs Manychat\nBotPenguin vs Tidio\nBotPenguin vs Tawk.to\nBotPenguin vs Wati\nBotPenguin vs Landbot\nAll Comparisons\nAlternatives\nWhat you get\nLead Generation Bot\nSupport Bot\nAssistant Bot\nPlatforms\nMicrosoft Teams\nNew\nInstagram\nNew\nWhatsapp\nTelegram\nFacebook\nWebsites\nGet Started\nIntegrations\nComparisons\nPartners\nFree Tools\nResources\nWhat you get\nTerms & Conditions\nPrivacy Policy\nSecurity\nGDPR\nRefund Policy\nTerms & Conditions\nPrivacy Policy\nSecurity\nGDPR\nRefund Policy\nCopyright \u00a9 2018 - 2024 Relinns Technologies Pvt. Ltd. All RightsReserved.", "embedding": [-0.0971568375825882, -0.07767454534769058, 0.02298257127404213, -0.009296460077166557, -0.019675834104418755, -0.03655306622385979, 0.06990635395050049, 0.07684007287025452, 0.01497835386544466, 0.021187493577599525, 0.03545752912759781, -0.002528232056647539, 0.0300979632884264, 0.017097709700465202, 0.12557244300842285, -0.0035603935830295086, 0.11217617988586426, -0.08029996603727341, -0.10751883685588837, -0.035892024636268616, 0.028528163209557533, -0.014480221085250378, 0.020578306168317795, -0.023142194375395775, -0.0011722553754225373, -0.015545024536550045, -0.060904838144779205, -0.04294125363230705, -0.007573425304144621, -0.04208865761756897, -0.010737123899161816, 0.02774367295205593, 0.010941718704998493, 0.07172863930463791, -0.016981706023216248, -0.03227785974740982, -0.017690975219011307, 0.009849818423390388, 0.10000816732645035, -0.07499155402183533, -0.07734903693199158, -0.11798273772001266, -0.09223846346139908, 0.005997648928314447, 0.11590300500392914, 0.018108991906046867, -0.044791966676712036, 0.03745289891958237, -0.021612457931041718, 0.0750342458486557, -0.07973822951316833, -0.07217083126306534, 0.031576961278915405, 0.0628964751958847, -0.07670127600431442, 0.009892038069665432, -0.0244300477206707, 0.02181921899318695, 0.049317147582769394, -0.019767368212342262, -0.06554243713617325, -0.00828613992780447, 0.05283510312438011, 0.0580470971763134, -0.039374932646751404, -0.0070876372046768665, -0.1139763817191124, -0.0005601368611678481, 0.004351532552391291, -0.03214150667190552, -0.001635484048165381, -0.0029625578317791224, -0.01628660038113594, 0.04988252371549606, 0.008595464751124382, -0.012346065603196621, 0.03236808627843857, 0.0103453379124403, 0.031435273587703705, -0.07961732149124146, 0.0029574749059975147, 0.04581742361187935, 0.04184301570057869, 0.07667264342308044, -0.0011352211004123092, -0.02326606772840023, 0.009456461295485497, 0.016050757840275764, -0.006393361836671829, 0.019014107063412666, 0.009602192789316177, -0.041591618210077286, 0.04437153786420822, 0.0033542208839207888, 0.004872781224548817, -0.02075878158211708, -0.07512318342924118, 0.017429977655410767, -0.04289735108613968, 0.012844429351389408, -0.017540154978632927, -0.020047396421432495, -0.04219433665275574, -0.10241198539733887, -0.03659424930810928, 0.0007986773853190243, 0.033677373081445694, -0.0400191955268383, 0.1692219078540802, 0.015793226659297943, -0.12109322845935822, -0.037578925490379333, -0.035037510097026825, -0.024857264012098312, -0.03702180087566376, 0.0152230029925704, -0.05267205834388733, 0.03774944692850113, 0.174313485622406, 0.036038052290678024, 0.05474024638533592, 0.05812722072005272, -0.008841108530759811, -0.010378011502325535, 0.00810962077230215, 0.034233756363391876, 0.033776093274354935, 1.05454707585061e-32, -0.05970138683915138, 0.05030195415019989, -0.06794685870409012, 0.0828368216753006, 0.02835596725344658, 0.0029030251316726208, -0.0049546933732926846, 0.025784626603126526, -0.08079024404287338, -0.042943283915519714, -0.0616099089384079, 0.08666389435529709, -0.051268141716718674, 0.06731265783309937, 0.028638571500778198, -0.10265111923217773, -0.02726159244775772, 0.02347923442721367, 0.04528803005814552, -0.0014289014507085085, 0.08299271762371063, -0.008660872466862202, 0.040029894560575485, 0.06896752864122391, 0.08954824507236481, 0.012209909036755562, 0.06935787200927734, -0.00342746963724494, 0.06158292666077614, 0.041846659034490585, -0.08936657011508942, -0.020384030416607857, -0.026255303993821144, 0.015001976862549782, -0.03353070840239525, -0.035056713968515396, -0.02171221189200878, -0.1181667223572731, -0.04365549981594086, 0.004776949994266033, -0.12806519865989685, 0.009690621867775917, -0.08596138656139374, -0.07082007825374603, 0.012895777821540833, 0.036134425550699234, 0.018529677763581276, 0.016239019110798836, 0.05059313401579857, 0.01362521480768919, -0.018405433744192123, 0.017649676650762558, 0.015550092794001102, 0.06112995371222496, -0.0008026538416743279, -0.0474027618765831, 0.008483247831463814, -0.055726196616888046, -0.014649416320025921, -0.06161174550652504, -0.01005009189248085, -0.03140067681670189, 0.021919822320342064, 0.0008329593692906201, 0.04801172763109207, 0.03562027961015701, 0.06490602344274521, 0.03833991661667824, 0.03261271491646767, 0.03192887082695961, 0.03699051961302757, 0.056629620492458344, -0.02353638969361782, 0.022302750498056412, -0.055177148431539536, 0.022266199812293053, -0.04603203386068344, 0.023091312497854233, -0.07085950672626495, 0.012915554456412792, -0.029155245050787926, -0.04284205660223961, -0.01770278438925743, -0.02344299666583538, 0.0454205684363842, -0.04118739813566208, 0.03408972546458244, -0.06475028395652771, -0.024104200303554535, 0.007420849520713091, -0.0015724771656095982, 0.07083622366189957, -0.07034765928983688, 0.08115542680025101, -0.06762219965457916, -8.744358477709021e-33, -0.05329398810863495, 0.005928561091423035, -0.04040933772921562, 0.09598420560359955, -0.029337221756577492, -0.03282802924513817, 0.028403153643012047, -0.020829884335398674, 0.08322837203741074, 0.0034621828235685825, -0.10354609787464142, -0.006735808216035366, 0.0027896584942936897, -0.02874883823096752, -0.02301684394478798, 0.012834089808166027, -0.051968544721603394, 2.3975755539140664e-05, 0.03284650295972824, 0.03111165761947632, -0.023104418069124222, 0.0414176769554615, -0.12481851130723953, 0.008407488465309143, 0.021689575165510178, 0.056467410176992416, -0.09574831277132034, 0.04498592019081116, 0.041850727051496506, 0.01295118685811758, -0.02485719695687294, 0.006096254102885723, -0.02311878465116024, -0.0055210948921740055, 0.009340784512460232, 0.0322575680911541, 0.017357636243104935, 0.0028523195069283247, -0.003182688495144248, -0.06314187496900558, 0.10334184020757675, -0.05568164587020874, -0.032923996448516846, -0.0599207766354084, -0.028900988399982452, 0.035493042320013046, -0.13115748763084412, -0.02789938449859619, -0.05344930291175842, 0.04399168863892555, 0.0763009563088417, 0.001617050846107304, 0.00962055567651987, -0.03523709625005722, -0.10976368933916092, -0.01567952148616314, 0.09026811271905899, -0.01959531009197235, -0.11345621943473816, -0.011213511228561401, 0.03883424773812294, 0.0070672668516635895, 0.09936299175024033, 0.061046428978443146, 0.0423477403819561, -0.038400594145059586, 0.05687227100133896, 0.0458521842956543, -0.02107352949678898, -0.08761864900588989, 0.05135432258248329, -0.028750520199537277, -0.019008032977581024, 0.009910883381962776, 0.015207500196993351, 0.08267924189567566, 0.055886317044496536, -0.1421205848455429, -0.0284098032861948, -0.02621270902454853, -0.038618702441453934, 0.007980916649103165, 0.0501885712146759, 0.041520826518535614, -0.055533573031425476, 0.04091303050518036, -0.040593814104795456, 0.03414653241634369, -0.01942397840321064, 0.03951701149344444, -0.03433002904057503, 0.024346617981791496, -0.008191747590899467, 0.12722711265087128, -0.00022618919319938868, -5.942807135284056e-08, -0.03311998397111893, -0.007657304871827364, 0.03000141866505146, 0.03430377319455147, 0.012941957451403141, -0.07347067445516586, -0.04888632521033287, 0.04328514263033867, 0.0156168881803751, 0.000932477880269289, 0.01936809904873371, -0.026452720165252686, -0.09624522924423218, 0.0313945896923542, 0.05403444543480873, 0.0011729680700227618, -0.01567867025732994, -0.006500764284282923, 0.009020891040563583, -0.04004695639014244, 0.040634747594594955, 0.0340718999505043, -0.04262614622712135, -0.05695459619164467, 0.019931592047214508, -0.0376037135720253, -0.05205262824892998, 0.10993645340204239, -0.04454148933291435, -0.04638117179274559, -0.021046610549092293, -0.0012930100783705711, 0.046191878616809845, -0.03933161497116089, -0.011506748385727406, 0.01492972020059824, -0.06983292102813721, -0.08561427146196365, 0.0194076094776392, 0.05417089909315109, 0.036621320992708206, 0.024750597774982452, 0.023360438644886017, -0.0739838257431984, -0.011426332406699657, -0.0186755433678627, -0.05640281364321709, -0.08858614414930344, 0.050412435084581375, -0.007662787567824125, -0.03608255088329315, 0.03074086830019951, 0.05707139894366264, 0.056455306708812714, 0.06483382731676102, 0.006164952646940947, 0.053168632090091705, 0.01141632441431284, 0.10382992774248123, 0.08495805412530899, 0.03313588351011276, 0.020374786108732224, 0.04362190142273903, 0.022528551518917084]}