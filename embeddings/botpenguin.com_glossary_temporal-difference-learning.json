{"file_name": "botpenguin.com_glossary_temporal-difference-learning", "text": "URL: https://botpenguin.com/glossary/temporal-difference-learning\nTemporal Difference Learning: Benefits & Limitations\nWhy BotPenguin\nProduct\nSolutions\nPricing\nPartners\nResources\nLogin\nGet Started FREE\nIntegrations\nExperience 80+ world-class integrations.\nKey Features\nTake your business to the next level with our awesome key features.\nLive Chat\nStay in the loop with your clientele 24*7!\nUnified Inbox\nServe your customers across all platforms.\nAnalytics\nSpeedtrack your growth with our top-of-the-line analytics tools!\nMobile App\nMake, monitor, & manage your AI chatbots with our mobile app.\nCHATBOT COMPARISONS\nBotPenguin vs. Manychat\nBotPenguin vs. Tidio\nBotPenguin vs. Tawk.to\nBotPenguin vs. Wati\nBotPenguin vs. Interakt\nBotPenguin vs. AiSensy\nBotPenguin vs. Landbot\nWHAT CAN IT DO?\nMarketing Automation\nMake marketing a boon from the automation gods!\nFB Automation\nEngage with your customers on a deeper level.\nWhatsApp Automation\nGet that nifty automation for WhatsApp too!\nAppointment Bookings\nNo more delays, BotPenguin\u2019s got you here!\nCustomer Support\nYour customers are in for a treat with this automation.\nLead Generation\nGain more lead without any extra effort or expenses\nWHO CAN USE IT?\nHealthcare\nGive your patients world-class healthcare service!\nEducation\nMake admissions and automate processes in a jiffy!\nE-commerce\nCreate the best E-commerce service with ease!\nReal Estate\nMake Real Estate great again with BotPenguin!\nConsultants\nBoost up with our one-stop marketing solution!\nSaaS\nTake your SAAS game to the next level with BotPenguin!\nTours & Travels\nProvide extraordinary tour and travel services with BotPenguin!\nInsurance\nLaunch AI-driven Insurance Bot to Promote, Sell, & Manage Policies.\nWHERE CAN IT RUN?\nInstagram Chatbot\nAttract leads, boost sales, and chat 24/7 with Instagram Chatbots.\nWhatsApp Chatbot\nStart conversing like a real person with BotPenguin!\nTelegram Chatbot\nCutting-edge features for you to grow your business on Telegram.\nFacebook Chatbot\nDo everything at once with BotPenguin on Facebook.\nWebsites Chatbot\nBotPenguin grows your website and keeps your customers engaged.\nWordpress Chatbot\nBotPenguin thrives on WordPress and makes it awesome.\nMicrosoft Teams Chatbot\nMaximize your teams productivity with MS Teams Bot.\nShopify Chatbot\nBoost your Shopify Business With BotPenguin\u2019s AI-powered chatbot.\nWooCommerce Chatbot\nSell effortlessly on your WooCommerce store with BotPenguin.\nSquarespace Chatbot\nGet the most out of Squarespace with BotPenguin\nCUSTOM DEVELOPMENT\nWhitelabel ChatGPT\nApply your branding on ChatGPT, Launch your own AI platform\nChatGPT Custom Plugins\nIntegrate your service straight into ChatGPT\nCustom Chatbot Development\nBuild enterprise-grade chatbots with the best\nChatGPT Clone\nAdd functionality and branding on ChatGPT\nHIRE DEVELOPERS\nChatbot Developers\nBuild Lighter, Faster, Smarter-Efficiently\nChatGPT Developers\nRide the GPT wave with trained surfers\nChatGPT Consultants\nAdvice that makes the difference in your AI journey\nPARTNER PROGRAMS\nPartners Home\nJoin hands with us, and welcome growth\nWhatsApp Whitelabel Partners\nConquer the WhatsApp land with BotPenguin\u2019s White Label Platform\nWhitelabel Partners\nSay hi to the best Whitelabel chatbot platform ever\nAffiliate Partners\nEarn more and keep your clients happier\nImplementation Partners\nAs they say, a partner is worth trillions!\nPARTNER PRICING\nWhitelabel Chatbot Pricing\nOur pricing for Whitelabel Chatbot\nImplementation Partnership Pricing\nOur pricing for Implementation Partnership\nOUR RESOURCES\nBlogs\nRead the latest blogs on chatbots, AI, automations & more\nVideos\nWatch tutorials, webinars, and demos to master our chatbots.\nCase Study\nRead how BotPenguin transformed business communication\nE-book\nExplore e-books written by experts for all your business needs!\nHelp Docs\nFind detailed guides and tips for all your chatbot needs.\nNewsroom\nExplore how BotPenguin is making headlines in the chatbot industry.\nCommunity Support\nJoin our vibrant community to unlock exclusive content & expert guidance\nLATEST BLOG\nWhy is BotPenguin the best platform to develop a chatbot?\nIntroducing ChatGPT 4o for BotPenguin\nCreate your first AI Chatbot\nGet Started FREE\nGLOSSARY\nTemporal Difference Learning\nTable of Contents\nWhat is Temporal Difference Learning (TD Learning)?\nHow Does Temporal Difference Learning Work?\nApplications of Temporal Difference Learning\nParameters in Temporal Difference Learning\nTemporal Difference Learning in Neuroscience\nBenefits of Temporal Difference Learning\nLimitations of Temporal Difference Learning\nTemporal Difference Error\nDifference Between Q-Learning and Temporal Difference Learning\nDifferent Algorithms in Temporal Difference Learning\nFrequently Asked Questions (FAQs)\nShare\nLink copied\nWhat is Temporal Difference Learning (TD Learning)?\nTemporal difference (TD) learning is a method in reinforcement learning for training agents to make optimal decisions. It works by learning from the difference between temporally successive predictions.\nFor example, a robot may predict that taking an action in a particular state will produce +5 reward. On taking that action, it receives +10 reward. The TD error of +5 causes the robot to update its predictions.\nOver time, by minimizing TD errors, the agent learns to make increasingly accurate predictions about long-term rewards. This lets it determine the best actions to take in different situations.\nTD learning has key advantages over other reinforcement learning techniques:\nIt learns from experience without detailed environment models.\nCan learn from incomplete sequences before final outcome.\nComputationally simple to implement.\nTD learning has enabled breakthroughs in game AI, robotics, and complex automation. The ability to make far-sighted decisions from ongoing experience makes TD learning powerful and broadly applicable.\nHow Does Temporal Difference Learning Work?\nTemporal Difference Learning employs a mathematical trick to make predictions more accurate over time.\nBy utilizing the difference between the predicted value and the actual value of a subsequent state, TD Learning can gradually match expectations with reality. This gradual increase in accuracy is key to the effectiveness of TD Learning.\nApplications of Temporal Difference Learning\nTemporal Difference Learning finds wide applications in various domains, including:\nUnderstanding Conditions like Schizophrenia:\nTD Learning is used to study and understand conditions such as schizophrenia, which involve impairments in learning and decision-making processes.\nPharmacological Manipulations of Dopamine:\nIt helps explore the effects of pharmacological interventions on learning by manipulating the dopamine system.\nMachine Learning:\nTemporal Difference Learning is widely used in machine learning algorithms, particularly in reinforcement learning tasks, due to its effectiveness and efficiency.\nParameters in Temporal Difference Learning\nIn this section, we'll be exploring the specific parameters that come into play when we talk about Temporal Difference Learning. Couldn't be more excited to embark on this learning journey with you!\nState-Value Function\nEver heard of 'state' in\nmachine learning\n? A state-value function reckons the expected long-term return with an initial state s, under policy \u03c0.\nIt's like a grading system for how good or bad states are in a day long adventure.\nAction-Value Function\nThis one's a bit different. An action-value function measures the expected rewards in a long term when one executes a certain action in a particular state.\nImagine it like checking what benefit you'll get if you make a specific move in your favorite board game!\nDiscount Factor\nSource: Towards Data Science\nIn TD learning, the discount factor is that cool dude that ensures the sum of the rewards remains finite. It\u2019s a measure between 0 and 1 and defines the weight of future rewards.\nThe larger the discount factor, the more attention we give to future rewards. It's like choosing between having a small piece of cake right now or a bigger one in a bit!\nLearning Rate\nLast but not least, is the 'learning rate'. Generally denoted by \u03b1, the learning rate is the degree to which the newly acquired information will override the old information. An analogy?\nThink of it like balancing between lending your ear to a fresh piece of gossip versus sticking to the old ones.\nTD Error\nTo make the learning process more precise, we have the TD error. It's a measure of the difference between the estimated value of a state-action pair and the observed reward.\nIf the gossips (new info) were accurate, the TD error would be zero!\nDocument\nAnswer Your Customers like a Human\nUsing an AI Chatbot!\nTry BotPenguin\nTemporal Difference Learning in Neuroscience\nIn this section, we'll delve into how Temporal Difference Learning has fascinating implications in the world of neuroscience.\nReward Prediction Error Theory\nTemporal Difference Learning models are an inspiration for the Reward Prediction Error Theory.\nThis theory suggests that the release of the neurotransmitter dopamine encodes a prediction error signal, informing animals (including humans) how to adjust their behaviors for maximizing future rewards.\nBiological Plausibility\nSource: ScienceDirect\nTemporal Difference learning algorithms are biologically plausible, with neural circuits in the brain exhibiting TD-like learning.\nObservations of the midbrain dopamine system show similarities in its error training signals and the signals produced by Temporal Difference Learning algorithms.\nNeural Basis of Learning\nModeling the neural basis of learning with the use of Temporal Difference Learning has significantly impacted our understanding of the brain.\nThe\nalgorithms\nshed light on how the brain computes reward prediction errors and updates representations of stimulus-value associations.\nArtificial Neural Networks\nSource: GeeksforGeeks\nTemporal Difference Learning is effectively employed in artificial\nneural networks\n.\nIn neuroscience, this translates to better understanding of neural plasticity mechanisms, synaptic changes, and the integration of artificial learning systems with biological counterparts.\nBehavioral and Cognitive Processes\nThe application of Temporal Difference Learning has enhanced research on behavioral and cognitive processes in psychology and cognitive neuroscience, enriching our awareness of decision-making, goal-directed behaviors, and learning.\nPeering into the intersection between Temporal Difference Learning and neuroscience, we find remarkable connections that serve as keystones in unlocking the intricate wonders of the brain and its learning processes.\nBenefits of Temporal Difference Learning\nIn this section, we'll explore the various advantages of Temporal Difference Learning, revealing why it's an integral part of\nreinforcement learning\n.\nEfficient Learning\nBy updating value estimates using differences between time-steps, Temporal Difference Learning greatly accelerates the learning process. The result? Efficiency in learning backed by the power of this algorithm family.\nIncreased Adaptability\nTemporal Difference Learning is an adaptive dynamo.\nIt enables agents to learn and adapt to changing environments due to its online learning ability, which yields better responsiveness to changing rewards and states.\nBalancing Exploration and Exploitation\nSource: Bizzdesign\nA well-honed equilibrium between exploration and exploitation is essential in reinforcement learning.\nTemporal Difference Learning techniques, like SARSA, learn and fine-tune policies that harmonize this delicate balance.\nOnline Learning Capabilities\nOne of the significant strengths of Temporal Difference Learning is its ability to learn online in real time.\nIt doesn't require the full details of an episode to be known in advance, unlike Monte Carlo methods, granting it added versatility in practical applications.\nApplicability in Various Domains\nThe sheer range of Temporal Difference Learning's applicability across numerous domains is truly impressive.\nFrom robotics and control systems to\nartificial intelligence\nand game playing, the benefits of this learning method reach far and wide.\nLimitations of Temporal Difference Learning\nWhile Temporal Difference Learning offers many benefits, it has certain limitations, including:\nDependence on Initial Estimates\nTD learning heavily banks on initial estimates. It might resemble a game of darts where we're trying to hit the bullseye, but our first throw decides a lot. If the initial estimate is not even close, we might unknowingly be practicing our aim in the wrong direction.\nSlow Convergence\nRemember the childhood game where you blindfolded your friends and directed them towards a target?\nWell, that's what convergence feels like in TD learning. Commonly, TD learning tends to converge slowly over iterations; it's like giving blind-folded instructions to a wandering friend where you only make minuscule corrections with each step.\nOverfitting\nOverfitting is like that annoying dinner guest that most machine learning models can't seem to escape.\nTD learning is also susceptible to overfitting, especially when the model complexity increases. It\u2019s like memorizing responses to trivia questions \u2013 great for those exact questions, not so much for similar but slightly different ones.\nSensitivity to Learning Rates\nTD Learning is like Goldilocks trying to find the perfect porridge \u2013 it\u2019s sensitive to the learning rates. Too high or too low, and we run into problems like unstable learning and slow convergence.\nStriking that perfect learning rate balance is a bit of a dance, and missing a step may lead to not-so-great results.\nDifficulty in Understanding\nLast, but certainly not least, is the fact that TD Learning can be a bit of a tough nut to crack.\nIt requires an understanding of technical concepts like Markov Decision Processes and rewards structures. It\u2019s kind of like learning a new language with its own rules, grammar, and idioms.\nTemporal Difference Error\nThe temporal difference (TD) error is a key concept in reinforcement learning that drives learning in temporal difference methods. It refers to the difference between two successive predictions made by the agent as it interacts with the environment.\nHere's an example to illustrate the TD error:\nImagine a robot learning to play chess. It makes the following predictions:\nIn state S1, the robot predicts making move M1 will give +2 reward\nIt makes move M1, reaches state S2, and receives an actual reward of +5\nIn S2, it predicts its reward is +10 for this state\nSo the TD error is +5 - +2 = +3\nThis TD error of +3 indicates the prediction for S1 was off by 3 points.\nThe key properties of the TD error are:\nIt is the difference between successive predictions by the agent.\nThe agent uses the TD error to incrementally update its predictions to be more accurate.\nMinimizing the TD error over time leads to optimal predictions.\nTD error provides a computationally simple training signal for temporal difference learning.\nIn summary, the TD error is a key mechanism for an agent to learn predictions about rewards. By repeatedly minimizing the TD error, the agent becomes progressively better at making decisions to maximize cumulative future reward.\nDifference Between Q-Learning and Temporal Difference Learning\nQ-Learning is a specific algorithm within the category of Temporal Difference Learning. It aims to learn the Q-function, which represents the expected reward for taking a specific action in a given state.\nTemporal Difference Learning, on the other hand, encompasses a broader range of algorithms that can learn both the V-function and the Q-function.\nHere\u2019s a detailed comparison between the two.\nBasic Framework\nTemporal Difference Learning is a framework that uses the difference between estimated values of two successive states to update value estimates.\nOn the other hand, Q-learning is an algorithm that operates within this Temporal Difference framework \u2013 it specifically learns the quality of actions to decide which action to take.\nLearning Method\nWhen it comes to learning, Temporal Difference Learning learns from consecutive states in an environment to forecast future rewards.\nQ-Learning, however, is a touch more specific. It uses updates in the action-value function to maximize expected rewards.\nPolicy\nAnother key point of divergence is their approach to policy. Temporal Difference Learning allows both on-policy learning like SARSA as well as off-policy learning like Q-Learning.\nBut Q-Learning itself is a type of off-policy learning that estimates the optimal policy, regardless of the policy followed by the agent.\nBias and Variance\nIn terms of bias and variance, Temporal Difference Learning tries to strike a balance between the two across its different algorithms.\nQ-Learning on the other hand, can occasionally be prone to overestimating future rewards due to its max operation, introducing a bit of bias into its action-value estimation.\nUse Case\nLastly, Temporal Difference Learning generally serves as a foundation for many learning algorithms like Q-Learning and SARSA, to mention a couple.\nQ-Learning, specific in its design, is widely used when the agent need to learn an optimal policy while exploring the environment.\nDifferent Algorithms in Temporal Difference Learning\nIn this section, we'll cover the key algorithms used in temporal difference learning.\nSarsa Algorithm\nThe Sarsa algorithm works by estimating the value of the current state-action pair based on the reward and the estimate of the next state-action pair.\nThis allows it to continually update and adjust to better policy.\nQ-Learning\nSource: Simplilearn\nUnlike Sarsa, Q-Learning directly estimates the optimal policy, even if actions are taken according to a more exploratory or random policy.\nQ-Learning works by updating the value of an action in a state without considering the next action.\nTD-Lambda\nTD-Lambda joins the benefits of Monte Carlo and TD Learning, leveraging a parameter \ud835\udf06 to balance the two.\nIt uses eligibility traces to keep a record of states, giving a proportion of the reward back to recently visited states.\nSarsa-Lambda\nSarsa-Lambda is an extension of the Sarsa algorithm, adding eligibility traces for a more effective 'look back' at previous state-action pairs.\nThis makes it more efficient by disseminating the reward to all preceding states.\nDouble Q-Learning\nDouble Q-learning addresses the overestimation bias of traditional Q-learning by maintaining two independent Q value estimates.\nThe choice of which Q value to update is random, providing a more unbiased approximation.\nDocument\nMake Your Own AI Chatbot\nWithout Any Coding!\nGet Started FREE\nFrequently Asked Questions (FAQs)\nWhat is Temporal Difference Learning?\nTemporal Difference Learning is a technique in reinforcement learning that predicts future values by gradually adjusting predictions based on incoming information.\nIt combines elements of Monte Carlo and dynamic programming approaches.\nHow does Temporal Difference Learning work?\nTemporal Difference Learning makes predictions more accurate over time by utilizing the difference between the predicted value and the actual value of a subsequent state. It gradually matches expectations with reality.\nWhat are the parameters in Temporal Difference Learning?\nThe parameters in Temporal Difference Learning include the learning rate (\u03b1), which determines the extent of new information influence, the discount rate (\u03b3), which prioritizes future rewards, and the exploration vs. exploitation parameter (e), balancing new information with known actions.\nWhat are the applications of Temporal Difference Learning?\nTemporal Difference Learning finds applications in neuroscience for understanding dopamine neurons, in studying conditions like schizophrenia, and in machine learning algorithms for reinforcement learning tasks.\nWhat are the benefits of Temporal Difference Learning?\nTemporal Difference Learning offers benefits such as online and offline learning, handling incomplete sequences, adapting to non-terminating environments, less variance and higher efficiency compared to Monte Carlo methods, and leveraging the Markov property for accurate predictions.\nBuild your first AI chatbot for FREE in just 5 minutes!\nGet Started FREE\nSurprise! BotPenguin has fun blogs too\nWe know you\u2019d love reading them, enjoy and learn.\nWhat is a WhatsApp Campaign? (With Real-World Examples)\nUpdated at Nov 15, 2024\n16 min to read\nBotPenguin\nContent Writer, BotPenguin\nA Comprehensive Look at Generative AI Use Cases Across Industries\nUpdated at Nov 14, 2024\n14 min to read\nManish Goyal\nAI Technical Lead, BotPenguin\nHow Generative AI Models Help in Enhancing Chatbot Conversations\nUpdated at Nov 14, 2024\n8 min to read\nManish Goyal\nAI Technical Lead, BotPenguin\nTable of Contents\nWhat is Temporal Difference Learning (TD Learning)?\nHow Does Temporal Difference Learning Work?\nApplications of Temporal Difference Learning\nParameters in Temporal Difference Learning\nTemporal Difference Learning in Neuroscience\nBenefits of Temporal Difference Learning\nLimitations of Temporal Difference Learning\nTemporal Difference Error\nDifference Between Q-Learning and Temporal Difference Learning\nDifferent Algorithms in Temporal Difference Learning\nFrequently Asked Questions (FAQs)\nBotPenguin is the best AI Chatbot maker platform. Create a Chatbot for WhatsApp, Website, Facebook Messenger, Telegram, WordPress & Shopify with BotPenguin - 100% FREE! Our chatbot creator helps with lead generation, appointment booking, customer support, marketing automation, WhatsApp & Facebook Automation for businesses. AI-powered No-Code chatbot maker with live chat plugin & ChatGPT integration.\nMobile app\niOS App\nAndroid App\nFully Operational\nStatus\nMobile app\niOS App\nAndroid App\nFully Operational\nStatus\nPlatforms\nMicrosoft Teams\nNew\nInstagram\nNew\nWhatsapp\nTelegram\nFacebook\nWebsites\nPartners\nWhitelabel Partner\nWhatsapp Whitelabel Partner\nImplementation Partner\nAffiliate Partner\nGet Started\nChatbot For Ecommerce\nChatbot For Real Estate\nChatbot For Education\nChatbot For Travel\nAll Templates\nFree Tools\nWhatsApp Link Generator\nWhatsApp QR Code Generator\nOpenAI API Pricing Calculator\nChatbot ROI Calculator\nAll Free Tools\nIntegrations\nChatGPT\nGoHighLevel\nBitrix 24\nZoho CRM\nZapier\nAll Integrations\nResources\nBlogs\nGlossary\nHelp Center\nWrite for us\nContact Us\nWhat\u2019s New\nProduct Updates\nComparisons\nBotPenguin vs Manychat\nBotPenguin vs Tidio\nBotPenguin vs Tawk.to\nBotPenguin vs Wati\nBotPenguin vs Landbot\nAll Comparisons\nAlternatives\nWhat you get\nLead Generation Bot\nSupport Bot\nAssistant Bot\nPlatforms\nMicrosoft Teams\nNew\nInstagram\nNew\nWhatsapp\nTelegram\nFacebook\nWebsites\nGet Started\nIntegrations\nComparisons\nPartners\nFree Tools\nResources\nWhat you get\nTerms & Conditions\nPrivacy Policy\nSecurity\nGDPR\nRefund Policy\nTerms & Conditions\nPrivacy Policy\nSecurity\nGDPR\nRefund Policy\nCopyright \u00a9 2018 - 2024 Relinns Technologies Pvt. Ltd. All RightsReserved.", "embedding": [-0.09030184149742126, -0.05754254013299942, 0.02425902523100376, 0.011238610371947289, -0.0021716055925935507, -0.02715197391808033, 0.009504183195531368, 0.06378868967294693, 0.01033429242670536, 0.013143541291356087, 0.04008385166525841, 0.017142580822110176, -0.005509879440069199, 0.03288428112864494, 0.10491776466369629, -0.03587523102760315, 0.09043601155281067, -0.09229723364114761, -0.06280247122049332, -0.030307071283459663, -0.006256395019590855, -0.036465492099523544, 0.0419655479490757, -0.004029560834169388, -0.013128106482326984, -0.013079854659736156, -0.031647853553295135, -0.025604361668229103, 0.010004496201872826, -0.06622689962387085, 0.00036457902751863003, 0.021431535482406616, -0.0007765252375975251, 0.03677225112915039, -0.06288551539182663, -0.039754681289196014, -0.01101261842995882, 0.030704129487276077, 0.05900132283568382, -0.06399362534284592, -0.0880814865231514, -0.07239671796560287, -0.056795164942741394, -0.004813646897673607, 0.09128388017416, -0.007632868830114603, -0.061818163841962814, -0.00868301186710596, -0.018666056916117668, 0.08853942155838013, -0.08638425171375275, -0.0510624498128891, 0.036963868886232376, 0.04514961689710617, -0.03309579938650131, 0.07299976795911789, -0.02645091339945793, 0.05524905025959015, 0.008436442352831364, -0.014290504157543182, -0.07071838527917862, -0.02583937905728817, 0.021008450537919998, 0.027561308816075325, -0.054114095866680145, 0.019159605726599693, -0.1002657413482666, -0.003534182207658887, 0.03158349171280861, -0.02955993451178074, -0.02075405977666378, 0.003625339362770319, -0.041555311530828476, 0.057340800762176514, 0.0289378110319376, -0.03280169889330864, 0.026757333427667618, 0.0031885707285255194, 0.023826414719223976, -0.07945936918258667, 0.0007980837253853679, 0.06266801804304123, -0.015519345179200172, 0.039866555482149124, -0.012209528125822544, -0.05420365184545517, -0.00022762722801417112, 0.05981791019439697, -0.046898551285266876, -0.020084330812096596, 0.058391179889440536, 0.0102725550532341, 0.03475199267268181, 0.007286005187779665, 0.022740663960576057, 0.011110177263617516, -0.06930006295442581, 0.04123232513666153, -0.027848536148667336, 0.00018982640176545829, -0.003278108546510339, -0.03607182949781418, -0.08211719244718552, -0.053254686295986176, -0.02781316637992859, 0.019423654302954674, 0.0536326989531517, -0.04241494461894035, 0.1445831060409546, -0.007326031103730202, -0.10586164891719818, -0.05700363218784332, -0.0036445395089685917, -0.019534412771463394, 0.015848154202103615, 0.038127265870571136, -0.04172760993242264, 0.046228520572185516, 0.17379212379455566, 0.055081482976675034, 0.06340005248785019, 0.03433239459991455, 0.008142897859215736, -0.005908509250730276, 0.040881793946027756, 0.0008134408853948116, 0.006235177628695965, 1.0125809724370943e-32, -0.022441959008574486, -0.00020882203534711152, -0.07242844998836517, 0.08949241787195206, 0.03797190263867378, 0.028835361823439598, 0.024318359792232513, 0.03677516058087349, -0.035236142575740814, -0.02593260258436203, -0.089938223361969, 0.11465766280889511, -0.08445355296134949, 0.043215956538915634, 0.053301021456718445, -0.07981763035058975, -0.05775611475110054, 0.02691277116537094, 0.07868053764104843, -0.0072643873281776905, 0.08479204028844833, -0.05535005033016205, 0.06442920118570328, 0.07165629416704178, 0.13197146356105804, 0.011780135333538055, 0.0773419663310051, 0.033824559301137924, 0.047361329197883606, 0.019924238324165344, -0.07976777106523514, 0.011941752396523952, -0.07260379940271378, 0.01457231119275093, -0.012361838482320309, -0.044277533888816833, -0.02373315580189228, -0.11058323830366135, -0.05680936947464943, 0.005024197045713663, -0.1316688507795334, -0.006746460683643818, -0.08248623460531235, -0.06884244829416275, 0.019551290199160576, 0.006268852856010199, 0.036721471697092056, 0.011450192891061306, 0.010983571410179138, 0.024846237152814865, -0.059134311974048615, 0.025525705888867378, 0.01701362431049347, -0.019291328266263008, 0.00847673136740923, -0.008478404022753239, 0.04195736348628998, -0.01856897585093975, -0.016123609617352486, -0.01793634332716465, 0.0005865906714461744, -0.02782866172492504, 0.012962378561496735, -0.0096717095002532, 0.052220430225133896, 0.054141055792570114, 0.0412273108959198, 0.03906660154461861, 0.039400652050971985, 0.019265005365014076, 0.042089011520147324, 0.05218752101063728, -0.0074346489273011684, 0.023513134568929672, -0.011500220745801926, 0.0007089884602464736, -0.06464331597089767, 0.015897687524557114, -0.04026227816939354, 0.006770460866391659, -0.021493639796972275, -0.025278888642787933, -0.016980107873678207, -0.04442364349961281, 0.05058317258954048, -0.03879905864596367, 0.04457909241318703, -0.05050811171531677, -0.004143786150962114, 0.03767819330096245, -0.04372334107756615, 0.07626727968454361, -0.06001999229192734, 0.10262708365917206, -0.02590760588645935, -8.169101664459333e-33, -0.02885579504072666, 0.014660974964499474, -0.05286359786987305, 0.08545158803462982, 0.019184982404112816, -0.024195561185479164, 0.013124263845384121, -0.013970954343676567, 0.08467909693717957, -0.01781422644853592, -0.08393434435129166, -0.029387803748250008, -0.014485212974250317, -0.023446859791874886, -0.015905503183603287, 0.035505037754774094, -0.05618338659405708, -0.04983578994870186, 0.011562048457562923, 0.006740539334714413, -0.013540007174015045, 0.05940108001232147, -0.14954008162021637, -0.01124323345720768, -0.018015291541814804, 0.015304180793464184, -0.06397487223148346, 0.06362384557723999, 0.016337912529706955, 0.031973786652088165, -0.018652785569429398, -0.027296656742691994, -0.021228130906820297, -0.012505389750003815, -0.005950265564024448, 0.061243731528520584, 0.023739339783787727, -0.046837396919727325, -0.018855134025216103, -0.03051883354783058, 0.1052541732788086, -0.060011133551597595, -0.02767728641629219, -0.1001545861363411, 0.005583508871495724, 0.05320986732840538, -0.1503029316663742, -0.018457068130373955, -0.048264481127262115, 0.04602452367544174, 0.07799995690584183, 0.012142066843807697, 0.03045922890305519, -0.07861150801181793, -0.09088832885026932, -0.03984663262963295, 0.11222653836011887, -0.006092581432312727, -0.08408771455287933, 0.02843955159187317, 0.019640322774648666, -0.012072460725903511, 0.07216456532478333, 0.07527195662260056, 0.020690718665719032, -0.010090604424476624, 0.05040623992681503, 0.032599031925201416, 0.004688370507210493, -0.0841258093714714, 0.0637924000620842, -0.05251429229974747, -0.025124862790107727, -0.015965312719345093, 0.008575018495321274, 0.07009325921535492, 0.03379032015800476, -0.08498882502317429, -0.023618405684828758, -0.04214067384600639, -0.0773458480834961, 0.0027788791339844465, 0.05199740454554558, 0.05414578318595886, -0.0974891185760498, 0.11918046325445175, -0.021513942629098892, 0.015510022640228271, 0.014005902223289013, -0.0027983146719634533, -0.032527562230825424, 0.03452667221426964, -0.04426407441496849, 0.08521931618452072, -0.010293541476130486, -5.774744238351559e-08, -0.034702520817518234, -0.006419374607503414, 0.050265274941921234, 0.05528292804956436, 0.051075730472803116, -0.051975317299366, -0.05146995931863785, 0.08393535763025284, 0.03505118563771248, 0.03724203631281853, 0.02131674624979496, -0.009617858566343784, -0.039549097418785095, 0.04634404927492142, 0.06742455810308456, 0.026563569903373718, -0.0033773083705455065, -0.04294678941369057, 0.02486477419734001, -0.029210064560174942, 0.08744955062866211, 0.016291143372654915, -0.0263126902282238, -0.032733574509620667, -0.0060755349695682526, -0.06217658147215843, -0.053052499890327454, 0.1211501732468605, -0.02654355578124523, -0.007729672826826572, 0.0006724545964971185, -0.010258463211357594, 0.05931968241930008, -0.06455094367265701, 0.02562222257256508, -0.009534064680337906, -0.05425405502319336, -0.08614201098680496, 0.0032151679042726755, 0.046190790832042694, -0.0017258675070479512, 0.018842827528715134, 0.009609526954591274, -0.0722208172082901, -0.029869019985198975, -0.06602009385824203, -0.07068155705928802, -0.1204669326543808, 0.030062833800911903, 0.01366698369383812, -0.042844563722610474, 0.023570144549012184, 0.03670989349484444, 0.014577209018170834, 0.12754783034324646, -0.013575064949691296, 0.07511134445667267, -0.04839304834604263, 0.0866178348660469, 0.12070254236459732, 0.031222624704241753, 0.01841508410871029, 0.02775343880057335, 0.046893633902072906]}